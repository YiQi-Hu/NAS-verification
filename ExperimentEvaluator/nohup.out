Evaluater: loading data
After 1 training epoch, loss on training batch is 4.037184
2019-06-20 21:06:16.271998: precision = 0.252
After 2 training epoch, loss on training batch is 3.564681
2019-06-20 21:06:17.537992: precision = 0.332
After 3 training epoch, loss on training batch is 3.128538
2019-06-20 21:06:18.805434: precision = 0.369
After 4 training epoch, loss on training batch is 2.925002
2019-06-20 21:06:20.066568: precision = 0.384
After 5 training epoch, loss on training batch is 2.418118
2019-06-20 21:06:21.332439: precision = 0.409
After 6 training epoch, loss on training batch is 2.119437
2019-06-20 21:06:22.598358: precision = 0.410
After 7 training epoch, loss on training batch is 1.851543
2019-06-20 21:06:23.856822: precision = 0.420
After 8 training epoch, loss on training batch is 1.486683
2019-06-20 21:06:25.121335: precision = 0.430
After 9 training epoch, loss on training batch is 1.377820
2019-06-20 21:06:26.386067: precision = 0.426
After 10 training epoch, loss on training batch is 1.105543
2019-06-20 21:06:27.645871: precision = 0.420
After 11 training epoch, loss on training batch is 0.919085
2019-06-20 21:06:28.909259: precision = 0.412
After 12 training epoch, loss on training batch is 0.880423
2019-06-20 21:06:30.176448: precision = 0.401
After 13 training epoch, loss on training batch is 0.825868
2019-06-20 21:06:31.434702: precision = 0.395
After 14 training epoch, loss on training batch is 0.454605
2019-06-20 21:06:32.688832: precision = 0.395
After 15 training epoch, loss on training batch is 0.926723
2019-06-20 21:06:33.943370: precision = 0.410
After 16 training epoch, loss on training batch is 0.615845
2019-06-20 21:06:35.209562: precision = 0.410
After 17 training epoch, loss on training batch is 0.243336
2019-06-20 21:06:36.466321: precision = 0.418
After 18 training epoch, loss on training batch is 0.379533
2019-06-20 21:06:37.727706: precision = 0.405
After 19 training epoch, loss on training batch is 0.581223
2019-06-20 21:06:38.988192: precision = 0.421
After 20 training epoch, loss on training batch is 0.246233
2019-06-20 21:06:40.246738: precision = 0.411
After 21 training epoch, loss on training batch is 0.242311
2019-06-20 21:06:41.504495: precision = 0.407
After 22 training epoch, loss on training batch is 0.072461
2019-06-20 21:06:42.764204: precision = 0.419
After 23 training epoch, loss on training batch is 0.171860
2019-06-20 21:06:44.031144: precision = 0.405
After 24 training epoch, loss on training batch is 0.107436
2019-06-20 21:06:45.299418: precision = 0.407
After 25 training epoch, loss on training batch is 0.031900
2019-06-20 21:06:46.562371: precision = 0.419
After 26 training epoch, loss on training batch is 0.027633
2019-06-20 21:06:47.815678: precision = 0.427
After 27 training epoch, loss on training batch is 0.040958
2019-06-20 21:06:49.072104: precision = 0.431
After 28 training epoch, loss on training batch is 0.030308
2019-06-20 21:06:50.342080: precision = 0.425
After 29 training epoch, loss on training batch is 0.018153
2019-06-20 21:06:51.600524: precision = 0.430
After 30 training epoch, loss on training batch is 0.006945
2019-06-20 21:06:52.863655: precision = 0.429
After 31 training epoch, loss on training batch is 0.003654
2019-06-20 21:06:54.128177: precision = 0.438
After 32 training epoch, loss on training batch is 0.006229
2019-06-20 21:06:55.392229: precision = 0.435
After 33 training epoch, loss on training batch is 0.016417
2019-06-20 21:06:56.656380: precision = 0.402
After 34 training epoch, loss on training batch is 0.011746
2019-06-20 21:06:57.911696: precision = 0.431
After 35 training epoch, loss on training batch is 0.006242
2019-06-20 21:06:59.179818: precision = 0.440
After 36 training epoch, loss on training batch is 0.002777
2019-06-20 21:07:00.452668: precision = 0.442
After 37 training epoch, loss on training batch is 0.008440
2019-06-20 21:07:01.718947: precision = 0.444
After 38 training epoch, loss on training batch is 0.002947
2019-06-20 21:07:02.983063: precision = 0.445
After 39 training epoch, loss on training batch is 0.001655
2019-06-20 21:07:04.240741: precision = 0.446
After 40 training epoch, loss on training batch is 0.001296
2019-06-20 21:07:05.495534: precision = 0.446
After 41 training epoch, loss on training batch is 0.001368
2019-06-20 21:07:06.761979: precision = 0.447
After 42 training epoch, loss on training batch is 0.001276
2019-06-20 21:07:08.029202: precision = 0.446
After 43 training epoch, loss on training batch is 0.001082
2019-06-20 21:07:09.294486: precision = 0.447
After 44 training epoch, loss on training batch is 0.001228
2019-06-20 21:07:10.553045: precision = 0.447
After 45 training epoch, loss on training batch is 0.001280
2019-06-20 21:07:11.818372: precision = 0.447
After 46 training epoch, loss on training batch is 0.001242
2019-06-20 21:07:13.082292: precision = 0.447
After 47 training epoch, loss on training batch is 0.001191
2019-06-20 21:07:14.353148: precision = 0.447
After 48 training epoch, loss on training batch is 0.001274
2019-06-20 21:07:15.613899: precision = 0.447
After 49 training epoch, loss on training batch is 0.001369
2019-06-20 21:07:16.874127: precision = 0.447
After 50 training epoch, loss on training batch is 0.001113
2019-06-20 21:07:18.131576: precision = 0.447
After 1 training epoch, loss on training batch is 3.533664
2019-06-20 21:07:25.124030: precision = 0.347
After 2 training epoch, loss on training batch is 3.187320
2019-06-20 21:07:27.365183: precision = 0.424
After 3 training epoch, loss on training batch is 2.889297
2019-06-20 21:07:29.607355: precision = 0.439
After 4 training epoch, loss on training batch is 2.627059
2019-06-20 21:07:31.848157: precision = 0.454
After 5 training epoch, loss on training batch is 2.091576
2019-06-20 21:07:34.086900: precision = 0.462
After 6 training epoch, loss on training batch is 2.022884
2019-06-20 21:07:36.329176: precision = 0.474
After 7 training epoch, loss on training batch is 2.075255
2019-06-20 21:07:38.572600: precision = 0.451
After 8 training epoch, loss on training batch is 1.633474
2019-06-20 21:07:40.816993: precision = 0.387
After 9 training epoch, loss on training batch is 1.560050
2019-06-20 21:07:43.057427: precision = 0.457
After 10 training epoch, loss on training batch is 1.088448
2019-06-20 21:07:45.302221: precision = 0.465
After 11 training epoch, loss on training batch is 0.996895
2019-06-20 21:07:47.541261: precision = 0.454
After 12 training epoch, loss on training batch is 1.120624
2019-06-20 21:07:49.783489: precision = 0.479
After 13 training epoch, loss on training batch is 0.871230
2019-06-20 21:07:52.025279: precision = 0.455
After 14 training epoch, loss on training batch is 0.580132
2019-06-20 21:07:54.260617: precision = 0.463
After 15 training epoch, loss on training batch is 0.401669
2019-06-20 21:07:56.505364: precision = 0.451
After 16 training epoch, loss on training batch is 0.599875
2019-06-20 21:07:58.745000: precision = 0.465
After 17 training epoch, loss on training batch is 0.413707
2019-06-20 21:08:00.991254: precision = 0.446
After 18 training epoch, loss on training batch is 0.528998
2019-06-20 21:08:03.234302: precision = 0.464
After 19 training epoch, loss on training batch is 0.436535
2019-06-20 21:08:05.474137: precision = 0.467
After 20 training epoch, loss on training batch is 0.346764
2019-06-20 21:08:07.716223: precision = 0.477
After 21 training epoch, loss on training batch is 0.410782
2019-06-20 21:08:09.957485: precision = 0.447
After 22 training epoch, loss on training batch is 0.528216
2019-06-20 21:08:12.197329: precision = 0.448
After 23 training epoch, loss on training batch is 0.250501
2019-06-20 21:08:14.436860: precision = 0.470
After 24 training epoch, loss on training batch is 0.097109
2019-06-20 21:08:16.677875: precision = 0.457
After 25 training epoch, loss on training batch is 0.386662
2019-06-20 21:08:18.913578: precision = 0.468
After 26 training epoch, loss on training batch is 0.175104
2019-06-20 21:08:21.160561: precision = 0.479
After 27 training epoch, loss on training batch is 0.136115
2019-06-20 21:08:23.395588: precision = 0.476
After 28 training epoch, loss on training batch is 0.170679
2019-06-20 21:08:25.636057: precision = 0.472
After 29 training epoch, loss on training batch is 0.070490
2019-06-20 21:08:27.875021: precision = 0.471
After 30 training epoch, loss on training batch is 0.103566
2019-06-20 21:08:30.108766: precision = 0.483
After 31 training epoch, loss on training batch is 0.057653
2019-06-20 21:08:32.336941: precision = 0.456
After 32 training epoch, loss on training batch is 0.055602
2019-06-20 21:08:34.577470: precision = 0.484
After 33 training epoch, loss on training batch is 0.177651
2019-06-20 21:08:36.805871: precision = 0.491
After 34 training epoch, loss on training batch is 0.016665
2019-06-20 21:08:39.044041: precision = 0.495
After 35 training epoch, loss on training batch is 0.015099
2019-06-20 21:08:41.285610: precision = 0.473
After 36 training epoch, loss on training batch is 0.024081
2019-06-20 21:08:43.527856: precision = 0.483
After 37 training epoch, loss on training batch is 0.167950
2019-06-20 21:08:45.767325: precision = 0.483
After 38 training epoch, loss on training batch is 0.034439
2019-06-20 21:08:48.004856: precision = 0.491
After 39 training epoch, loss on training batch is 0.022084
2019-06-20 21:08:50.241273: precision = 0.485
After 40 training epoch, loss on training batch is 0.023044
2019-06-20 21:08:52.485134: precision = 0.486
After 41 training epoch, loss on training batch is 0.025490
2019-06-20 21:08:54.722273: precision = 0.500
After 42 training epoch, loss on training batch is 0.002213
2019-06-20 21:08:56.956151: precision = 0.503
After 43 training epoch, loss on training batch is 0.004772
2019-06-20 21:08:59.194956: precision = 0.501
After 44 training epoch, loss on training batch is 0.000698
2019-06-20 21:09:01.443262: precision = 0.507
After 45 training epoch, loss on training batch is 0.000541
2019-06-20 21:09:03.680371: precision = 0.507
After 46 training epoch, loss on training batch is 0.000402
2019-06-20 21:09:05.916832: precision = 0.509
After 47 training epoch, loss on training batch is 0.000268
2019-06-20 21:09:08.155876: precision = 0.510
After 48 training epoch, loss on training batch is 0.000204
2019-06-20 21:09:10.396292: precision = 0.510
After 49 training epoch, loss on training batch is 0.000137
2019-06-20 21:09:12.631819: precision = 0.510
After 50 training epoch, loss on training batch is 0.000408
2019-06-20 21:09:14.870114: precision = 0.511
After 1 training epoch, loss on training batch is 3.205395
2019-06-20 21:09:24.663891: precision = 0.412
After 2 training epoch, loss on training batch is 2.725874
2019-06-20 21:09:28.414320: precision = 0.457
After 3 training epoch, loss on training batch is 2.427557
2019-06-20 21:09:32.158380: precision = 0.493
After 4 training epoch, loss on training batch is 1.875447
2019-06-20 21:09:35.917012: precision = 0.499
After 5 training epoch, loss on training batch is 1.675951
2019-06-20 21:09:39.675186: precision = 0.511
After 6 training epoch, loss on training batch is 1.464411
2019-06-20 21:09:43.425744: precision = 0.524
After 7 training epoch, loss on training batch is 1.545675
2019-06-20 21:09:47.180108: precision = 0.497
After 8 training epoch, loss on training batch is 1.272763
2019-06-20 21:09:50.931401: precision = 0.490
After 9 training epoch, loss on training batch is 1.308143
2019-06-20 21:09:54.686072: precision = 0.505
After 10 training epoch, loss on training batch is 1.011108
2019-06-20 21:09:58.436694: precision = 0.481
After 11 training epoch, loss on training batch is 1.056489
2019-06-20 21:10:02.204611: precision = 0.502
After 12 training epoch, loss on training batch is 1.043609
2019-06-20 21:10:05.958138: precision = 0.505
After 13 training epoch, loss on training batch is 0.912842
2019-06-20 21:10:09.714858: precision = 0.483
After 14 training epoch, loss on training batch is 0.672046
2019-06-20 21:10:13.460201: precision = 0.507
After 15 training epoch, loss on training batch is 0.495677
2019-06-20 21:10:17.217497: precision = 0.493
After 16 training epoch, loss on training batch is 0.526024
2019-06-20 21:10:20.958565: precision = 0.501
After 17 training epoch, loss on training batch is 0.618809
2019-06-20 21:10:24.705321: precision = 0.479
After 18 training epoch, loss on training batch is 0.467101
2019-06-20 21:10:28.460400: precision = 0.492
After 19 training epoch, loss on training batch is 0.563227
2019-06-20 21:10:32.214871: precision = 0.515
After 20 training epoch, loss on training batch is 0.764228
2019-06-20 21:10:35.967171: precision = 0.485
After 21 training epoch, loss on training batch is 0.397347
2019-06-20 21:10:39.709887: precision = 0.479
After 22 training epoch, loss on training batch is 0.227243
2019-06-20 21:10:43.464955: precision = 0.501
After 23 training epoch, loss on training batch is 0.286674
2019-06-20 21:10:47.213734: precision = 0.516
After 24 training epoch, loss on training batch is 0.240836
2019-06-20 21:10:50.964551: precision = 0.519
After 25 training epoch, loss on training batch is 0.148252
2019-06-20 21:10:54.723097: precision = 0.518
After 26 training epoch, loss on training batch is 0.200427
2019-06-20 21:10:58.476816: precision = 0.512
After 27 training epoch, loss on training batch is 0.092927
2019-06-20 21:11:02.223833: precision = 0.518
After 28 training epoch, loss on training batch is 0.141243
2019-06-20 21:11:05.975210: precision = 0.523
After 29 training epoch, loss on training batch is 0.104821
2019-06-20 21:11:09.725781: precision = 0.517
After 30 training epoch, loss on training batch is 0.075311
2019-06-20 21:11:13.475231: precision = 0.519
After 31 training epoch, loss on training batch is 0.104514
2019-06-20 21:11:17.225063: precision = 0.515
After 32 training epoch, loss on training batch is 0.177738
2019-06-20 21:11:20.970250: precision = 0.524
After 33 training epoch, loss on training batch is 0.019217
2019-06-20 21:11:24.723826: precision = 0.517
After 34 training epoch, loss on training batch is 0.127111
2019-06-20 21:11:28.466905: precision = 0.522
After 35 training epoch, loss on training batch is 0.153303
2019-06-20 21:11:32.220198: precision = 0.519
After 36 training epoch, loss on training batch is 0.095618
2019-06-20 21:11:35.973432: precision = 0.523
After 37 training epoch, loss on training batch is 0.122646
2019-06-20 21:11:39.726917: precision = 0.518
After 38 training epoch, loss on training batch is 0.042076
2019-06-20 21:11:43.493501: precision = 0.523
After 39 training epoch, loss on training batch is 0.031697
2019-06-20 21:11:47.255430: precision = 0.526
After 40 training epoch, loss on training batch is 0.036499
2019-06-20 21:11:51.010773: precision = 0.519
After 41 training epoch, loss on training batch is 0.161158
2019-06-20 21:11:54.756989: precision = 0.516
After 42 training epoch, loss on training batch is 0.114875
2019-06-20 21:11:58.502336: precision = 0.512
After 43 training epoch, loss on training batch is 0.081778
2019-06-20 21:12:02.260831: precision = 0.516
After 44 training epoch, loss on training batch is 0.066218
2019-06-20 21:12:06.006402: precision = 0.504
After 45 training epoch, loss on training batch is 0.162885
2019-06-20 21:12:09.761883: precision = 0.504
After 46 training epoch, loss on training batch is 0.070095
2019-06-20 21:12:13.498504: precision = 0.503
After 47 training epoch, loss on training batch is 0.053836
2019-06-20 21:12:17.241423: precision = 0.507
After 48 training epoch, loss on training batch is 0.066403
2019-06-20 21:12:20.979787: precision = 0.497
After 49 training epoch, loss on training batch is 0.031506
2019-06-20 21:12:24.730287: precision = 0.504
After 50 training epoch, loss on training batch is 0.052175
2019-06-20 21:12:28.485843: precision = 0.506
After 1 training epoch, loss on training batch is 2.805787
2019-06-20 21:12:40.431279: precision = 0.447
After 2 training epoch, loss on training batch is 2.732132
2019-06-20 21:12:46.109696: precision = 0.498
After 3 training epoch, loss on training batch is 2.562368
2019-06-20 21:12:51.788415: precision = 0.532
After 4 training epoch, loss on training batch is 2.309646
2019-06-20 21:12:57.461243: precision = 0.546
After 5 training epoch, loss on training batch is 2.103252
2019-06-20 21:13:03.148834: precision = 0.551
After 6 training epoch, loss on training batch is 1.891941
2019-06-20 21:13:08.821438: precision = 0.570
After 7 training epoch, loss on training batch is 1.755062
2019-06-20 21:13:14.500384: precision = 0.560
After 8 training epoch, loss on training batch is 1.903368
2019-06-20 21:13:20.180344: precision = 0.547
After 9 training epoch, loss on training batch is 1.587147
2019-06-20 21:13:25.859667: precision = 0.534
After 10 training epoch, loss on training batch is 2.181257
2019-06-20 21:13:31.535962: precision = 0.541
After 11 training epoch, loss on training batch is 1.327265
2019-06-20 21:13:37.220190: precision = 0.548
After 12 training epoch, loss on training batch is 1.125386
2019-06-20 21:13:42.898293: precision = 0.536
After 13 training epoch, loss on training batch is 1.126618
2019-06-20 21:13:48.578906: precision = 0.534
After 14 training epoch, loss on training batch is 1.272145
2019-06-20 21:13:54.254609: precision = 0.499
After 15 training epoch, loss on training batch is 0.742682
2019-06-20 21:13:59.927724: precision = 0.506
After 16 training epoch, loss on training batch is 0.785574
2019-06-20 21:14:05.604061: precision = 0.540
After 17 training epoch, loss on training batch is 0.751317
2019-06-20 21:14:11.280202: precision = 0.542
After 18 training epoch, loss on training batch is 0.539852
2019-06-20 21:14:16.960558: precision = 0.528
After 19 training epoch, loss on training batch is 0.599809
2019-06-20 21:14:22.636068: precision = 0.539
After 20 training epoch, loss on training batch is 0.593991
2019-06-20 21:14:28.310802: precision = 0.539
After 21 training epoch, loss on training batch is 0.445425
2019-06-20 21:14:33.978494: precision = 0.535
After 22 training epoch, loss on training batch is 0.290567
2019-06-20 21:14:39.647473: precision = 0.536
After 23 training epoch, loss on training batch is 0.304150
2019-06-20 21:14:45.322791: precision = 0.541
After 24 training epoch, loss on training batch is 0.295257
2019-06-20 21:14:50.991303: precision = 0.535
After 25 training epoch, loss on training batch is 0.183779
2019-06-20 21:14:56.665624: precision = 0.531
After 26 training epoch, loss on training batch is 0.241327
2019-06-20 21:15:02.345077: precision = 0.530
After 27 training epoch, loss on training batch is 0.235298
2019-06-20 21:15:08.020460: precision = 0.551
After 28 training epoch, loss on training batch is 0.183541
2019-06-20 21:15:13.699160: precision = 0.557
After 29 training epoch, loss on training batch is 0.200775
2019-06-20 21:15:19.373335: precision = 0.548
After 30 training epoch, loss on training batch is 0.354445
2019-06-20 21:15:25.049777: precision = 0.537
After 31 training epoch, loss on training batch is 0.196933
2019-06-20 21:15:30.727855: precision = 0.535
After 32 training epoch, loss on training batch is 0.171861
2019-06-20 21:15:36.400577: precision = 0.524
After 33 training epoch, loss on training batch is 0.098086
2019-06-20 21:15:42.079176: precision = 0.537
After 34 training epoch, loss on training batch is 0.275465
2019-06-20 21:15:47.748528: precision = 0.538
After 35 training epoch, loss on training batch is 0.247823
2019-06-20 21:15:53.424439: precision = 0.546
After 36 training epoch, loss on training batch is 0.157919
2019-06-20 21:15:59.101140: precision = 0.541
After 37 training epoch, loss on training batch is 0.131143
2019-06-20 21:16:04.786601: precision = 0.539
After 38 training epoch, loss on training batch is 0.111735
2019-06-20 21:16:10.460868: precision = 0.553
After 39 training epoch, loss on training batch is 0.111728
2019-06-20 21:16:16.141309: precision = 0.552
After 40 training epoch, loss on training batch is 0.030444
2019-06-20 21:16:21.822108: precision = 0.544
After 41 training epoch, loss on training batch is 0.112184
2019-06-20 21:16:27.501681: precision = 0.544
After 42 training epoch, loss on training batch is 0.041348
2019-06-20 21:16:33.172792: precision = 0.551
After 43 training epoch, loss on training batch is 0.079596
2019-06-20 21:16:38.841631: precision = 0.530
After 44 training epoch, loss on training batch is 0.104576
2019-06-20 21:16:44.507417: precision = 0.539
After 45 training epoch, loss on training batch is 0.057446
2019-06-20 21:16:50.180792: precision = 0.532
After 46 training epoch, loss on training batch is 0.050028
2019-06-20 21:16:55.853614: precision = 0.540
After 47 training epoch, loss on training batch is 0.219009
2019-06-20 21:17:01.525969: precision = 0.547
After 48 training epoch, loss on training batch is 0.141871
2019-06-20 21:17:07.196424: precision = 0.544
After 49 training epoch, loss on training batch is 0.167426
2019-06-20 21:17:12.860926: precision = 0.547
After 50 training epoch, loss on training batch is 0.115648
2019-06-20 21:17:18.532130: precision = 0.554
After 1 training epoch, loss on training batch is 3.046030
2019-06-20 21:17:32.168329: precision = 0.477
After 2 training epoch, loss on training batch is 2.508507
2019-06-20 21:17:40.277954: precision = 0.524
After 3 training epoch, loss on training batch is 2.349018
2019-06-20 21:17:48.378777: precision = 0.548
After 4 training epoch, loss on training batch is 2.380088
2019-06-20 21:17:56.492550: precision = 0.565
After 5 training epoch, loss on training batch is 1.956535
2019-06-20 21:18:04.600785: precision = 0.572
After 6 training epoch, loss on training batch is 1.999451
2019-06-20 21:18:12.708377: precision = 0.570
After 7 training epoch, loss on training batch is 1.413400
2019-06-20 21:18:20.815314: precision = 0.538
After 8 training epoch, loss on training batch is 1.213619
2019-06-20 21:18:28.923515: precision = 0.556
After 9 training epoch, loss on training batch is 1.447617
2019-06-20 21:18:37.030080: precision = 0.562
After 10 training epoch, loss on training batch is 1.361111
2019-06-20 21:18:45.134275: precision = 0.563
After 11 training epoch, loss on training batch is 1.410896
2019-06-20 21:18:53.228762: precision = 0.557
After 12 training epoch, loss on training batch is 0.823362
2019-06-20 21:19:01.329106: precision = 0.550
After 13 training epoch, loss on training batch is 0.833038
2019-06-20 21:19:09.434505: precision = 0.554
After 14 training epoch, loss on training batch is 1.053032
2019-06-20 21:19:17.538842: precision = 0.557
After 15 training epoch, loss on training batch is 0.856544
2019-06-20 21:19:25.642878: precision = 0.554
After 16 training epoch, loss on training batch is 0.531286
2019-06-20 21:19:33.748105: precision = 0.554
After 17 training epoch, loss on training batch is 0.745251
2019-06-20 21:19:41.850489: precision = 0.549
After 18 training epoch, loss on training batch is 0.450342
2019-06-20 21:19:49.954077: precision = 0.561
After 19 training epoch, loss on training batch is 0.734039
2019-06-20 21:19:58.052683: precision = 0.554
After 20 training epoch, loss on training batch is 0.567061
2019-06-20 21:20:06.152423: precision = 0.554
After 21 training epoch, loss on training batch is 0.365176
2019-06-20 21:20:14.253281: precision = 0.558
After 22 training epoch, loss on training batch is 0.398874
2019-06-20 21:20:22.361806: precision = 0.554
After 23 training epoch, loss on training batch is 0.559082
2019-06-20 21:20:30.454279: precision = 0.566
After 24 training epoch, loss on training batch is 0.722738
2019-06-20 21:20:38.560377: precision = 0.550
After 25 training epoch, loss on training batch is 0.275976
2019-06-20 21:20:46.666938: precision = 0.557
After 26 training epoch, loss on training batch is 0.398575
2019-06-20 21:20:54.761141: precision = 0.554
After 27 training epoch, loss on training batch is 0.262981
2019-06-20 21:21:02.864034: precision = 0.543
After 28 training epoch, loss on training batch is 0.192281
2019-06-20 21:21:10.976096: precision = 0.557
After 29 training epoch, loss on training batch is 0.160100
2019-06-20 21:21:19.074714: precision = 0.560
After 30 training epoch, loss on training batch is 0.383839
2019-06-20 21:21:27.172653: precision = 0.574
After 31 training epoch, loss on training batch is 0.203009
2019-06-20 21:21:35.269497: precision = 0.566
After 32 training epoch, loss on training batch is 0.135660
2019-06-20 21:21:43.369660: precision = 0.550
After 33 training epoch, loss on training batch is 0.183048
2019-06-20 21:21:51.470537: precision = 0.562
After 34 training epoch, loss on training batch is 0.217508
2019-06-20 21:21:59.565519: precision = 0.559
After 35 training epoch, loss on training batch is 0.160285
2019-06-20 21:22:07.668526: precision = 0.555
After 36 training epoch, loss on training batch is 0.232060
2019-06-20 21:22:15.772387: precision = 0.567
After 37 training epoch, loss on training batch is 0.173924
2019-06-20 21:22:23.865085: precision = 0.563
After 38 training epoch, loss on training batch is 0.141746
2019-06-20 21:22:31.970327: precision = 0.563
After 39 training epoch, loss on training batch is 0.092685
2019-06-20 21:22:40.070171: precision = 0.565
After 40 training epoch, loss on training batch is 0.248861
2019-06-20 21:22:48.162370: precision = 0.566
After 41 training epoch, loss on training batch is 0.207189
2019-06-20 21:22:56.261031: precision = 0.558
After 42 training epoch, loss on training batch is 0.060692
2019-06-20 21:23:04.367264: precision = 0.552
After 43 training epoch, loss on training batch is 0.165693
2019-06-20 21:23:12.465411: precision = 0.563
After 44 training epoch, loss on training batch is 0.190378
2019-06-20 21:23:20.565326: precision = 0.562
After 45 training epoch, loss on training batch is 0.078602
2019-06-20 21:23:28.663816: precision = 0.552
After 46 training epoch, loss on training batch is 0.087195
2019-06-20 21:23:36.768860: precision = 0.555
After 47 training epoch, loss on training batch is 0.104214
2019-06-20 21:23:44.865997: precision = 0.563
After 48 training epoch, loss on training batch is 0.139383
2019-06-20 21:23:52.969037: precision = 0.556
After 49 training epoch, loss on training batch is 0.046544
2019-06-20 21:24:01.069537: precision = 0.577
After 50 training epoch, loss on training batch is 0.080075
2019-06-20 21:24:09.165397: precision = 0.567
After 1 training epoch, loss on training batch is 2.816475
2019-06-20 21:24:22.837735: precision = 0.485
After 2 training epoch, loss on training batch is 2.381701
2019-06-20 21:24:34.070536: precision = 0.538
After 3 training epoch, loss on training batch is 2.349269
2019-06-20 21:24:45.299664: precision = 0.564
After 4 training epoch, loss on training batch is 2.041475
2019-06-20 21:24:56.524137: precision = 0.566
After 5 training epoch, loss on training batch is 1.717081
2019-06-20 21:25:07.755320: precision = 0.593
After 6 training epoch, loss on training batch is 1.926284
2019-06-20 21:25:18.985075: precision = 0.577
After 7 training epoch, loss on training batch is 1.600395
2019-06-20 21:25:30.212975: precision = 0.569
After 8 training epoch, loss on training batch is 1.678067
2019-06-20 21:25:41.434550: precision = 0.597
After 9 training epoch, loss on training batch is 1.472514
2019-06-20 21:25:52.660678: precision = 0.594
After 10 training epoch, loss on training batch is 0.818090
2019-06-20 21:26:03.884269: precision = 0.588
After 11 training epoch, loss on training batch is 1.051189
2019-06-20 21:26:15.109165: precision = 0.577
After 12 training epoch, loss on training batch is 1.076723
2019-06-20 21:26:26.330359: precision = 0.575
After 13 training epoch, loss on training batch is 0.585953
2019-06-20 21:26:37.550316: precision = 0.569
After 14 training epoch, loss on training batch is 0.735179
2019-06-20 21:26:48.767712: precision = 0.578
After 15 training epoch, loss on training batch is 1.047530
2019-06-20 21:26:59.976708: precision = 0.571
After 16 training epoch, loss on training batch is 0.949051
2019-06-20 21:27:11.199388: precision = 0.561
After 17 training epoch, loss on training batch is 0.504490
2019-06-20 21:27:22.414986: precision = 0.574
After 18 training epoch, loss on training batch is 0.444602
2019-06-20 21:27:33.635106: precision = 0.573
After 19 training epoch, loss on training batch is 0.525751
2019-06-20 21:27:44.855427: precision = 0.565
After 20 training epoch, loss on training batch is 0.402922
2019-06-20 21:27:56.074382: precision = 0.552
After 21 training epoch, loss on training batch is 0.240521
2019-06-20 21:28:07.291094: precision = 0.567
After 22 training epoch, loss on training batch is 0.387537
2019-06-20 21:28:18.512545: precision = 0.574
After 23 training epoch, loss on training batch is 0.647192
2019-06-20 21:28:29.733292: precision = 0.570
After 24 training epoch, loss on training batch is 0.302917
2019-06-20 21:28:40.949240: precision = 0.581
After 25 training epoch, loss on training batch is 0.214160
2019-06-20 21:28:52.163322: precision = 0.588
After 26 training epoch, loss on training batch is 0.371429
2019-06-20 21:29:03.382011: precision = 0.592
After 27 training epoch, loss on training batch is 0.346246
2019-06-20 21:29:14.593877: precision = 0.584
After 28 training epoch, loss on training batch is 0.349784
2019-06-20 21:29:25.813219: precision = 0.582
After 29 training epoch, loss on training batch is 0.207437
2019-06-20 21:29:37.026177: precision = 0.589
After 30 training epoch, loss on training batch is 0.151477
2019-06-20 21:29:48.235120: precision = 0.587
After 31 training epoch, loss on training batch is 0.433170
2019-06-20 21:29:59.452294: precision = 0.582
After 32 training epoch, loss on training batch is 0.503952
2019-06-20 21:30:10.671282: precision = 0.595
After 33 training epoch, loss on training batch is 0.175502
2019-06-20 21:30:21.893332: precision = 0.599
After 34 training epoch, loss on training batch is 0.268445
2019-06-20 21:30:33.107232: precision = 0.597
After 35 training epoch, loss on training batch is 0.169601
2019-06-20 21:30:44.320118: precision = 0.593
After 36 training epoch, loss on training batch is 0.112379
2019-06-20 21:30:55.531356: precision = 0.593
After 37 training epoch, loss on training batch is 0.107211
2019-06-20 21:31:06.754320: precision = 0.602
After 38 training epoch, loss on training batch is 0.086147
2019-06-20 21:31:17.969916: precision = 0.595
After 39 training epoch, loss on training batch is 0.182982
2019-06-20 21:31:29.178733: precision = 0.586
After 40 training epoch, loss on training batch is 0.087306
2019-06-20 21:31:40.392975: precision = 0.593
After 41 training epoch, loss on training batch is 0.130889
2019-06-20 21:31:51.607817: precision = 0.596
After 42 training epoch, loss on training batch is 0.091227
2019-06-20 21:32:02.819484: precision = 0.596
After 43 training epoch, loss on training batch is 0.091353
2019-06-20 21:32:14.027952: precision = 0.589
After 44 training epoch, loss on training batch is 0.258014
2019-06-20 21:32:25.244730: precision = 0.593
After 45 training epoch, loss on training batch is 0.118438
2019-06-20 21:32:36.453833: precision = 0.597
After 46 training epoch, loss on training batch is 0.076981
2019-06-20 21:32:47.667271: precision = 0.591
After 47 training epoch, loss on training batch is 0.039987
2019-06-20 21:32:58.876373: precision = 0.596
After 48 training epoch, loss on training batch is 0.228601
2019-06-20 21:33:10.086247: precision = 0.599
After 49 training epoch, loss on training batch is 0.068471
2019-06-20 21:33:21.295996: precision = 0.600
After 50 training epoch, loss on training batch is 0.311884
2019-06-20 21:33:32.505905: precision = 0.595
After 1 training epoch, loss on training batch is 2.736001
2019-06-20 21:33:44.192757: precision = 0.484
After 2 training epoch, loss on training batch is 2.529330
2019-06-20 21:33:55.494680: precision = 0.537
After 3 training epoch, loss on training batch is 2.338368
2019-06-20 21:34:06.806956: precision = 0.560
After 4 training epoch, loss on training batch is 1.949708
2019-06-20 21:34:18.103967: precision = 0.565
After 5 training epoch, loss on training batch is 1.774903
2019-06-20 21:34:29.404933: precision = 0.560
After 6 training epoch, loss on training batch is 1.747375
2019-06-20 21:34:40.696201: precision = 0.588
After 7 training epoch, loss on training batch is 1.834710
2019-06-20 21:34:51.994882: precision = 0.589
After 8 training epoch, loss on training batch is 1.603785
2019-06-20 21:35:03.298863: precision = 0.579
After 9 training epoch, loss on training batch is 1.958948
2019-06-20 21:35:14.598256: precision = 0.566
After 10 training epoch, loss on training batch is 0.882739
2019-06-20 21:35:25.886758: precision = 0.581
After 11 training epoch, loss on training batch is 0.852035
2019-06-20 21:35:37.181748: precision = 0.587
After 12 training epoch, loss on training batch is 0.768340
2019-06-20 21:35:48.472496: precision = 0.585
After 13 training epoch, loss on training batch is 0.584157
2019-06-20 21:35:59.764873: precision = 0.586
After 14 training epoch, loss on training batch is 0.485678
2019-06-20 21:36:11.048590: precision = 0.583
After 15 training epoch, loss on training batch is 0.844383
2019-06-20 21:36:22.345144: precision = 0.566
After 16 training epoch, loss on training batch is 0.403089
2019-06-20 21:36:33.642730: precision = 0.571
After 17 training epoch, loss on training batch is 0.677488
2019-06-20 21:36:44.934817: precision = 0.594
After 18 training epoch, loss on training batch is 0.436612
2019-06-20 21:36:56.225125: precision = 0.579
After 19 training epoch, loss on training batch is 0.548735
2019-06-20 21:37:07.519824: precision = 0.581
After 20 training epoch, loss on training batch is 0.261052
2019-06-20 21:37:18.810283: precision = 0.588
After 21 training epoch, loss on training batch is 0.282557
2019-06-20 21:37:30.093290: precision = 0.583
After 22 training epoch, loss on training batch is 0.300034
2019-06-20 21:37:41.380574: precision = 0.589
After 23 training epoch, loss on training batch is 0.297302
2019-06-20 21:37:52.672307: precision = 0.580
After 24 training epoch, loss on training batch is 0.307280
2019-06-20 21:38:03.964023: precision = 0.570
After 25 training epoch, loss on training batch is 0.212847
2019-06-20 21:38:15.249086: precision = 0.577
After 26 training epoch, loss on training batch is 0.272113
2019-06-20 21:38:26.538337: precision = 0.580
After 27 training epoch, loss on training batch is 0.282028
2019-06-20 21:38:37.830603: precision = 0.580
After 28 training epoch, loss on training batch is 0.559163
2019-06-20 21:38:49.121784: precision = 0.577
After 29 training epoch, loss on training batch is 0.376494
2019-06-20 21:39:00.395956: precision = 0.568
After 30 training epoch, loss on training batch is 0.235552
2019-06-20 21:39:11.687680: precision = 0.576
After 31 training epoch, loss on training batch is 0.137078
2019-06-20 21:39:22.972368: precision = 0.584
After 32 training epoch, loss on training batch is 0.268064
2019-06-20 21:39:34.250205: precision = 0.583
After 33 training epoch, loss on training batch is 0.180874
2019-06-20 21:39:45.527544: precision = 0.588
After 34 training epoch, loss on training batch is 0.327537
2019-06-20 21:39:56.815316: precision = 0.577
After 35 training epoch, loss on training batch is 0.183878
2019-06-20 21:40:08.113666: precision = 0.581
After 36 training epoch, loss on training batch is 0.061069
2019-06-20 21:40:19.403970: precision = 0.582
After 37 training epoch, loss on training batch is 0.184131
2019-06-20 21:40:30.692225: precision = 0.581
After 38 training epoch, loss on training batch is 0.078649
2019-06-20 21:40:41.980247: precision = 0.588
After 39 training epoch, loss on training batch is 0.048510
2019-06-20 21:40:53.267678: precision = 0.579
After 40 training epoch, loss on training batch is 0.298787
2019-06-20 21:41:04.555978: precision = 0.569
After 41 training epoch, loss on training batch is 0.136747
2019-06-20 21:41:15.838109: precision = 0.579
After 42 training epoch, loss on training batch is 0.151017
2019-06-20 21:41:27.123848: precision = 0.572
After 43 training epoch, loss on training batch is 0.272760
2019-06-20 21:41:38.416314: precision = 0.588
After 44 training epoch, loss on training batch is 0.304137
2019-06-20 21:41:49.704526: precision = 0.577
After 45 training epoch, loss on training batch is 0.196827
2019-06-20 21:42:00.996899: precision = 0.591
After 46 training epoch, loss on training batch is 0.094001
2019-06-20 21:42:12.275715: precision = 0.593
After 47 training epoch, loss on training batch is 0.056384
2019-06-20 21:42:23.559223: precision = 0.590
After 48 training epoch, loss on training batch is 0.062440
2019-06-20 21:42:34.846979: precision = 0.586
After 49 training epoch, loss on training batch is 0.058999
2019-06-20 21:42:46.128944: precision = 0.585
After 50 training epoch, loss on training batch is 0.452109
2019-06-20 21:42:57.411837: precision = 0.594
After 1 training epoch, loss on training batch is 2.708335
2019-06-20 21:43:08.683663: precision = 0.470
After 2 training epoch, loss on training batch is 2.562653
2019-06-20 21:43:19.440219: precision = 0.538
After 3 training epoch, loss on training batch is 2.416062
2019-06-20 21:43:30.198187: precision = 0.551
After 4 training epoch, loss on training batch is 2.034384
2019-06-20 21:43:40.948329: precision = 0.576
After 5 training epoch, loss on training batch is 1.750394
2019-06-20 21:43:51.698418: precision = 0.594
After 6 training epoch, loss on training batch is 1.764994
2019-06-20 21:44:02.458314: precision = 0.593
After 7 training epoch, loss on training batch is 1.531488
2019-06-20 21:44:13.214756: precision = 0.589
After 8 training epoch, loss on training batch is 1.322054
2019-06-20 21:44:23.961739: precision = 0.575
After 9 training epoch, loss on training batch is 1.720920
2019-06-20 21:44:34.706579: precision = 0.592
After 10 training epoch, loss on training batch is 1.110667
2019-06-20 21:44:45.463992: precision = 0.588
After 11 training epoch, loss on training batch is 0.741512
2019-06-20 21:44:56.213553: precision = 0.581
After 12 training epoch, loss on training batch is 0.867957
2019-06-20 21:45:06.968897: precision = 0.586
After 13 training epoch, loss on training batch is 0.652194
2019-06-20 21:45:17.715196: precision = 0.583
After 14 training epoch, loss on training batch is 0.639544
2019-06-20 21:45:28.468400: precision = 0.576
After 15 training epoch, loss on training batch is 0.864062
2019-06-20 21:45:39.211244: precision = 0.565
After 16 training epoch, loss on training batch is 0.471911
2019-06-20 21:45:49.951129: precision = 0.566
After 17 training epoch, loss on training batch is 0.415777
2019-06-20 21:46:00.707159: precision = 0.571
After 18 training epoch, loss on training batch is 0.360179
2019-06-20 21:46:11.457361: precision = 0.577
After 19 training epoch, loss on training batch is 0.340930
2019-06-20 21:46:22.213185: precision = 0.587
After 20 training epoch, loss on training batch is 0.416082
2019-06-20 21:46:32.961694: precision = 0.581
After 21 training epoch, loss on training batch is 0.430920
2019-06-20 21:46:43.708817: precision = 0.572
After 22 training epoch, loss on training batch is 0.257953
2019-06-20 21:46:54.462120: precision = 0.580
After 23 training epoch, loss on training batch is 0.322111
2019-06-20 21:47:05.210410: precision = 0.586
After 24 training epoch, loss on training batch is 0.321737
2019-06-20 21:47:15.949376: precision = 0.580
After 25 training epoch, loss on training batch is 0.281139
2019-06-20 21:47:26.691683: precision = 0.588
After 26 training epoch, loss on training batch is 0.254159
2019-06-20 21:47:37.425910: precision = 0.588
After 27 training epoch, loss on training batch is 0.394538
2019-06-20 21:47:48.169711: precision = 0.576
After 28 training epoch, loss on training batch is 0.437878
2019-06-20 21:47:58.908381: precision = 0.582
After 29 training epoch, loss on training batch is 0.197640
2019-06-20 21:48:09.657763: precision = 0.585
After 30 training epoch, loss on training batch is 0.316830
2019-06-20 21:48:20.404341: precision = 0.583
After 31 training epoch, loss on training batch is 0.668310
2019-06-20 21:48:31.149210: precision = 0.575
After 32 training epoch, loss on training batch is 0.309278
2019-06-20 21:48:41.885356: precision = 0.566
After 33 training epoch, loss on training batch is 0.269833
2019-06-20 21:48:52.625784: precision = 0.573
After 34 training epoch, loss on training batch is 0.045320
2019-06-20 21:49:03.373229: precision = 0.586
After 35 training epoch, loss on training batch is 0.092724
2019-06-20 21:49:14.109683: precision = 0.580
After 36 training epoch, loss on training batch is 0.109565
2019-06-20 21:49:24.858374: precision = 0.581
After 37 training epoch, loss on training batch is 0.055111
2019-06-20 21:49:35.590855: precision = 0.581
After 38 training epoch, loss on training batch is 0.059014
2019-06-20 21:49:46.339932: precision = 0.590
After 39 training epoch, loss on training batch is 0.340148
2019-06-20 21:49:57.085275: precision = 0.589
After 40 training epoch, loss on training batch is 0.095131
2019-06-20 21:50:07.837413: precision = 0.583
After 41 training epoch, loss on training batch is 0.102669
2019-06-20 21:50:18.584116: precision = 0.577
After 42 training epoch, loss on training batch is 0.036665
2019-06-20 21:50:29.320532: precision = 0.590
After 43 training epoch, loss on training batch is 0.045594
2019-06-20 21:50:40.058996: precision = 0.591
After 44 training epoch, loss on training batch is 0.357110
2019-06-20 21:50:50.798137: precision = 0.587
After 45 training epoch, loss on training batch is 0.060743
2019-06-20 21:51:01.540627: precision = 0.588
After 46 training epoch, loss on training batch is 0.102302
2019-06-20 21:51:12.280584: precision = 0.586
After 47 training epoch, loss on training batch is 0.054363
2019-06-20 21:51:23.021791: precision = 0.586
After 48 training epoch, loss on training batch is 0.132415
2019-06-20 21:51:33.756463: precision = 0.590
After 49 training epoch, loss on training batch is 0.101234
2019-06-20 21:51:44.503325: precision = 0.588
After 50 training epoch, loss on training batch is 0.141233
2019-06-20 21:51:55.237601: precision = 0.587
After 1 training epoch, loss on training batch is 3.007901
2019-06-20 21:52:06.259158: precision = 0.481
After 2 training epoch, loss on training batch is 2.508914
2019-06-20 21:52:16.843584: precision = 0.540
After 3 training epoch, loss on training batch is 2.357627
2019-06-20 21:52:27.424104: precision = 0.568
After 4 training epoch, loss on training batch is 1.917887
2019-06-20 21:52:38.018496: precision = 0.580
After 5 training epoch, loss on training batch is 1.862070
2019-06-20 21:52:48.603647: precision = 0.585
After 6 training epoch, loss on training batch is 1.904826
2019-06-20 21:52:59.191000: precision = 0.587
After 7 training epoch, loss on training batch is 1.751353
2019-06-20 21:53:09.783074: precision = 0.572
After 8 training epoch, loss on training batch is 1.400298
2019-06-20 21:53:20.378225: precision = 0.579
After 9 training epoch, loss on training batch is 1.430941
2019-06-20 21:53:30.970708: precision = 0.595
After 10 training epoch, loss on training batch is 1.137880
2019-06-20 21:53:41.559398: precision = 0.573
After 11 training epoch, loss on training batch is 0.908151
2019-06-20 21:53:52.140090: precision = 0.560
After 12 training epoch, loss on training batch is 1.015902
2019-06-20 21:54:02.731293: precision = 0.566
After 13 training epoch, loss on training batch is 0.704759
2019-06-20 21:54:13.316092: precision = 0.581
After 14 training epoch, loss on training batch is 0.678831
2019-06-20 21:54:23.906243: precision = 0.573
After 15 training epoch, loss on training batch is 0.697691
2019-06-20 21:54:34.487186: precision = 0.566
After 16 training epoch, loss on training batch is 0.564225
2019-06-20 21:54:45.071205: precision = 0.569
After 17 training epoch, loss on training batch is 0.589392
2019-06-20 21:54:55.652695: precision = 0.574
After 18 training epoch, loss on training batch is 0.384566
2019-06-20 21:55:06.240351: precision = 0.580
After 19 training epoch, loss on training batch is 0.376225
2019-06-20 21:55:16.826693: precision = 0.584
After 20 training epoch, loss on training batch is 0.334493
2019-06-20 21:55:27.412986: precision = 0.574
After 21 training epoch, loss on training batch is 0.360673
2019-06-20 21:55:38.001718: precision = 0.581
After 22 training epoch, loss on training batch is 0.375909
2019-06-20 21:55:48.581533: precision = 0.584
After 23 training epoch, loss on training batch is 0.808805
2019-06-20 21:55:59.154270: precision = 0.584
After 24 training epoch, loss on training batch is 0.203105
2019-06-20 21:56:09.734101: precision = 0.576
After 25 training epoch, loss on training batch is 0.230974
2019-06-20 21:56:20.321458: precision = 0.577
After 26 training epoch, loss on training batch is 0.224517
2019-06-20 21:56:30.893440: precision = 0.579
After 27 training epoch, loss on training batch is 0.137766
2019-06-20 21:56:41.455716: precision = 0.586
After 28 training epoch, loss on training batch is 0.095150
2019-06-20 21:56:52.038451: precision = 0.579
After 29 training epoch, loss on training batch is 0.094698
2019-06-20 21:57:02.619085: precision = 0.572
After 30 training epoch, loss on training batch is 0.273101
2019-06-20 21:57:13.192310: precision = 0.580
After 31 training epoch, loss on training batch is 0.215891
2019-06-20 21:57:23.773106: precision = 0.583
After 32 training epoch, loss on training batch is 0.243328
2019-06-20 21:57:34.351270: precision = 0.581
After 33 training epoch, loss on training batch is 0.097888
2019-06-20 21:57:44.923114: precision = 0.567
After 34 training epoch, loss on training batch is 0.278812
2019-06-20 21:57:55.503168: precision = 0.582
After 35 training epoch, loss on training batch is 0.289594
2019-06-20 21:58:06.080936: precision = 0.577
After 36 training epoch, loss on training batch is 0.166819
2019-06-20 21:58:16.663946: precision = 0.586
After 37 training epoch, loss on training batch is 0.057112
2019-06-20 21:58:27.242320: precision = 0.588
After 38 training epoch, loss on training batch is 0.137833
2019-06-20 21:58:37.819640: precision = 0.585
After 39 training epoch, loss on training batch is 0.057487
2019-06-20 21:58:48.396334: precision = 0.584
After 40 training epoch, loss on training batch is 0.147774
2019-06-20 21:58:58.973401: precision = 0.588
After 41 training epoch, loss on training batch is 0.538565
2019-06-20 21:59:09.556711: precision = 0.580
After 42 training epoch, loss on training batch is 0.206332
2019-06-20 21:59:20.133900: precision = 0.572
After 43 training epoch, loss on training batch is 0.194601
2019-06-20 21:59:30.712783: precision = 0.581
After 44 training epoch, loss on training batch is 0.267811
2019-06-20 21:59:41.285624: precision = 0.584
After 45 training epoch, loss on training batch is 0.211503
2019-06-20 21:59:51.855534: precision = 0.597
After 46 training epoch, loss on training batch is 0.271770
2019-06-20 22:00:02.428387: precision = 0.585
After 47 training epoch, loss on training batch is 0.183164
2019-06-20 22:00:13.008751: precision = 0.590
After 48 training epoch, loss on training batch is 0.135384
2019-06-20 22:00:23.588635: precision = 0.594
After 49 training epoch, loss on training batch is 0.154870
2019-06-20 22:00:34.165425: precision = 0.596
After 50 training epoch, loss on training batch is 0.185927
2019-06-20 22:00:44.740166: precision = 0.597
After 1 training epoch, loss on training batch is 2.771071
2019-06-20 22:00:55.754254: precision = 0.483
After 2 training epoch, loss on training batch is 2.728593
2019-06-20 22:01:06.334818: precision = 0.531
After 3 training epoch, loss on training batch is 2.409192
2019-06-20 22:01:16.891296: precision = 0.570
After 4 training epoch, loss on training batch is 1.941683
2019-06-20 22:01:27.450772: precision = 0.565
After 5 training epoch, loss on training batch is 1.552676
2019-06-20 22:01:38.014151: precision = 0.588
After 6 training epoch, loss on training batch is 1.849291
2019-06-20 22:01:48.585660: precision = 0.596
After 7 training epoch, loss on training batch is 1.748487
2019-06-20 22:01:59.154413: precision = 0.594
After 8 training epoch, loss on training batch is 1.525828
2019-06-20 22:02:09.721137: precision = 0.593
After 9 training epoch, loss on training batch is 2.009336
2019-06-20 22:02:20.276839: precision = 0.586
After 10 training epoch, loss on training batch is 1.043833
2019-06-20 22:02:30.831936: precision = 0.566
After 11 training epoch, loss on training batch is 0.988243
2019-06-20 22:02:41.387091: precision = 0.575
After 12 training epoch, loss on training batch is 1.118981
2019-06-20 22:02:51.944467: precision = 0.571
After 13 training epoch, loss on training batch is 0.613636
2019-06-20 22:03:02.502685: precision = 0.581
After 14 training epoch, loss on training batch is 0.806164
2019-06-20 22:03:13.055441: precision = 0.575
After 15 training epoch, loss on training batch is 0.950210
2019-06-20 22:03:23.603475: precision = 0.575
After 16 training epoch, loss on training batch is 0.743959
2019-06-20 22:03:34.160022: precision = 0.557
After 17 training epoch, loss on training batch is 0.552870
2019-06-20 22:03:44.716735: precision = 0.581
After 18 training epoch, loss on training batch is 0.414795
2019-06-20 22:03:55.267585: precision = 0.583
After 19 training epoch, loss on training batch is 0.377371
2019-06-20 22:04:05.825004: precision = 0.584
After 20 training epoch, loss on training batch is 0.373371
2019-06-20 22:04:16.377762: precision = 0.569
After 21 training epoch, loss on training batch is 0.360381
2019-06-20 22:04:26.931049: precision = 0.571
After 22 training epoch, loss on training batch is 0.411886
2019-06-20 22:04:37.486784: precision = 0.581
After 23 training epoch, loss on training batch is 0.685312
2019-06-20 22:04:48.034370: precision = 0.567
After 24 training epoch, loss on training batch is 0.195206
2019-06-20 22:04:58.582196: precision = 0.551
After 25 training epoch, loss on training batch is 0.234069
2019-06-20 22:05:09.145026: precision = 0.569
After 26 training epoch, loss on training batch is 0.267801
2019-06-20 22:05:19.695444: precision = 0.577
After 27 training epoch, loss on training batch is 0.150695
2019-06-20 22:05:30.241303: precision = 0.572
After 28 training epoch, loss on training batch is 0.388715
2019-06-20 22:05:40.788722: precision = 0.577
After 29 training epoch, loss on training batch is 0.151043
2019-06-20 22:05:51.346331: precision = 0.586
After 30 training epoch, loss on training batch is 0.174936
2019-06-20 22:06:01.897189: precision = 0.584
After 31 training epoch, loss on training batch is 0.374835
2019-06-20 22:06:12.446503: precision = 0.575
After 32 training epoch, loss on training batch is 0.195264
2019-06-20 22:06:22.999119: precision = 0.567
After 33 training epoch, loss on training batch is 0.276965
2019-06-20 22:06:33.552638: precision = 0.580
After 34 training epoch, loss on training batch is 0.102311
2019-06-20 22:06:44.101109: precision = 0.585
After 35 training epoch, loss on training batch is 0.212211
2019-06-20 22:06:54.640904: precision = 0.583
After 36 training epoch, loss on training batch is 0.165697
2019-06-20 22:07:05.198588: precision = 0.580
After 37 training epoch, loss on training batch is 0.106729
2019-06-20 22:07:15.753360: precision = 0.582
After 38 training epoch, loss on training batch is 0.066182
2019-06-20 22:07:26.312016: precision = 0.578
After 39 training epoch, loss on training batch is 0.113583
2019-06-20 22:07:36.857968: precision = 0.590
After 40 training epoch, loss on training batch is 0.061530
2019-06-20 22:07:47.398077: precision = 0.588
After 41 training epoch, loss on training batch is 0.125014
2019-06-20 22:07:57.951991: precision = 0.585
After 42 training epoch, loss on training batch is 0.207523
2019-06-20 22:08:08.512013: precision = 0.588
After 43 training epoch, loss on training batch is 0.200325
2019-06-20 22:08:19.067928: precision = 0.583
After 44 training epoch, loss on training batch is 0.125375
2019-06-20 22:08:29.619940: precision = 0.587
After 45 training epoch, loss on training batch is 0.127976
2019-06-20 22:08:40.166919: precision = 0.576
After 46 training epoch, loss on training batch is 0.360433
2019-06-20 22:08:50.717456: precision = 0.588
After 47 training epoch, loss on training batch is 0.151143
2019-06-20 22:09:01.272163: precision = 0.585
After 48 training epoch, loss on training batch is 0.053471
2019-06-20 22:09:11.822269: precision = 0.584
After 49 training epoch, loss on training batch is 0.239824
2019-06-20 22:09:22.359997: precision = 0.587
After 50 training epoch, loss on training batch is 0.194330
2019-06-20 22:09:32.905460: precision = 0.587
After 1 training epoch, loss on training batch is 2.788677
2019-06-20 22:09:44.987087: precision = 0.484
After 2 training epoch, loss on training batch is 2.595987
2019-06-20 22:09:56.504814: precision = 0.532
After 3 training epoch, loss on training batch is 2.380419
2019-06-20 22:10:08.048586: precision = 0.563
After 4 training epoch, loss on training batch is 1.852920
2019-06-20 22:10:19.587172: precision = 0.580
After 5 training epoch, loss on training batch is 1.669653
2019-06-20 22:10:31.114186: precision = 0.590
After 6 training epoch, loss on training batch is 1.871907
2019-06-20 22:10:42.650668: precision = 0.598
After 7 training epoch, loss on training batch is 1.709895
2019-06-20 22:10:54.168601: precision = 0.589
After 8 training epoch, loss on training batch is 1.520003
2019-06-20 22:11:05.706770: precision = 0.574
After 9 training epoch, loss on training batch is 1.818100
2019-06-20 22:11:17.235025: precision = 0.579
After 10 training epoch, loss on training batch is 1.017888
2019-06-20 22:11:28.759419: precision = 0.585
After 11 training epoch, loss on training batch is 1.265002
2019-06-20 22:11:40.280475: precision = 0.553
After 12 training epoch, loss on training batch is 0.913595
2019-06-20 22:11:51.813641: precision = 0.572
After 13 training epoch, loss on training batch is 0.528574
2019-06-20 22:12:03.346525: precision = 0.586
After 14 training epoch, loss on training batch is 0.595774
2019-06-20 22:12:14.859345: precision = 0.582
After 15 training epoch, loss on training batch is 0.966310
2019-06-20 22:12:26.390833: precision = 0.557
After 16 training epoch, loss on training batch is 0.658638
2019-06-20 22:12:37.930744: precision = 0.561
After 17 training epoch, loss on training batch is 0.451897
2019-06-20 22:12:49.436354: precision = 0.567
After 18 training epoch, loss on training batch is 0.382420
2019-06-20 22:13:00.979337: precision = 0.565
After 19 training epoch, loss on training batch is 0.479883
2019-06-20 22:13:12.515965: precision = 0.563
After 20 training epoch, loss on training batch is 0.518923
2019-06-20 22:13:24.014400: precision = 0.571
After 21 training epoch, loss on training batch is 0.284712
2019-06-20 22:13:35.496271: precision = 0.557
After 22 training epoch, loss on training batch is 0.249438
2019-06-20 22:13:47.055094: precision = 0.580
After 23 training epoch, loss on training batch is 0.625308
2019-06-20 22:13:58.587940: precision = 0.575
After 24 training epoch, loss on training batch is 0.562774
2019-06-20 22:14:10.128908: precision = 0.572
After 25 training epoch, loss on training batch is 0.240026
2019-06-20 22:14:21.660107: precision = 0.574
After 26 training epoch, loss on training batch is 0.181121
2019-06-20 22:14:33.148714: precision = 0.589
After 27 training epoch, loss on training batch is 0.370457
2019-06-20 22:14:44.674321: precision = 0.575
After 28 training epoch, loss on training batch is 0.216659
2019-06-20 22:14:56.185390: precision = 0.582
After 29 training epoch, loss on training batch is 0.175477
2019-06-20 22:15:07.685699: precision = 0.586
After 30 training epoch, loss on training batch is 0.144790
2019-06-20 22:15:19.178113: precision = 0.583
After 31 training epoch, loss on training batch is 0.239265
2019-06-20 22:15:30.701485: precision = 0.588
After 32 training epoch, loss on training batch is 0.365337
2019-06-20 22:15:42.211186: precision = 0.590
After 33 training epoch, loss on training batch is 0.156127
2019-06-20 22:15:53.701370: precision = 0.584
After 34 training epoch, loss on training batch is 0.268931
2019-06-20 22:16:05.198098: precision = 0.592
After 35 training epoch, loss on training batch is 0.251778
2019-06-20 22:16:16.696784: precision = 0.577
After 36 training epoch, loss on training batch is 0.091384
2019-06-20 22:16:28.199352: precision = 0.569
After 37 training epoch, loss on training batch is 0.206227
2019-06-20 22:16:39.750408: precision = 0.579
After 38 training epoch, loss on training batch is 0.168923
2019-06-20 22:16:51.291266: precision = 0.568
After 39 training epoch, loss on training batch is 0.029608
2019-06-20 22:17:02.764986: precision = 0.578
After 40 training epoch, loss on training batch is 0.101369
2019-06-20 22:17:14.288767: precision = 0.584
After 41 training epoch, loss on training batch is 0.188550
2019-06-20 22:17:25.793205: precision = 0.588
After 42 training epoch, loss on training batch is 0.107338
2019-06-20 22:17:37.271140: precision = 0.587
After 43 training epoch, loss on training batch is 0.169917
2019-06-20 22:17:48.765753: precision = 0.582
After 44 training epoch, loss on training batch is 0.455395
2019-06-20 22:18:00.283221: precision = 0.583
After 45 training epoch, loss on training batch is 0.170115
2019-06-20 22:18:11.809996: precision = 0.576
After 46 training epoch, loss on training batch is 0.057546
2019-06-20 22:18:23.343561: precision = 0.578
After 47 training epoch, loss on training batch is 0.094896
2019-06-20 22:18:34.874129: precision = 0.585
After 48 training epoch, loss on training batch is 0.043258
2019-06-20 22:18:46.386065: precision = 0.587
After 49 training epoch, loss on training batch is 0.107051
2019-06-20 22:18:57.934156: precision = 0.584
After 50 training epoch, loss on training batch is 0.217051
2019-06-20 22:19:09.467668: precision = 0.577
After 1 training epoch, loss on training batch is 2.968499
2019-06-20 22:19:21.473832: precision = 0.487
After 2 training epoch, loss on training batch is 2.574055
2019-06-20 22:19:33.031374: precision = 0.547
After 3 training epoch, loss on training batch is 2.523512
2019-06-20 22:19:44.591048: precision = 0.574
After 4 training epoch, loss on training batch is 1.993473
2019-06-20 22:19:56.152516: precision = 0.585
After 5 training epoch, loss on training batch is 1.711495
2019-06-20 22:20:07.716470: precision = 0.600
After 6 training epoch, loss on training batch is 1.704961
2019-06-20 22:20:19.235982: precision = 0.598
After 7 training epoch, loss on training batch is 1.684171
2019-06-20 22:20:30.794928: precision = 0.590
After 8 training epoch, loss on training batch is 1.471509
2019-06-20 22:20:42.351584: precision = 0.594
After 9 training epoch, loss on training batch is 1.452746
2019-06-20 22:20:53.877154: precision = 0.596
After 10 training epoch, loss on training batch is 0.896827
2019-06-20 22:21:05.429099: precision = 0.594
After 11 training epoch, loss on training batch is 1.001178
2019-06-20 22:21:16.984577: precision = 0.592
After 12 training epoch, loss on training batch is 0.759921
2019-06-20 22:21:28.533461: precision = 0.591
After 13 training epoch, loss on training batch is 0.694601
2019-06-20 22:21:40.066771: precision = 0.583
After 14 training epoch, loss on training batch is 0.492752
2019-06-20 22:21:51.591356: precision = 0.563
After 15 training epoch, loss on training batch is 0.776524
2019-06-20 22:22:03.159964: precision = 0.573
After 16 training epoch, loss on training batch is 0.535182
2019-06-20 22:22:14.677459: precision = 0.566
After 17 training epoch, loss on training batch is 0.395500
2019-06-20 22:22:26.206657: precision = 0.575
After 18 training epoch, loss on training batch is 0.495615
2019-06-20 22:22:37.747786: precision = 0.584
After 19 training epoch, loss on training batch is 0.579476
2019-06-20 22:22:49.294943: precision = 0.581
After 20 training epoch, loss on training batch is 0.483467
2019-06-20 22:23:00.834198: precision = 0.570
After 21 training epoch, loss on training batch is 0.341430
2019-06-20 22:23:12.366220: precision = 0.579
After 22 training epoch, loss on training batch is 0.539983
2019-06-20 22:23:23.942760: precision = 0.578
After 23 training epoch, loss on training batch is 0.652138
2019-06-20 22:23:35.493942: precision = 0.589
After 24 training epoch, loss on training batch is 0.134538
2019-06-20 22:23:47.024328: precision = 0.591
After 25 training epoch, loss on training batch is 0.183638
2019-06-20 22:23:58.563614: precision = 0.599
After 26 training epoch, loss on training batch is 0.236724
2019-06-20 22:24:10.100190: precision = 0.590
After 27 training epoch, loss on training batch is 0.384608
2019-06-20 22:24:21.641915: precision = 0.601
After 28 training epoch, loss on training batch is 0.290535
2019-06-20 22:24:33.182111: precision = 0.588
After 29 training epoch, loss on training batch is 0.377047
2019-06-20 22:24:44.726586: precision = 0.587
After 30 training epoch, loss on training batch is 0.263157
2019-06-20 22:24:56.258741: precision = 0.591
After 31 training epoch, loss on training batch is 0.114091
2019-06-20 22:25:07.804741: precision = 0.583
After 32 training epoch, loss on training batch is 0.280540
2019-06-20 22:25:19.345471: precision = 0.588
After 33 training epoch, loss on training batch is 0.411274
2019-06-20 22:25:30.895577: precision = 0.579
After 34 training epoch, loss on training batch is 0.210815
2019-06-20 22:25:42.426869: precision = 0.579
After 35 training epoch, loss on training batch is 0.221547
2019-06-20 22:25:53.963726: precision = 0.566
After 36 training epoch, loss on training batch is 0.136819
2019-06-20 22:26:05.504638: precision = 0.567
After 37 training epoch, loss on training batch is 0.105191
2019-06-20 22:26:17.033212: precision = 0.587
After 38 training epoch, loss on training batch is 0.337930
2019-06-20 22:26:28.582533: precision = 0.588
After 39 training epoch, loss on training batch is 0.189214
2019-06-20 22:26:40.111222: precision = 0.586
After 40 training epoch, loss on training batch is 0.112122
2019-06-20 22:26:51.625290: precision = 0.589
After 41 training epoch, loss on training batch is 0.115561
2019-06-20 22:27:03.144974: precision = 0.593
After 42 training epoch, loss on training batch is 0.124268
2019-06-20 22:27:14.661727: precision = 0.595
After 43 training epoch, loss on training batch is 0.257785
2019-06-20 22:27:26.205128: precision = 0.590
After 44 training epoch, loss on training batch is 0.126070
2019-06-20 22:27:37.739300: precision = 0.586
After 45 training epoch, loss on training batch is 0.079074
2019-06-20 22:27:49.267919: precision = 0.590
After 46 training epoch, loss on training batch is 0.173558
2019-06-20 22:28:00.794544: precision = 0.592
After 47 training epoch, loss on training batch is 0.135635
2019-06-20 22:28:12.335677: precision = 0.583
After 48 training epoch, loss on training batch is 0.164580
2019-06-20 22:28:23.864363: precision = 0.591
After 49 training epoch, loss on training batch is 0.026230
2019-06-20 22:28:35.404281: precision = 0.596
After 50 training epoch, loss on training batch is 0.231346
2019-06-20 22:28:46.947556: precision = 0.593
After 1 training epoch, loss on training batch is 2.803775
2019-06-20 22:28:58.801047: precision = 0.473
After 2 training epoch, loss on training batch is 2.598541
2019-06-20 22:29:10.210987: precision = 0.542
After 3 training epoch, loss on training batch is 2.394460
2019-06-20 22:29:21.622813: precision = 0.556
After 4 training epoch, loss on training batch is 2.004603
2019-06-20 22:29:33.030320: precision = 0.581
After 5 training epoch, loss on training batch is 1.813163
2019-06-20 22:29:44.442178: precision = 0.598
After 6 training epoch, loss on training batch is 1.920063
2019-06-20 22:29:55.853998: precision = 0.605
After 7 training epoch, loss on training batch is 1.699759
2019-06-20 22:30:07.263158: precision = 0.575
After 8 training epoch, loss on training batch is 1.636853
2019-06-20 22:30:18.668008: precision = 0.584
After 9 training epoch, loss on training batch is 1.761953
2019-06-20 22:30:30.065532: precision = 0.585
After 10 training epoch, loss on training batch is 1.153971
2019-06-20 22:30:41.468741: precision = 0.585
After 11 training epoch, loss on training batch is 0.931370
2019-06-20 22:30:52.870154: precision = 0.585
After 12 training epoch, loss on training batch is 1.036386
2019-06-20 22:31:04.269642: precision = 0.598
After 13 training epoch, loss on training batch is 0.654819
2019-06-20 22:31:15.670245: precision = 0.590
After 14 training epoch, loss on training batch is 0.718295
2019-06-20 22:31:27.071630: precision = 0.575
After 15 training epoch, loss on training batch is 0.662299
2019-06-20 22:31:38.477502: precision = 0.572
After 16 training epoch, loss on training batch is 0.642987
2019-06-20 22:31:49.878752: precision = 0.576
After 17 training epoch, loss on training batch is 0.544938
2019-06-20 22:32:01.279340: precision = 0.590
After 18 training epoch, loss on training batch is 0.527643
2019-06-20 22:32:12.681658: precision = 0.584
After 19 training epoch, loss on training batch is 0.695090
2019-06-20 22:32:24.076556: precision = 0.573
After 20 training epoch, loss on training batch is 0.551679
2019-06-20 22:32:35.470610: precision = 0.567
After 21 training epoch, loss on training batch is 0.304603
2019-06-20 22:32:46.866891: precision = 0.580
After 22 training epoch, loss on training batch is 0.474606
2019-06-20 22:32:58.263250: precision = 0.585
After 23 training epoch, loss on training batch is 0.672619
2019-06-20 22:33:09.669217: precision = 0.583
After 24 training epoch, loss on training batch is 0.325941
2019-06-20 22:33:21.062647: precision = 0.574
After 25 training epoch, loss on training batch is 0.165207
2019-06-20 22:33:32.472434: precision = 0.583
After 26 training epoch, loss on training batch is 0.509606
2019-06-20 22:33:43.859996: precision = 0.585
After 27 training epoch, loss on training batch is 0.346746
2019-06-20 22:33:55.260385: precision = 0.583
After 28 training epoch, loss on training batch is 0.283199
2019-06-20 22:34:06.661609: precision = 0.583
After 29 training epoch, loss on training batch is 0.296747
2019-06-20 22:34:18.059684: precision = 0.574
After 30 training epoch, loss on training batch is 0.249025
2019-06-20 22:34:29.452559: precision = 0.581
After 31 training epoch, loss on training batch is 0.197158
2019-06-20 22:34:40.844782: precision = 0.581
After 32 training epoch, loss on training batch is 0.199285
2019-06-20 22:34:52.239743: precision = 0.574
After 33 training epoch, loss on training batch is 0.274411
2019-06-20 22:35:03.649577: precision = 0.574
After 34 training epoch, loss on training batch is 0.287244
2019-06-20 22:35:15.043652: precision = 0.583
After 35 training epoch, loss on training batch is 0.182666
2019-06-20 22:35:26.453871: precision = 0.581
After 36 training epoch, loss on training batch is 0.072645
2019-06-20 22:35:37.856338: precision = 0.589
After 37 training epoch, loss on training batch is 0.170480
2019-06-20 22:35:49.254580: precision = 0.586
After 38 training epoch, loss on training batch is 0.090972
2019-06-20 22:36:00.649520: precision = 0.590
After 39 training epoch, loss on training batch is 0.212132
2019-06-20 22:36:12.051138: precision = 0.594
After 40 training epoch, loss on training batch is 0.214793
2019-06-20 22:36:23.445984: precision = 0.587
After 41 training epoch, loss on training batch is 0.158253
2019-06-20 22:36:34.841732: precision = 0.586
After 42 training epoch, loss on training batch is 0.058795
2019-06-20 22:36:46.242816: precision = 0.590
After 43 training epoch, loss on training batch is 0.267621
2019-06-20 22:36:57.642888: precision = 0.587
After 44 training epoch, loss on training batch is 0.070833
2019-06-20 22:37:09.034696: precision = 0.583
After 45 training epoch, loss on training batch is 0.148009
2019-06-20 22:37:20.419726: precision = 0.582
After 46 training epoch, loss on training batch is 0.229997
2019-06-20 22:37:31.812233: precision = 0.583
After 47 training epoch, loss on training batch is 0.158054
2019-06-20 22:37:43.205520: precision = 0.576
After 48 training epoch, loss on training batch is 0.025628
2019-06-20 22:37:54.598807: precision = 0.586
After 49 training epoch, loss on training batch is 0.064517
2019-06-20 22:38:06.000981: precision = 0.591
After 50 training epoch, loss on training batch is 0.156616
2019-06-20 22:38:17.391028: precision = 0.586
After 1 training epoch, loss on training batch is 2.783405
2019-06-20 22:38:29.500664: precision = 0.480
After 2 training epoch, loss on training batch is 2.435897
2019-06-20 22:38:41.160302: precision = 0.536
After 3 training epoch, loss on training batch is 2.400325
2019-06-20 22:38:52.819847: precision = 0.569
After 4 training epoch, loss on training batch is 1.980219
2019-06-20 22:39:04.510240: precision = 0.585
After 5 training epoch, loss on training batch is 1.735418
2019-06-20 22:39:16.160355: precision = 0.582
After 6 training epoch, loss on training batch is 1.582861
2019-06-20 22:39:27.839915: precision = 0.594
After 7 training epoch, loss on training batch is 1.875604
2019-06-20 22:39:39.516702: precision = 0.595
After 8 training epoch, loss on training batch is 1.605917
2019-06-20 22:39:51.186930: precision = 0.600
After 9 training epoch, loss on training batch is 1.437972
2019-06-20 22:40:02.854061: precision = 0.584
After 10 training epoch, loss on training batch is 0.958339
2019-06-20 22:40:14.528666: precision = 0.587
After 11 training epoch, loss on training batch is 0.891512
2019-06-20 22:40:26.182950: precision = 0.594
After 12 training epoch, loss on training batch is 1.100502
2019-06-20 22:40:37.829790: precision = 0.573
After 13 training epoch, loss on training batch is 0.484102
2019-06-20 22:40:49.506174: precision = 0.569
After 14 training epoch, loss on training batch is 0.665919
2019-06-20 22:41:01.150124: precision = 0.570
After 15 training epoch, loss on training batch is 0.690255
2019-06-20 22:41:12.819136: precision = 0.580
After 16 training epoch, loss on training batch is 0.778454
2019-06-20 22:41:24.472223: precision = 0.543
After 17 training epoch, loss on training batch is 0.739760
2019-06-20 22:41:36.132212: precision = 0.566
After 18 training epoch, loss on training batch is 0.733920
2019-06-20 22:41:47.796544: precision = 0.581
After 19 training epoch, loss on training batch is 0.662094
2019-06-20 22:41:59.458668: precision = 0.572
After 20 training epoch, loss on training batch is 0.315691
2019-06-20 22:42:11.140048: precision = 0.571
After 21 training epoch, loss on training batch is 0.429132
2019-06-20 22:42:22.795228: precision = 0.565
After 22 training epoch, loss on training batch is 0.352753
2019-06-20 22:42:34.437300: precision = 0.568
After 23 training epoch, loss on training batch is 0.547116
2019-06-20 22:42:46.098729: precision = 0.587
After 24 training epoch, loss on training batch is 0.294370
2019-06-20 22:42:57.752605: precision = 0.590
After 25 training epoch, loss on training batch is 0.293619
2019-06-20 22:43:09.393960: precision = 0.586
After 26 training epoch, loss on training batch is 0.608016
2019-06-20 22:43:21.044902: precision = 0.589
After 27 training epoch, loss on training batch is 0.315959
2019-06-20 22:43:32.689174: precision = 0.573
After 28 training epoch, loss on training batch is 0.180223
2019-06-20 22:43:44.344633: precision = 0.589
After 29 training epoch, loss on training batch is 0.349468
2019-06-20 22:43:56.011701: precision = 0.582
After 30 training epoch, loss on training batch is 0.114060
2019-06-20 22:44:07.682335: precision = 0.584
After 31 training epoch, loss on training batch is 0.181237
2019-06-20 22:44:19.352666: precision = 0.579
After 32 training epoch, loss on training batch is 0.328276
2019-06-20 22:44:31.024574: precision = 0.583
After 33 training epoch, loss on training batch is 0.272844
2019-06-20 22:44:42.668744: precision = 0.577
After 34 training epoch, loss on training batch is 0.158391
2019-06-20 22:44:54.314222: precision = 0.576
After 35 training epoch, loss on training batch is 0.219125
2019-06-20 22:45:05.976525: precision = 0.589
After 36 training epoch, loss on training batch is 0.204622
2019-06-20 22:45:17.630352: precision = 0.598
After 37 training epoch, loss on training batch is 0.265717
2019-06-20 22:45:29.285112: precision = 0.592
After 38 training epoch, loss on training batch is 0.190638
2019-06-20 22:45:40.956849: precision = 0.591
After 39 training epoch, loss on training batch is 0.023382
2019-06-20 22:45:52.618807: precision = 0.592
After 40 training epoch, loss on training batch is 0.109195
2019-06-20 22:46:04.309104: precision = 0.590
After 41 training epoch, loss on training batch is 0.191353
2019-06-20 22:46:15.980390: precision = 0.587
After 42 training epoch, loss on training batch is 0.181021
2019-06-20 22:46:27.626184: precision = 0.588
After 43 training epoch, loss on training batch is 0.073785
2019-06-20 22:46:39.300017: precision = 0.591
After 44 training epoch, loss on training batch is 0.150376
2019-06-20 22:46:50.953093: precision = 0.589
After 45 training epoch, loss on training batch is 0.082959
2019-06-20 22:47:02.624040: precision = 0.592
After 46 training epoch, loss on training batch is 0.169096
2019-06-20 22:47:14.277651: precision = 0.590
After 47 training epoch, loss on training batch is 0.165494
2019-06-20 22:47:25.947944: precision = 0.589
After 48 training epoch, loss on training batch is 0.053762
2019-06-20 22:47:37.593602: precision = 0.585
After 49 training epoch, loss on training batch is 0.120851
2019-06-20 22:47:49.255431: precision = 0.597
After 50 training epoch, loss on training batch is 0.190791
2019-06-20 22:48:00.912290: precision = 0.582
After 1 training epoch, loss on training batch is 2.635866
2019-06-20 22:48:12.577271: precision = 0.488
After 2 training epoch, loss on training batch is 2.576018
2019-06-20 22:48:23.709641: precision = 0.555
After 3 training epoch, loss on training batch is 2.458612
2019-06-20 22:48:34.801521: precision = 0.556
After 4 training epoch, loss on training batch is 1.950374
2019-06-20 22:48:45.844208: precision = 0.578
After 5 training epoch, loss on training batch is 1.670674
2019-06-20 22:48:56.923530: precision = 0.586
After 6 training epoch, loss on training batch is 1.800774
2019-06-20 22:49:07.996591: precision = 0.585
After 7 training epoch, loss on training batch is 1.656864
2019-06-20 22:49:19.061397: precision = 0.584
After 8 training epoch, loss on training batch is 1.547878
2019-06-20 22:49:30.131747: precision = 0.582
After 9 training epoch, loss on training batch is 1.618135
2019-06-20 22:49:41.182005: precision = 0.564
After 10 training epoch, loss on training batch is 0.890490
2019-06-20 22:49:52.275227: precision = 0.575
After 11 training epoch, loss on training batch is 1.038228
2019-06-20 22:50:03.342988: precision = 0.573
After 12 training epoch, loss on training batch is 0.775554
2019-06-20 22:50:14.429444: precision = 0.579
After 13 training epoch, loss on training batch is 0.533115
2019-06-20 22:50:25.518642: precision = 0.591
After 14 training epoch, loss on training batch is 0.583197
2019-06-20 22:50:36.579301: precision = 0.583
After 15 training epoch, loss on training batch is 0.871212
2019-06-20 22:50:47.649933: precision = 0.580
After 16 training epoch, loss on training batch is 0.998657
2019-06-20 22:50:58.702683: precision = 0.552
After 17 training epoch, loss on training batch is 0.435558
2019-06-20 22:51:09.787816: precision = 0.576
After 18 training epoch, loss on training batch is 0.760254
2019-06-20 22:51:20.871411: precision = 0.573
After 19 training epoch, loss on training batch is 0.436741
2019-06-20 22:51:31.929149: precision = 0.566
After 20 training epoch, loss on training batch is 0.346785
2019-06-20 22:51:42.998877: precision = 0.565
After 21 training epoch, loss on training batch is 0.302843
2019-06-20 22:51:54.061255: precision = 0.585
After 22 training epoch, loss on training batch is 0.335086
2019-06-20 22:52:05.163384: precision = 0.580
After 23 training epoch, loss on training batch is 0.663284
2019-06-20 22:52:16.242367: precision = 0.568
After 24 training epoch, loss on training batch is 0.315943
2019-06-20 22:52:27.348532: precision = 0.567
After 25 training epoch, loss on training batch is 0.239488
2019-06-20 22:52:38.438473: precision = 0.573
After 26 training epoch, loss on training batch is 0.397769
2019-06-20 22:52:49.480594: precision = 0.584
After 27 training epoch, loss on training batch is 0.255623
2019-06-20 22:53:00.522707: precision = 0.588
After 28 training epoch, loss on training batch is 0.286909
2019-06-20 22:53:11.634554: precision = 0.591
After 29 training epoch, loss on training batch is 0.155190
2019-06-20 22:53:22.699219: precision = 0.583
After 30 training epoch, loss on training batch is 0.164422
2019-06-20 22:53:33.812092: precision = 0.583
After 31 training epoch, loss on training batch is 0.106764
2019-06-20 22:53:44.912758: precision = 0.589
After 32 training epoch, loss on training batch is 0.322738
2019-06-20 22:53:55.967321: precision = 0.583
After 33 training epoch, loss on training batch is 0.175416
2019-06-20 22:54:07.026426: precision = 0.587
After 34 training epoch, loss on training batch is 0.229049
2019-06-20 22:54:18.118129: precision = 0.585
After 35 training epoch, loss on training batch is 0.212592
2019-06-20 22:54:29.162328: precision = 0.584
After 36 training epoch, loss on training batch is 0.282236
2019-06-20 22:54:40.236392: precision = 0.569
After 37 training epoch, loss on training batch is 0.116477
2019-06-20 22:54:51.330035: precision = 0.572
After 38 training epoch, loss on training batch is 0.222612
2019-06-20 22:55:02.423726: precision = 0.573
After 39 training epoch, loss on training batch is 0.217807
2019-06-20 22:55:13.525414: precision = 0.576
After 40 training epoch, loss on training batch is 0.165640
2019-06-20 22:55:24.564500: precision = 0.587
After 41 training epoch, loss on training batch is 0.193459
2019-06-20 22:55:35.634547: precision = 0.580
After 42 training epoch, loss on training batch is 0.222965
2019-06-20 22:55:46.705205: precision = 0.587
After 43 training epoch, loss on training batch is 0.160445
2019-06-20 22:55:57.765506: precision = 0.587
After 44 training epoch, loss on training batch is 0.122844
2019-06-20 22:56:08.828636: precision = 0.587
After 45 training epoch, loss on training batch is 0.096215
2019-06-20 22:56:19.889405: precision = 0.586
After 46 training epoch, loss on training batch is 0.024633
2019-06-20 22:56:30.954812: precision = 0.597
After 47 training epoch, loss on training batch is 0.062298
2019-06-20 22:56:42.035907: precision = 0.586
After 48 training epoch, loss on training batch is 0.137611
2019-06-20 22:56:53.088541: precision = 0.587
After 49 training epoch, loss on training batch is 0.133279
2019-06-20 22:57:04.141819: precision = 0.600
After 50 training epoch, loss on training batch is 0.119925
2019-06-20 22:57:15.193700: precision = 0.586
After 1 training epoch, loss on training batch is 2.683915
2019-06-20 22:57:26.921059: precision = 0.481
After 2 training epoch, loss on training batch is 2.477468
2019-06-20 22:57:38.199850: precision = 0.539
After 3 training epoch, loss on training batch is 2.233744
2019-06-20 22:57:49.471296: precision = 0.568
After 4 training epoch, loss on training batch is 1.924209
2019-06-20 22:58:00.756244: precision = 0.579
After 5 training epoch, loss on training batch is 1.622760
2019-06-20 22:58:12.024028: precision = 0.587
After 6 training epoch, loss on training batch is 1.812446
2019-06-20 22:58:23.303380: precision = 0.580
After 7 training epoch, loss on training batch is 1.536731
2019-06-20 22:58:34.577300: precision = 0.590
After 8 training epoch, loss on training batch is 1.631840
2019-06-20 22:58:45.863397: precision = 0.585
After 9 training epoch, loss on training batch is 1.395686
2019-06-20 22:58:57.134835: precision = 0.591
After 10 training epoch, loss on training batch is 0.809567
2019-06-20 22:59:08.405121: precision = 0.590
After 11 training epoch, loss on training batch is 0.798597
2019-06-20 22:59:19.671776: precision = 0.586
After 12 training epoch, loss on training batch is 0.952984
2019-06-20 22:59:30.952107: precision = 0.584
After 13 training epoch, loss on training batch is 0.788208
2019-06-20 22:59:42.222543: precision = 0.586
After 14 training epoch, loss on training batch is 0.375137
2019-06-20 22:59:53.498303: precision = 0.569
After 15 training epoch, loss on training batch is 0.600391
2019-06-20 23:00:04.768573: precision = 0.577
After 16 training epoch, loss on training batch is 0.389274
2019-06-20 23:00:16.035530: precision = 0.588
After 17 training epoch, loss on training batch is 0.779409
2019-06-20 23:00:27.305661: precision = 0.571
After 18 training epoch, loss on training batch is 0.646806
2019-06-20 23:00:38.572591: precision = 0.563
After 19 training epoch, loss on training batch is 0.394614
2019-06-20 23:00:49.837553: precision = 0.576
After 20 training epoch, loss on training batch is 0.227263
2019-06-20 23:01:01.104073: precision = 0.583
After 21 training epoch, loss on training batch is 0.303641
2019-06-20 23:01:12.372951: precision = 0.574
After 22 training epoch, loss on training batch is 0.380862
2019-06-20 23:01:23.644959: precision = 0.584
After 23 training epoch, loss on training batch is 0.157305
2019-06-20 23:01:34.911431: precision = 0.583
After 24 training epoch, loss on training batch is 0.120495
2019-06-20 23:01:46.182697: precision = 0.589
After 25 training epoch, loss on training batch is 0.167865
2019-06-20 23:01:57.455211: precision = 0.587
After 26 training epoch, loss on training batch is 0.176202
2019-06-20 23:02:08.729621: precision = 0.584
After 27 training epoch, loss on training batch is 0.254628
2019-06-20 23:02:19.997294: precision = 0.601
After 28 training epoch, loss on training batch is 0.220545
2019-06-20 23:02:31.261932: precision = 0.593
After 29 training epoch, loss on training batch is 0.090026
2019-06-20 23:02:42.532028: precision = 0.598
After 30 training epoch, loss on training batch is 0.711426
2019-06-20 23:02:53.796673: precision = 0.585
After 31 training epoch, loss on training batch is 0.319289
2019-06-20 23:03:05.067223: precision = 0.583
After 32 training epoch, loss on training batch is 0.244416
2019-06-20 23:03:16.340214: precision = 0.582
After 33 training epoch, loss on training batch is 0.170806
2019-06-20 23:03:27.613003: precision = 0.588
After 34 training epoch, loss on training batch is 0.124486
2019-06-20 23:03:38.872898: precision = 0.585
After 35 training epoch, loss on training batch is 0.050618
2019-06-20 23:03:50.141512: precision = 0.587
After 36 training epoch, loss on training batch is 0.129168
2019-06-20 23:04:01.401150: precision = 0.586
After 37 training epoch, loss on training batch is 0.082567
2019-06-20 23:04:12.664430: precision = 0.596
After 38 training epoch, loss on training batch is 0.031104
2019-06-20 23:04:23.924271: precision = 0.589
After 39 training epoch, loss on training batch is 0.143510
2019-06-20 23:04:35.183080: precision = 0.601
After 40 training epoch, loss on training batch is 0.133477
2019-06-20 23:04:46.439326: precision = 0.610
After 41 training epoch, loss on training batch is 0.161432
2019-06-20 23:04:57.710739: precision = 0.606
After 42 training epoch, loss on training batch is 0.030554
2019-06-20 23:05:08.981915: precision = 0.601
After 43 training epoch, loss on training batch is 0.049795
2019-06-20 23:05:20.239875: precision = 0.596
After 44 training epoch, loss on training batch is 0.180788
2019-06-20 23:05:31.499335: precision = 0.599
After 45 training epoch, loss on training batch is 0.230186
2019-06-20 23:05:42.758377: precision = 0.598
After 46 training epoch, loss on training batch is 0.169680
2019-06-20 23:05:54.017389: precision = 0.598
After 47 training epoch, loss on training batch is 0.066766
2019-06-20 23:06:05.279877: precision = 0.595
After 48 training epoch, loss on training batch is 0.065796
2019-06-20 23:06:16.537728: precision = 0.589
After 49 training epoch, loss on training batch is 0.152365
2019-06-20 23:06:27.799100: precision = 0.589
After 50 training epoch, loss on training batch is 0.226190
2019-06-20 23:06:39.064457: precision = 0.596
After 1 training epoch, loss on training batch is 2.715602
2019-06-20 23:06:51.004019: precision = 0.489
After 2 training epoch, loss on training batch is 2.630707
2019-06-20 23:07:02.527670: precision = 0.534
After 3 training epoch, loss on training batch is 2.316072
2019-06-20 23:07:14.029654: precision = 0.576
After 4 training epoch, loss on training batch is 1.838636
2019-06-20 23:07:25.553358: precision = 0.572
After 5 training epoch, loss on training batch is 1.622432
2019-06-20 23:07:37.066324: precision = 0.593
After 6 training epoch, loss on training batch is 1.956377
2019-06-20 23:07:48.605004: precision = 0.598
After 7 training epoch, loss on training batch is 1.525191
2019-06-20 23:08:00.131541: precision = 0.588
After 8 training epoch, loss on training batch is 1.822845
2019-06-20 23:08:11.682257: precision = 0.586
After 9 training epoch, loss on training batch is 1.826271
2019-06-20 23:08:23.217455: precision = 0.598
After 10 training epoch, loss on training batch is 0.903258
2019-06-20 23:08:34.728081: precision = 0.583
After 11 training epoch, loss on training batch is 0.978395
2019-06-20 23:08:46.228350: precision = 0.596
After 12 training epoch, loss on training batch is 1.111896
2019-06-20 23:08:57.747097: precision = 0.585
After 13 training epoch, loss on training batch is 0.556392
2019-06-20 23:09:09.290980: precision = 0.578
After 14 training epoch, loss on training batch is 0.468391
2019-06-20 23:09:20.801628: precision = 0.584
After 15 training epoch, loss on training batch is 0.949711
2019-06-20 23:09:32.301856: precision = 0.583
After 16 training epoch, loss on training batch is 0.749299
2019-06-20 23:09:43.805033: precision = 0.575
After 17 training epoch, loss on training batch is 0.411218
2019-06-20 23:09:55.342188: precision = 0.585
After 18 training epoch, loss on training batch is 0.489090
2019-06-20 23:10:06.846392: precision = 0.576
After 19 training epoch, loss on training batch is 0.572024
2019-06-20 23:10:18.382765: precision = 0.586
After 20 training epoch, loss on training batch is 0.472500
2019-06-20 23:10:29.919954: precision = 0.578
After 21 training epoch, loss on training batch is 0.522264
2019-06-20 23:10:41.427350: precision = 0.568
After 22 training epoch, loss on training batch is 0.424861
2019-06-20 23:10:52.929982: precision = 0.584
After 23 training epoch, loss on training batch is 0.322691
2019-06-20 23:11:04.433908: precision = 0.587
After 24 training epoch, loss on training batch is 0.325395
2019-06-20 23:11:15.943627: precision = 0.586
After 25 training epoch, loss on training batch is 0.117323
2019-06-20 23:11:27.475541: precision = 0.589
After 26 training epoch, loss on training batch is 0.178074
2019-06-20 23:11:38.985858: precision = 0.589
After 27 training epoch, loss on training batch is 0.168164
2019-06-20 23:11:50.512261: precision = 0.593
After 28 training epoch, loss on training batch is 0.224299
2019-06-20 23:12:02.041229: precision = 0.589
After 29 training epoch, loss on training batch is 0.199328
2019-06-20 23:12:13.531582: precision = 0.584
After 30 training epoch, loss on training batch is 0.129868
2019-06-20 23:12:25.027379: precision = 0.591
After 31 training epoch, loss on training batch is 0.234836
2019-06-20 23:12:36.549710: precision = 0.596
After 32 training epoch, loss on training batch is 0.111658
2019-06-20 23:12:48.057370: precision = 0.600
After 33 training epoch, loss on training batch is 0.105793
2019-06-20 23:12:59.585401: precision = 0.598
After 34 training epoch, loss on training batch is 0.183940
2019-06-20 23:13:11.078297: precision = 0.590
After 35 training epoch, loss on training batch is 0.210185
2019-06-20 23:13:22.613292: precision = 0.595
After 36 training epoch, loss on training batch is 0.128167
2019-06-20 23:13:34.133401: precision = 0.592
After 37 training epoch, loss on training batch is 0.127760
2019-06-20 23:13:45.649965: precision = 0.583
After 38 training epoch, loss on training batch is 0.127243
2019-06-20 23:13:57.153008: precision = 0.598
After 39 training epoch, loss on training batch is 0.136879
2019-06-20 23:14:08.676844: precision = 0.588
After 40 training epoch, loss on training batch is 0.115123
2019-06-20 23:14:20.200186: precision = 0.594
After 41 training epoch, loss on training batch is 0.086266
2019-06-20 23:14:31.703573: precision = 0.595
After 42 training epoch, loss on training batch is 0.053254
2019-06-20 23:14:43.191607: precision = 0.589
After 43 training epoch, loss on training batch is 0.220894
2019-06-20 23:14:54.707083: precision = 0.586
After 44 training epoch, loss on training batch is 0.080816
2019-06-20 23:15:06.228523: precision = 0.592
After 45 training epoch, loss on training batch is 0.109892
2019-06-20 23:15:17.757222: precision = 0.593
After 46 training epoch, loss on training batch is 0.169269
2019-06-20 23:15:29.276030: precision = 0.594
After 47 training epoch, loss on training batch is 0.207813
2019-06-20 23:15:40.778810: precision = 0.587
After 48 training epoch, loss on training batch is 0.106268
2019-06-20 23:15:52.290961: precision = 0.597
After 49 training epoch, loss on training batch is 0.062074
2019-06-20 23:16:03.823825: precision = 0.602
After 50 training epoch, loss on training batch is 0.130338
2019-06-20 23:16:15.330929: precision = 0.602
After 1 training epoch, loss on training batch is 2.802612
2019-06-20 23:16:27.040430: precision = 0.477
After 2 training epoch, loss on training batch is 2.466077
2019-06-20 23:16:38.309519: precision = 0.539
After 3 training epoch, loss on training batch is 2.378845
2019-06-20 23:16:49.584884: precision = 0.556
After 4 training epoch, loss on training batch is 1.962976
2019-06-20 23:17:00.856672: precision = 0.580
After 5 training epoch, loss on training batch is 1.751155
2019-06-20 23:17:12.122258: precision = 0.577
After 6 training epoch, loss on training batch is 1.957910
2019-06-20 23:17:23.389081: precision = 0.598
After 7 training epoch, loss on training batch is 1.636781
2019-06-20 23:17:34.667943: precision = 0.594
After 8 training epoch, loss on training batch is 1.664155
2019-06-20 23:17:45.929527: precision = 0.591
After 9 training epoch, loss on training batch is 1.653005
2019-06-20 23:17:57.191620: precision = 0.589
After 10 training epoch, loss on training batch is 1.202877
2019-06-20 23:18:08.462852: precision = 0.582
After 11 training epoch, loss on training batch is 1.000465
2019-06-20 23:18:19.724774: precision = 0.592
After 12 training epoch, loss on training batch is 1.021116
2019-06-20 23:18:30.984418: precision = 0.595
After 13 training epoch, loss on training batch is 0.660924
2019-06-20 23:18:42.247031: precision = 0.598
After 14 training epoch, loss on training batch is 0.588518
2019-06-20 23:18:53.513716: precision = 0.595
After 15 training epoch, loss on training batch is 0.830242
2019-06-20 23:19:04.772022: precision = 0.574
After 16 training epoch, loss on training batch is 0.633486
2019-06-20 23:19:16.027980: precision = 0.579
After 17 training epoch, loss on training batch is 0.416644
2019-06-20 23:19:27.291504: precision = 0.578
After 18 training epoch, loss on training batch is 0.453544
2019-06-20 23:19:38.555790: precision = 0.583
After 19 training epoch, loss on training batch is 0.322930
2019-06-20 23:19:49.816682: precision = 0.570
After 20 training epoch, loss on training batch is 0.330764
2019-06-20 23:20:01.079039: precision = 0.580
After 21 training epoch, loss on training batch is 0.566379
2019-06-20 23:20:12.336687: precision = 0.593
After 22 training epoch, loss on training batch is 0.364359
2019-06-20 23:20:23.600430: precision = 0.589
After 23 training epoch, loss on training batch is 0.416799
2019-06-20 23:20:34.861662: precision = 0.569
After 24 training epoch, loss on training batch is 0.211039
2019-06-20 23:20:46.112560: precision = 0.588
After 25 training epoch, loss on training batch is 0.218735
2019-06-20 23:20:57.369060: precision = 0.598
After 26 training epoch, loss on training batch is 0.170732
2019-06-20 23:21:08.633052: precision = 0.593
After 27 training epoch, loss on training batch is 0.338072
2019-06-20 23:21:19.893927: precision = 0.587
After 28 training epoch, loss on training batch is 0.248025
2019-06-20 23:21:31.143648: precision = 0.593
After 29 training epoch, loss on training batch is 0.125545
2019-06-20 23:21:42.399075: precision = 0.597
After 30 training epoch, loss on training batch is 0.185296
2019-06-20 23:21:53.648977: precision = 0.594
After 31 training epoch, loss on training batch is 0.318055
2019-06-20 23:22:04.908207: precision = 0.601
After 32 training epoch, loss on training batch is 0.168935
2019-06-20 23:22:16.168771: precision = 0.592
After 33 training epoch, loss on training batch is 0.186951
2019-06-20 23:22:27.432209: precision = 0.594
After 34 training epoch, loss on training batch is 0.037999
2019-06-20 23:22:38.684143: precision = 0.584
After 35 training epoch, loss on training batch is 0.171654
2019-06-20 23:22:49.945323: precision = 0.592
After 36 training epoch, loss on training batch is 0.092501
2019-06-20 23:23:01.207065: precision = 0.585
After 37 training epoch, loss on training batch is 0.111667
2019-06-20 23:23:12.464813: precision = 0.598
After 38 training epoch, loss on training batch is 0.153232
2019-06-20 23:23:23.711155: precision = 0.593
After 39 training epoch, loss on training batch is 0.513634
2019-06-20 23:23:34.958899: precision = 0.586
After 40 training epoch, loss on training batch is 0.169627
2019-06-20 23:23:46.207184: precision = 0.592
After 41 training epoch, loss on training batch is 0.210770
2019-06-20 23:23:57.463199: precision = 0.602
After 42 training epoch, loss on training batch is 0.049922
2019-06-20 23:24:08.715353: precision = 0.603
After 43 training epoch, loss on training batch is 0.108732
2019-06-20 23:24:19.972074: precision = 0.599
After 44 training epoch, loss on training batch is 0.100097
2019-06-20 23:24:31.228185: precision = 0.603
After 45 training epoch, loss on training batch is 0.035796
2019-06-20 23:24:42.475269: precision = 0.604
After 46 training epoch, loss on training batch is 0.065917
2019-06-20 23:24:53.732482: precision = 0.599
After 47 training epoch, loss on training batch is 0.023270
2019-06-20 23:25:04.985927: precision = 0.594
After 48 training epoch, loss on training batch is 0.293606
2019-06-20 23:25:16.242853: precision = 0.599
After 49 training epoch, loss on training batch is 0.119224
2019-06-20 23:25:27.501704: precision = 0.603
After 50 training epoch, loss on training batch is 0.133453
2019-06-20 23:25:38.754548: precision = 0.606
After 1 training epoch, loss on training batch is 2.843569
2019-06-20 23:25:49.893337: precision = 0.486
After 2 training epoch, loss on training batch is 2.417114
2019-06-20 23:26:00.592320: precision = 0.540
After 3 training epoch, loss on training batch is 2.358730
2019-06-20 23:26:11.285302: precision = 0.562
After 4 training epoch, loss on training batch is 1.754414
2019-06-20 23:26:21.969830: precision = 0.580
After 5 training epoch, loss on training batch is 1.599295
2019-06-20 23:26:32.649797: precision = 0.591
After 6 training epoch, loss on training batch is 1.811673
2019-06-20 23:26:43.339588: precision = 0.586
After 7 training epoch, loss on training batch is 2.078643
2019-06-20 23:26:54.023303: precision = 0.575
After 8 training epoch, loss on training batch is 1.567160
2019-06-20 23:27:04.719567: precision = 0.580
After 9 training epoch, loss on training batch is 1.447774
2019-06-20 23:27:15.413314: precision = 0.575
After 10 training epoch, loss on training batch is 1.009039
2019-06-20 23:27:26.095935: precision = 0.579
After 11 training epoch, loss on training batch is 1.162287
2019-06-20 23:27:36.779853: precision = 0.570
After 12 training epoch, loss on training batch is 0.820283
2019-06-20 23:27:47.468990: precision = 0.571
After 13 training epoch, loss on training batch is 0.601950
2019-06-20 23:27:58.161497: precision = 0.582
After 14 training epoch, loss on training batch is 0.540935
2019-06-20 23:28:08.839666: precision = 0.588
After 15 training epoch, loss on training batch is 0.744792
2019-06-20 23:28:19.520669: precision = 0.595
After 16 training epoch, loss on training batch is 0.784557
2019-06-20 23:28:30.207684: precision = 0.584
After 17 training epoch, loss on training batch is 0.611018
2019-06-20 23:28:40.881900: precision = 0.581
After 18 training epoch, loss on training batch is 0.422023
2019-06-20 23:28:51.571322: precision = 0.568
After 19 training epoch, loss on training batch is 0.476555
2019-06-20 23:29:02.264138: precision = 0.567
After 20 training epoch, loss on training batch is 0.267187
2019-06-20 23:29:12.955121: precision = 0.566
After 21 training epoch, loss on training batch is 0.565639
2019-06-20 23:29:23.638002: precision = 0.578
After 22 training epoch, loss on training batch is 0.331279
2019-06-20 23:29:34.309967: precision = 0.576
After 23 training epoch, loss on training batch is 0.157567
2019-06-20 23:29:44.982435: precision = 0.573
After 24 training epoch, loss on training batch is 0.300779
2019-06-20 23:29:55.655164: precision = 0.570
After 25 training epoch, loss on training batch is 0.155085
2019-06-20 23:30:06.340590: precision = 0.572
After 26 training epoch, loss on training batch is 0.194204
2019-06-20 23:30:17.017800: precision = 0.580
After 27 training epoch, loss on training batch is 0.403874
2019-06-20 23:30:27.693097: precision = 0.577
After 28 training epoch, loss on training batch is 0.301251
2019-06-20 23:30:38.369500: precision = 0.578
After 29 training epoch, loss on training batch is 0.242245
2019-06-20 23:30:49.049623: precision = 0.580
After 30 training epoch, loss on training batch is 0.144102
2019-06-20 23:30:59.733426: precision = 0.585
After 31 training epoch, loss on training batch is 0.142575
2019-06-20 23:31:10.414776: precision = 0.579
After 32 training epoch, loss on training batch is 0.182519
2019-06-20 23:31:21.090475: precision = 0.581
After 33 training epoch, loss on training batch is 0.211518
2019-06-20 23:31:31.759426: precision = 0.581
After 34 training epoch, loss on training batch is 0.252449
2019-06-20 23:31:42.441665: precision = 0.586
After 35 training epoch, loss on training batch is 0.241283
2019-06-20 23:31:53.119163: precision = 0.581
After 36 training epoch, loss on training batch is 0.244741
2019-06-20 23:32:03.800055: precision = 0.573
After 37 training epoch, loss on training batch is 0.110213
2019-06-20 23:32:14.467799: precision = 0.576
After 38 training epoch, loss on training batch is 0.136935
2019-06-20 23:32:25.148909: precision = 0.583
After 39 training epoch, loss on training batch is 0.061106
2019-06-20 23:32:35.835301: precision = 0.590
After 40 training epoch, loss on training batch is 0.169431
2019-06-20 23:32:46.512558: precision = 0.587
After 41 training epoch, loss on training batch is 0.189373
2019-06-20 23:32:57.177349: precision = 0.586
After 42 training epoch, loss on training batch is 0.113656
2019-06-20 23:33:07.860238: precision = 0.579
After 43 training epoch, loss on training batch is 0.057267
2019-06-20 23:33:18.538502: precision = 0.582
After 44 training epoch, loss on training batch is 0.430748
2019-06-20 23:33:29.209936: precision = 0.580
After 45 training epoch, loss on training batch is 0.037855
2019-06-20 23:33:39.891544: precision = 0.587
After 46 training epoch, loss on training batch is 0.259214
2019-06-20 23:33:50.572163: precision = 0.590
After 47 training epoch, loss on training batch is 0.083808
2019-06-20 23:34:01.253080: precision = 0.598
After 48 training epoch, loss on training batch is 0.113306
2019-06-20 23:34:11.919110: precision = 0.593
After 49 training epoch, loss on training batch is 0.100212
2019-06-20 23:34:22.593028: precision = 0.592
After 50 training epoch, loss on training batch is 0.247712
2019-06-20 23:34:33.259224: precision = 0.595
[[0.2519030448717949, 0.33193108974358976, 0.36868990384615385, 0.3838141025641026, 0.4087540064102564, 0.40995592948717946, 0.4198717948717949, 0.4299879807692308, 0.42638221153846156, 0.41967147435897434, 0.41195913461538464, 0.4007411858974359, 0.3952323717948718, 0.39513221153846156, 0.41005608974358976, 0.41035657051282054, 0.41776842948717946, 0.4053485576923077, 0.42107371794871795, 0.4114583333333333, 0.40675080128205127, 0.4189703525641026, 0.4053485576923077, 0.40735176282051283, 0.41947115384615385, 0.4267828525641026, 0.43078926282051283, 0.4254807692307692, 0.4299879807692308, 0.4293870192307692, 0.43810096153846156, 0.4345953525641026, 0.4016426282051282, 0.43139022435897434, 0.4404046474358974, 0.44150641025641024, 0.4441105769230769, 0.44541266025641024, 0.44551282051282054, 0.4463141025641026, 0.4467147435897436, 0.4460136217948718, 0.44701522435897434, 0.44711538461538464, 0.44701522435897434, 0.4465144230769231, 0.4467147435897436, 0.44701522435897434, 0.4466145833333333, 0.4467147435897436], [0.3473557692307692, 0.42397836538461536, 0.4388020833333333, 0.45372596153846156, 0.4619391025641026, 0.47435897435897434, 0.45102163461538464, 0.38721955128205127, 0.4573317307692308, 0.46544471153846156, 0.4540264423076923, 0.4785657051282051, 0.45492788461538464, 0.4628405448717949, 0.4512219551282051, 0.465244391025641, 0.445713141025641, 0.4641426282051282, 0.4667467948717949, 0.476963141025641, 0.44701522435897434, 0.4478165064102564, 0.47025240384615385, 0.456630608974359, 0.4684495192307692, 0.47896634615384615, 0.47636217948717946, 0.4716546474358974, 0.4710536858974359, 0.4830729166666667, 0.45622996794871795, 0.48447516025641024, 0.49068509615384615, 0.49499198717948717, 0.4733573717948718, 0.4827724358974359, 0.48267227564102566, 0.4905849358974359, 0.48547676282051283, 0.48647836538461536, 0.499599358974359, 0.503104967948718, 0.5014022435897436, 0.5071113782051282, 0.5069110576923077, 0.5092147435897436, 0.5099158653846154, 0.5101161858974359, 0.5103165064102564, 0.5108173076923077], [0.4121594551282051, 0.45723157051282054, 0.4930889423076923, 0.4990985576923077, 0.5112179487179487, 0.5235376602564102, 0.4969951923076923, 0.4895833333333333, 0.5051081730769231, 0.4807692307692308, 0.5017027243589743, 0.5053084935897436, 0.48267227564102566, 0.5068108974358975, 0.492588141025641, 0.5014022435897436, 0.4794671474358974, 0.4924879807692308, 0.5150240384615384, 0.4852764423076923, 0.47866586538461536, 0.5014022435897436, 0.516426282051282, 0.5193309294871795, 0.5179286858974359, 0.51171875, 0.5183293269230769, 0.5228365384615384, 0.5166266025641025, 0.5193309294871795, 0.5151241987179487, 0.5244391025641025, 0.5169270833333334, 0.5222355769230769, 0.5186298076923077, 0.5234375, 0.5180288461538461, 0.5232371794871795, 0.5259415064102564, 0.5192307692307693, 0.5155248397435898, 0.5116185897435898, 0.5155248397435898, 0.50390625, 0.5040064102564102, 0.5034054487179487, 0.5069110576923077, 0.4970953525641026, 0.5040064102564102, 0.5060096153846154], [0.4473157051282051, 0.49849759615384615, 0.5316506410256411, 0.5462740384615384, 0.5509815705128205, 0.5697115384615384, 0.5601963141025641, 0.5465745192307693, 0.5336538461538461, 0.5407652243589743, 0.5482772435897436, 0.5358573717948718, 0.5335536858974359, 0.4992988782051282, 0.5063100961538461, 0.5400641025641025, 0.5417668269230769, 0.5282451923076923, 0.5391626602564102, 0.5388621794871795, 0.5347556089743589, 0.535957532051282, 0.5409655448717948, 0.5350560897435898, 0.5308493589743589, 0.5301482371794872, 0.5508814102564102, 0.5567908653846154, 0.5475761217948718, 0.5370592948717948, 0.5352564102564102, 0.5235376602564102, 0.5371594551282052, 0.5380608974358975, 0.5458733974358975, 0.5405649038461539, 0.5386618589743589, 0.5528846153846154, 0.5520833333333334, 0.5443709935897436, 0.5444711538461539, 0.5505809294871795, 0.5296474358974359, 0.5393629807692307, 0.5317508012820513, 0.5400641025641025, 0.5471754807692307, 0.5442708333333334, 0.5467748397435898, 0.5535857371794872], [0.47716346153846156, 0.5241386217948718, 0.5484775641025641, 0.5651041666666666, 0.5720152243589743, 0.5704126602564102, 0.5379607371794872, 0.5563902243589743, 0.5615985576923077, 0.5632011217948718, 0.5567908653846154, 0.5502804487179487, 0.5541866987179487, 0.5573918269230769, 0.5541866987179487, 0.5535857371794872, 0.5493790064102564, 0.5613982371794872, 0.553886217948718, 0.5537860576923077, 0.5579927884615384, 0.5536858974358975, 0.5662059294871795, 0.5500801282051282, 0.5572916666666666, 0.5543870192307693, 0.5434695512820513, 0.5571915064102564, 0.5595953525641025, 0.5740184294871795, 0.5661057692307693, 0.5500801282051282, 0.5615985576923077, 0.5586939102564102, 0.555488782051282, 0.5668068910256411, 0.5629006410256411, 0.5632011217948718, 0.5646033653846154, 0.5660056089743589, 0.5578926282051282, 0.5521834935897436, 0.5626001602564102, 0.5623998397435898, 0.5524839743589743, 0.5549879807692307, 0.5630008012820513, 0.5562900641025641, 0.5766225961538461, 0.5668068910256411], [0.4846754807692308, 0.5380608974358975, 0.5636017628205128, 0.5661057692307693, 0.5933493589743589, 0.5768229166666666, 0.5692107371794872, 0.5969551282051282, 0.5940504807692307, 0.5882411858974359, 0.5772235576923077, 0.5751201923076923, 0.5686097756410257, 0.5779246794871795, 0.5710136217948718, 0.5611979166666666, 0.5736177884615384, 0.5729166666666666, 0.5650040064102564, 0.5518830128205128, 0.5671073717948718, 0.5743189102564102, 0.5698116987179487, 0.5811298076923077, 0.5876402243589743, 0.5924479166666666, 0.5843349358974359, 0.58203125, 0.5891426282051282, 0.5868389423076923, 0.5821314102564102, 0.594551282051282, 0.5987580128205128, 0.5970552884615384, 0.5931490384615384, 0.5925480769230769, 0.6016626602564102, 0.5953525641025641, 0.5859375, 0.5933493589743589, 0.5957532051282052, 0.5955528846153846, 0.5893429487179487, 0.5932491987179487, 0.5973557692307693, 0.5905448717948718, 0.5961538461538461, 0.5991586538461539, 0.6000600961538461, 0.5949519230769231], [0.483974358974359, 0.5374599358974359, 0.5600961538461539, 0.5647035256410257, 0.5601963141025641, 0.5876402243589743, 0.589042467948718, 0.5791266025641025, 0.5662059294871795, 0.5809294871794872, 0.5871394230769231, 0.5853365384615384, 0.5855368589743589, 0.5827323717948718, 0.5659054487179487, 0.5707131410256411, 0.5943509615384616, 0.578926282051282, 0.5806290064102564, 0.5876402243589743, 0.5833333333333334, 0.5889423076923077, 0.5797275641025641, 0.5704126602564102, 0.5765224358974359, 0.5802283653846154, 0.5797275641025641, 0.577323717948718, 0.5676081730769231, 0.5759214743589743, 0.5838341346153846, 0.5832331730769231, 0.5875400641025641, 0.5769230769230769, 0.5808293269230769, 0.58203125, 0.5808293269230769, 0.5879407051282052, 0.5787259615384616, 0.5690104166666666, 0.5792267628205128, 0.5723157051282052, 0.5877403846153846, 0.5765224358974359, 0.5912459935897436, 0.5930488782051282, 0.5900440705128205, 0.5855368589743589, 0.5852363782051282, 0.5942508012820513], [0.4700520833333333, 0.5380608974358975, 0.5510817307692307, 0.5764222756410257, 0.5935496794871795, 0.5926482371794872, 0.5892427884615384, 0.575020032051282, 0.5917467948717948, 0.5875400641025641, 0.5808293269230769, 0.5855368589743589, 0.5825320512820513, 0.5759214743589743, 0.5647035256410257, 0.5660056089743589, 0.5712139423076923, 0.5769230769230769, 0.5869391025641025, 0.581229967948718, 0.5719150641025641, 0.5803285256410257, 0.5856370192307693, 0.5796274038461539, 0.5876402243589743, 0.5880408653846154, 0.5756209935897436, 0.5817307692307693, 0.5853365384615384, 0.5831330128205128, 0.5745192307692307, 0.5662059294871795, 0.5728165064102564, 0.5860376602564102, 0.5798277243589743, 0.5808293269230769, 0.5808293269230769, 0.5903445512820513, 0.5892427884615384, 0.5832331730769231, 0.5766225961538461, 0.5900440705128205, 0.5911458333333334, 0.5873397435897436, 0.5883413461538461, 0.5863381410256411, 0.5861378205128205, 0.5899439102564102, 0.5880408653846154, 0.5871394230769231], [0.480869391025641, 0.5395633012820513, 0.5683092948717948, 0.5799278846153846, 0.5852363782051282, 0.5871394230769231, 0.5716145833333334, 0.578926282051282, 0.594551282051282, 0.5732171474358975, 0.5601963141025641, 0.5661057692307693, 0.5814302884615384, 0.5732171474358975, 0.56640625, 0.5688100961538461, 0.5739182692307693, 0.5796274038461539, 0.5843349358974359, 0.5735176282051282, 0.5807291666666666, 0.5837339743589743, 0.5841346153846154, 0.5760216346153846, 0.5770232371794872, 0.5794270833333334, 0.5857371794871795, 0.5787259615384616, 0.5724158653846154, 0.5800280448717948, 0.5827323717948718, 0.5810296474358975, 0.5673076923076923, 0.5824318910256411, 0.5768229166666666, 0.5861378205128205, 0.5875400641025641, 0.5852363782051282, 0.5839342948717948, 0.5879407051282052, 0.5796274038461539, 0.5720152243589743, 0.5809294871794872, 0.5839342948717948, 0.5966546474358975, 0.5847355769230769, 0.5899439102564102, 0.5943509615384616, 0.5957532051282052, 0.5974559294871795], [0.4831730769230769, 0.5314503205128205, 0.5701121794871795, 0.5650040064102564, 0.5880408653846154, 0.5961538461538461, 0.59375, 0.5930488782051282, 0.5855368589743589, 0.5662059294871795, 0.5754206730769231, 0.5709134615384616, 0.5811298076923077, 0.5753205128205128, 0.5745192307692307, 0.5571915064102564, 0.5806290064102564, 0.582832532051282, 0.5835336538461539, 0.5694110576923077, 0.5707131410256411, 0.5809294871794872, 0.5669070512820513, 0.5506810897435898, 0.5685096153846154, 0.5765224358974359, 0.5716145833333334, 0.5774238782051282, 0.5861378205128205, 0.5839342948717948, 0.5752203525641025, 0.5666065705128205, 0.5799278846153846, 0.5853365384615384, 0.582832532051282, 0.5804286858974359, 0.58203125, 0.5784254807692307, 0.58984375, 0.5876402243589743, 0.5854366987179487, 0.5884415064102564, 0.5826322115384616, 0.5866386217948718, 0.5755208333333334, 0.5882411858974359, 0.5854366987179487, 0.5835336538461539, 0.5873397435897436, 0.5870392628205128], [0.4836738782051282, 0.5317508012820513, 0.5628004807692307, 0.5803285256410257, 0.5904447115384616, 0.5980568910256411, 0.5893429487179487, 0.5737179487179487, 0.578926282051282, 0.5853365384615384, 0.5531850961538461, 0.5719150641025641, 0.5857371794871795, 0.5822315705128205, 0.5574919871794872, 0.5614983974358975, 0.567207532051282, 0.5645032051282052, 0.5626001602564102, 0.5707131410256411, 0.5568910256410257, 0.5798277243589743, 0.5753205128205128, 0.5715144230769231, 0.5740184294871795, 0.5886418269230769, 0.5748197115384616, 0.5824318910256411, 0.5859375, 0.5834334935897436, 0.5883413461538461, 0.5895432692307693, 0.5844350961538461, 0.5924479166666666, 0.5766225961538461, 0.5691105769230769, 0.5793269230769231, 0.5679086538461539, 0.5776241987179487, 0.5843349358974359, 0.5884415064102564, 0.586738782051282, 0.5815304487179487, 0.5834334935897436, 0.5758213141025641, 0.5783253205128205, 0.5854366987179487, 0.5872395833333334, 0.5838341346153846, 0.5766225961538461], [0.4873798076923077, 0.5466746794871795, 0.5744190705128205, 0.5847355769230769, 0.5998597756410257, 0.5978565705128205, 0.5900440705128205, 0.5935496794871795, 0.5957532051282052, 0.5938501602564102, 0.5921474358974359, 0.5908453525641025, 0.5831330128205128, 0.5631009615384616, 0.5725160256410257, 0.5663060897435898, 0.5752203525641025, 0.5841346153846154, 0.581229967948718, 0.5700120192307693, 0.5792267628205128, 0.5784254807692307, 0.5891426282051282, 0.5911458333333334, 0.5991586538461539, 0.5902443910256411, 0.6008613782051282, 0.5875400641025641, 0.5870392628205128, 0.5909455128205128, 0.5834334935897436, 0.5881410256410257, 0.5787259615384616, 0.5793269230769231, 0.56640625, 0.5666065705128205, 0.5869391025641025, 0.5878405448717948, 0.5858373397435898, 0.5893429487179487, 0.5927483974358975, 0.5946514423076923, 0.5902443910256411, 0.5856370192307693, 0.5896434294871795, 0.5921474358974359, 0.5833333333333334, 0.5912459935897436, 0.5956530448717948, 0.5931490384615384], [0.47315705128205127, 0.5419671474358975, 0.5559895833333334, 0.5810296474358975, 0.5978565705128205, 0.6045673076923077, 0.5751201923076923, 0.5843349358974359, 0.5852363782051282, 0.5850360576923077, 0.5852363782051282, 0.5982572115384616, 0.5904447115384616, 0.5754206730769231, 0.5716145833333334, 0.5759214743589743, 0.5897435897435898, 0.5835336538461539, 0.5727163461538461, 0.5668068910256411, 0.5803285256410257, 0.5852363782051282, 0.5831330128205128, 0.5743189102564102, 0.5829326923076923, 0.5850360576923077, 0.5830328525641025, 0.5831330128205128, 0.5743189102564102, 0.5813301282051282, 0.5805288461538461, 0.5744190705128205, 0.5741185897435898, 0.5831330128205128, 0.5814302884615384, 0.589042467948718, 0.5859375, 0.5895432692307693, 0.5940504807692307, 0.5872395833333334, 0.5858373397435898, 0.58984375, 0.5871394230769231, 0.5831330128205128, 0.5824318910256411, 0.5831330128205128, 0.5763221153846154, 0.5863381410256411, 0.5911458333333334, 0.5860376602564102], [0.47966746794871795, 0.535957532051282, 0.5694110576923077, 0.5853365384615384, 0.5822315705128205, 0.5943509615384616, 0.594551282051282, 0.6004607371794872, 0.5840344551282052, 0.5874399038461539, 0.5936498397435898, 0.5725160256410257, 0.5690104166666666, 0.5696113782051282, 0.5804286858974359, 0.5425681089743589, 0.5658052884615384, 0.5805288461538461, 0.5724158653846154, 0.5707131410256411, 0.5645032051282052, 0.5676081730769231, 0.5874399038461539, 0.5897435897435898, 0.5859375, 0.5885416666666666, 0.5731169871794872, 0.5886418269230769, 0.5821314102564102, 0.5842347756410257, 0.5787259615384616, 0.5831330128205128, 0.5770232371794872, 0.5760216346153846, 0.5886418269230769, 0.5978565705128205, 0.5919471153846154, 0.5909455128205128, 0.5920472756410257, 0.5903445512820513, 0.5866386217948718, 0.5883413461538461, 0.5908453525641025, 0.5889423076923077, 0.5920472756410257, 0.5899439102564102, 0.5893429487179487, 0.585136217948718, 0.5972556089743589, 0.5822315705128205], [0.487880608974359, 0.555488782051282, 0.5555889423076923, 0.5775240384615384, 0.5864383012820513, 0.585136217948718, 0.5841346153846154, 0.5823317307692307, 0.5635016025641025, 0.5752203525641025, 0.5733173076923077, 0.5791266025641025, 0.5910456730769231, 0.5825320512820513, 0.5799278846153846, 0.5517828525641025, 0.5758213141025641, 0.5726161858974359, 0.5661057692307693, 0.5650040064102564, 0.5849358974358975, 0.5801282051282052, 0.5676081730769231, 0.5673076923076923, 0.5729166666666666, 0.5839342948717948, 0.5884415064102564, 0.5908453525641025, 0.5832331730769231, 0.5827323717948718, 0.5887419871794872, 0.5829326923076923, 0.5873397435897436, 0.5846354166666666, 0.5843349358974359, 0.5685096153846154, 0.5716145833333334, 0.5730168269230769, 0.5755208333333334, 0.5868389423076923, 0.5797275641025641, 0.5873397435897436, 0.5865384615384616, 0.5870392628205128, 0.5861378205128205, 0.5965544871794872, 0.5860376602564102, 0.5870392628205128, 0.5995592948717948, 0.5858373397435898], [0.48096955128205127, 0.5389623397435898, 0.5675080128205128, 0.5786258012820513, 0.5871394230769231, 0.5804286858974359, 0.5900440705128205, 0.5849358974358975, 0.5912459935897436, 0.5899439102564102, 0.5862379807692307, 0.5839342948717948, 0.5856370192307693, 0.5686097756410257, 0.5767227564102564, 0.5876402243589743, 0.5706129807692307, 0.5628004807692307, 0.5757211538461539, 0.5831330128205128, 0.5739182692307693, 0.5839342948717948, 0.5831330128205128, 0.5889423076923077, 0.5868389423076923, 0.5839342948717948, 0.6006610576923077, 0.5925480769230769, 0.5980568910256411, 0.5848357371794872, 0.5831330128205128, 0.5817307692307693, 0.5883413461538461, 0.585136217948718, 0.5866386217948718, 0.5859375, 0.5961538461538461, 0.589042467948718, 0.6010616987179487, 0.6103766025641025, 0.6064703525641025, 0.6013621794871795, 0.5957532051282052, 0.5986578525641025, 0.5980568910256411, 0.5978565705128205, 0.5953525641025641, 0.5886418269230769, 0.5889423076923077, 0.5956530448717948], [0.48938301282051283, 0.5336538461538461, 0.5758213141025641, 0.5723157051282052, 0.5927483974358975, 0.598457532051282, 0.5875400641025641, 0.5858373397435898, 0.598457532051282, 0.5832331730769231, 0.5961538461538461, 0.5846354166666666, 0.5778245192307693, 0.5837339743589743, 0.5829326923076923, 0.5748197115384616, 0.5848357371794872, 0.5755208333333334, 0.5864383012820513, 0.5780248397435898, 0.5681089743589743, 0.5841346153846154, 0.586738782051282, 0.5856370192307693, 0.5891426282051282, 0.5886418269230769, 0.5931490384615384, 0.5892427884615384, 0.5844350961538461, 0.5907451923076923, 0.5957532051282052, 0.5996594551282052, 0.5982572115384616, 0.5896434294871795, 0.5947516025641025, 0.5919471153846154, 0.582832532051282, 0.5975560897435898, 0.5878405448717948, 0.5935496794871795, 0.5951522435897436, 0.589042467948718, 0.5860376602564102, 0.5916466346153846, 0.5926482371794872, 0.59375, 0.5871394230769231, 0.5973557692307693, 0.602363782051282, 0.6016626602564102], [0.47676282051282054, 0.5387620192307693, 0.5557892628205128, 0.5804286858974359, 0.577323717948718, 0.5980568910256411, 0.59375, 0.5912459935897436, 0.5894431089743589, 0.5822315705128205, 0.5924479166666666, 0.5950520833333334, 0.5979567307692307, 0.5946514423076923, 0.5743189102564102, 0.5787259615384616, 0.5783253205128205, 0.5829326923076923, 0.5699118589743589, 0.5804286858974359, 0.5925480769230769, 0.5891426282051282, 0.5694110576923077, 0.5880408653846154, 0.5983573717948718, 0.5927483974358975, 0.5869391025641025, 0.5934495192307693, 0.5969551282051282, 0.59375, 0.6008613782051282, 0.5924479166666666, 0.5936498397435898, 0.5836338141025641, 0.5917467948717948, 0.5848357371794872, 0.5977564102564102, 0.5933493589743589, 0.5862379807692307, 0.5924479166666666, 0.6022636217948718, 0.6028645833333334, 0.5986578525641025, 0.6033653846153846, 0.6036658653846154, 0.5985576923076923, 0.5942508012820513, 0.5993589743589743, 0.6033653846153846, 0.6056690705128205], [0.4863782051282051, 0.539863782051282, 0.5618990384615384, 0.5802283653846154, 0.5910456730769231, 0.5860376602564102, 0.5748197115384616, 0.5798277243589743, 0.5754206730769231, 0.5786258012820513, 0.5704126602564102, 0.5710136217948718, 0.5819310897435898, 0.5883413461538461, 0.5946514423076923, 0.5836338141025641, 0.5807291666666666, 0.5681089743589743, 0.5674078525641025, 0.5658052884615384, 0.5776241987179487, 0.5760216346153846, 0.5727163461538461, 0.5704126602564102, 0.5718149038461539, 0.5799278846153846, 0.5766225961538461, 0.5777243589743589, 0.5802283653846154, 0.5846354166666666, 0.5793269230769231, 0.5809294871794872, 0.5810296474358975, 0.5855368589743589, 0.5805288461538461, 0.5726161858974359, 0.5755208333333334, 0.5826322115384616, 0.5899439102564102, 0.5870392628205128, 0.5855368589743589, 0.5788261217948718, 0.58203125, 0.5797275641025641, 0.5869391025641025, 0.5900440705128205, 0.5978565705128205, 0.5931490384615384, 0.5917467948717948, 0.5953525641025641]]
Evaluater: loading data
After 1 training epoch, loss on training batch is 4.590131
2019-06-20 23:35:11.530122: precision = 0.110
After 2 training epoch, loss on training batch is 4.461248
2019-06-20 23:35:42.843485: precision = 0.134
After 3 training epoch, loss on training batch is 4.237085
2019-06-20 23:36:14.147683: precision = 0.224
After 4 training epoch, loss on training batch is 3.953499
2019-06-20 23:36:45.485235: precision = 0.227
After 5 training epoch, loss on training batch is 3.979149
2019-06-20 23:37:16.863169: precision = 0.262
After 6 training epoch, loss on training batch is 4.212250
2019-06-20 23:37:48.107286: precision = 0.273
After 7 training epoch, loss on training batch is 3.830714
2019-06-20 23:38:19.439322: precision = 0.286
After 8 training epoch, loss on training batch is 3.833815
2019-06-20 23:38:50.662651: precision = 0.296
After 9 training epoch, loss on training batch is 3.977097
2019-06-20 23:39:21.987231: precision = 0.295
After 10 training epoch, loss on training batch is 3.557150
2019-06-20 23:39:53.300965: precision = 0.299
After 11 training epoch, loss on training batch is 3.377515
2019-06-20 23:40:24.614774: precision = 0.315
After 12 training epoch, loss on training batch is 3.418994
2019-06-20 23:40:55.822424: precision = 0.316
After 13 training epoch, loss on training batch is 3.396577
2019-06-20 23:41:27.073061: precision = 0.319
After 14 training epoch, loss on training batch is 3.503017
2019-06-20 23:41:58.435145: precision = 0.344
After 15 training epoch, loss on training batch is 3.389908
2019-06-20 23:42:29.785595: precision = 0.355
After 16 training epoch, loss on training batch is 3.424178
2019-06-20 23:43:01.101897: precision = 0.364
After 17 training epoch, loss on training batch is 3.404427
2019-06-20 23:43:32.361039: precision = 0.365
After 18 training epoch, loss on training batch is 3.217688
2019-06-20 23:44:03.602845: precision = 0.379
After 19 training epoch, loss on training batch is 3.012436
2019-06-20 23:44:34.865298: precision = 0.365
After 20 training epoch, loss on training batch is 3.146469
2019-06-20 23:45:06.202984: precision = 0.393
After 21 training epoch, loss on training batch is 3.185241
2019-06-20 23:45:37.418731: precision = 0.409
After 22 training epoch, loss on training batch is 3.250064
2019-06-20 23:46:08.741452: precision = 0.406
After 23 training epoch, loss on training batch is 2.950434
2019-06-20 23:46:40.062476: precision = 0.407
After 24 training epoch, loss on training batch is 3.078837
2019-06-20 23:47:11.224994: precision = 0.393
After 25 training epoch, loss on training batch is 3.210423
2019-06-20 23:47:42.517380: precision = 0.392
After 26 training epoch, loss on training batch is 2.919522
2019-06-20 23:48:13.756994: precision = 0.396
After 27 training epoch, loss on training batch is 2.670568
2019-06-20 23:48:45.049987: precision = 0.425
After 28 training epoch, loss on training batch is 2.707394
2019-06-20 23:49:16.290975: precision = 0.447
After 29 training epoch, loss on training batch is 3.014108
2019-06-20 23:49:47.589389: precision = 0.451
After 30 training epoch, loss on training batch is 2.541651
2019-06-20 23:50:18.909451: precision = 0.448
After 31 training epoch, loss on training batch is 2.456016
2019-06-20 23:50:50.260624: precision = 0.457
After 32 training epoch, loss on training batch is 2.822762
2019-06-20 23:51:21.471468: precision = 0.460
After 33 training epoch, loss on training batch is 2.396651
2019-06-20 23:51:52.781297: precision = 0.440
After 34 training epoch, loss on training batch is 2.234006
2019-06-20 23:52:24.057911: precision = 0.463
After 35 training epoch, loss on training batch is 2.207108
2019-06-20 23:52:55.323343: precision = 0.482
After 36 training epoch, loss on training batch is 2.156786
2019-06-20 23:53:26.671329: precision = 0.460
After 37 training epoch, loss on training batch is 2.430051
2019-06-20 23:53:57.947846: precision = 0.475
After 38 training epoch, loss on training batch is 2.413876
2019-06-20 23:54:29.252653: precision = 0.474
After 39 training epoch, loss on training batch is 2.443217
2019-06-20 23:55:00.510648: precision = 0.481
After 40 training epoch, loss on training batch is 2.279629
2019-06-20 23:55:31.728310: precision = 0.469
After 41 training epoch, loss on training batch is 2.141353
2019-06-20 23:56:03.082463: precision = 0.480
After 42 training epoch, loss on training batch is 2.400154
2019-06-20 23:56:34.391614: precision = 0.477
After 43 training epoch, loss on training batch is 2.700916
2019-06-20 23:57:05.740753: precision = 0.462
After 44 training epoch, loss on training batch is 2.314740
2019-06-20 23:57:37.026102: precision = 0.465
After 45 training epoch, loss on training batch is 2.305703
2019-06-20 23:58:08.338986: precision = 0.459
After 46 training epoch, loss on training batch is 2.084417
2019-06-20 23:58:39.637675: precision = 0.489
After 47 training epoch, loss on training batch is 1.857541
2019-06-20 23:59:10.947627: precision = 0.482
After 48 training epoch, loss on training batch is 1.634033
2019-06-20 23:59:42.215439: precision = 0.483
After 49 training epoch, loss on training batch is 1.711637
2019-06-21 00:00:13.538209: precision = 0.486
After 50 training epoch, loss on training batch is 2.038415
2019-06-21 00:00:44.829133: precision = 0.476
Traceback (most recent call last):
  File "evaluater.py", line 489, in <module>
    g.append(a)
AttributeError: 'numpy.ndarray' object has no attribute 'append'
Evaluater: loading data
After 1 training epoch, loss on training batch is 4.598129
2019-06-22 11:08:21.742447: precision = 0.105
After 2 training epoch, loss on training batch is 4.244218
2019-06-22 11:08:52.078020: precision = 0.177
After 3 training epoch, loss on training batch is 4.220504
2019-06-22 11:09:22.290531: precision = 0.253
After 4 training epoch, loss on training batch is 4.049747
2019-06-22 11:09:52.674410: precision = 0.219
After 5 training epoch, loss on training batch is 3.967461
2019-06-22 11:10:23.110152: precision = 0.278
After 6 training epoch, loss on training batch is 4.117232
2019-06-22 11:10:53.451109: precision = 0.315
After 7 training epoch, loss on training batch is 3.930895
2019-06-22 11:11:23.858009: precision = 0.328
After 8 training epoch, loss on training batch is 3.632770
2019-06-22 11:11:54.229151: precision = 0.326
After 9 training epoch, loss on training batch is 3.495211
2019-06-22 11:12:24.625005: precision = 0.332
After 10 training epoch, loss on training batch is 3.459452
2019-06-22 11:12:54.941931: precision = 0.350
After 11 training epoch, loss on training batch is 3.264951
2019-06-22 11:13:25.240389: precision = 0.344
After 12 training epoch, loss on training batch is 3.086876
2019-06-22 11:13:55.755452: precision = 0.358
After 13 training epoch, loss on training batch is 3.325557
2019-06-22 11:14:26.118108: precision = 0.375
After 14 training epoch, loss on training batch is 3.329818
2019-06-22 11:14:56.580829: precision = 0.363
After 15 training epoch, loss on training batch is 3.303904
2019-06-22 11:15:27.009644: precision = 0.391
After 16 training epoch, loss on training batch is 3.186016
2019-06-22 11:15:57.451854: precision = 0.402
After 17 training epoch, loss on training batch is 3.187398
2019-06-22 11:16:27.936568: precision = 0.413
After 18 training epoch, loss on training batch is 3.157465
2019-06-22 11:16:58.322071: precision = 0.411
After 19 training epoch, loss on training batch is 3.018290
2019-06-22 11:17:28.796162: precision = 0.401
After 20 training epoch, loss on training batch is 3.002756
2019-06-22 11:17:59.156108: precision = 0.407
After 21 training epoch, loss on training batch is 2.880874
2019-06-22 11:18:29.565791: precision = 0.420
After 22 training epoch, loss on training batch is 2.952518
2019-06-22 11:18:59.826942: precision = 0.414
After 23 training epoch, loss on training batch is 2.801720
2019-06-22 11:19:30.212914: precision = 0.422
After 24 training epoch, loss on training batch is 2.910062
2019-06-22 11:20:00.649953: precision = 0.413
After 25 training epoch, loss on training batch is 2.652542
2019-06-22 11:20:30.991351: precision = 0.452
After 26 training epoch, loss on training batch is 2.576847
2019-06-22 11:21:01.419989: precision = 0.436
After 27 training epoch, loss on training batch is 2.724077
2019-06-22 11:21:31.752146: precision = 0.467
After 28 training epoch, loss on training batch is 2.391135
2019-06-22 11:22:02.157076: precision = 0.458
After 29 training epoch, loss on training batch is 2.314342
2019-06-22 11:22:32.540564: precision = 0.467
After 30 training epoch, loss on training batch is 2.470360
2019-06-22 11:23:02.994544: precision = 0.471
After 31 training epoch, loss on training batch is 2.677426
2019-06-22 11:23:33.328486: precision = 0.469
After 32 training epoch, loss on training batch is 2.966997
2019-06-22 11:24:03.769001: precision = 0.465
After 33 training epoch, loss on training batch is 2.815451
2019-06-22 11:24:34.136481: precision = 0.461
After 34 training epoch, loss on training batch is 2.135643
2019-06-22 11:25:04.510557: precision = 0.473
After 35 training epoch, loss on training batch is 1.963600
2019-06-22 11:25:34.892341: precision = 0.478
After 36 training epoch, loss on training batch is 2.122344
2019-06-22 11:26:05.382055: precision = 0.472
After 37 training epoch, loss on training batch is 2.297263
2019-06-22 11:26:35.797575: precision = 0.462
After 38 training epoch, loss on training batch is 2.176420
2019-06-22 11:27:06.190702: precision = 0.472
After 39 training epoch, loss on training batch is 2.578767
2019-06-22 11:27:36.584010: precision = 0.479
After 40 training epoch, loss on training batch is 2.425564
2019-06-22 11:28:07.067397: precision = 0.466
After 41 training epoch, loss on training batch is 2.343117
2019-06-22 11:28:37.485489: precision = 0.495
After 42 training epoch, loss on training batch is 2.157335
2019-06-22 11:29:07.866873: precision = 0.487
After 43 training epoch, loss on training batch is 2.275703
2019-06-22 11:29:38.146651: precision = 0.496
After 44 training epoch, loss on training batch is 1.836213
2019-06-22 11:30:08.514824: precision = 0.487
After 45 training epoch, loss on training batch is 1.775612
2019-06-22 11:30:38.886079: precision = 0.472
After 46 training epoch, loss on training batch is 1.750804
2019-06-22 11:31:09.318238: precision = 0.487
After 47 training epoch, loss on training batch is 1.773749
2019-06-22 11:31:39.642587: precision = 0.512
After 48 training epoch, loss on training batch is 1.611144
2019-06-22 11:32:10.080812: precision = 0.501
After 49 training epoch, loss on training batch is 1.693534
2019-06-22 11:32:40.499498: precision = 0.498
After 50 training epoch, loss on training batch is 1.591197
2019-06-22 11:33:10.886589: precision = 0.495
After 1 training epoch, loss on training batch is 3.991153
2019-06-22 11:34:14.037320: precision = 0.195
After 2 training epoch, loss on training batch is 3.987075
2019-06-22 11:35:11.608098: precision = 0.267
After 3 training epoch, loss on training batch is 3.743267
2019-06-22 11:36:09.248637: precision = 0.304
After 4 training epoch, loss on training batch is 3.520527
2019-06-22 11:37:06.821660: precision = 0.357
After 5 training epoch, loss on training batch is 3.527306
2019-06-22 11:38:04.373254: precision = 0.385
After 6 training epoch, loss on training batch is 3.679019
2019-06-22 11:39:02.123678: precision = 0.396
After 7 training epoch, loss on training batch is 3.306396
2019-06-22 11:39:59.765804: precision = 0.420
After 8 training epoch, loss on training batch is 3.192957
2019-06-22 11:40:57.385139: precision = 0.400
After 9 training epoch, loss on training batch is 2.726585
2019-06-22 11:41:54.837414: precision = 0.413
After 10 training epoch, loss on training batch is 2.807562
2019-06-22 11:42:52.654192: precision = 0.463
After 11 training epoch, loss on training batch is 2.988414
2019-06-22 11:43:50.309242: precision = 0.499
After 12 training epoch, loss on training batch is 2.639760
2019-06-22 11:44:47.909792: precision = 0.498
After 13 training epoch, loss on training batch is 2.342787
2019-06-22 11:45:45.529745: precision = 0.502
After 14 training epoch, loss on training batch is 2.487256
2019-06-22 11:46:43.124647: precision = 0.524
After 15 training epoch, loss on training batch is 2.555597
2019-06-22 11:47:40.698863: precision = 0.526
After 16 training epoch, loss on training batch is 2.432270
2019-06-22 11:48:38.518818: precision = 0.516
After 17 training epoch, loss on training batch is 2.301863
2019-06-22 11:49:36.084897: precision = 0.527
After 18 training epoch, loss on training batch is 2.196616
2019-06-22 11:50:33.645024: precision = 0.540
After 19 training epoch, loss on training batch is 2.198619
2019-06-22 11:51:31.401744: precision = 0.549
After 20 training epoch, loss on training batch is 2.438565
2019-06-22 11:52:29.040572: precision = 0.558
After 21 training epoch, loss on training batch is 2.399483
2019-06-22 11:53:26.604984: precision = 0.548
After 22 training epoch, loss on training batch is 2.161184
2019-06-22 11:54:24.311760: precision = 0.550
After 23 training epoch, loss on training batch is 2.080060
2019-06-22 11:55:21.896115: precision = 0.561
After 24 training epoch, loss on training batch is 2.211662
2019-06-22 11:56:19.542921: precision = 0.554
After 25 training epoch, loss on training batch is 1.837181
2019-06-22 11:57:17.245179: precision = 0.538
After 26 training epoch, loss on training batch is 1.763669
2019-06-22 11:58:14.938026: precision = 0.549
After 27 training epoch, loss on training batch is 2.025526
2019-06-22 11:59:12.599459: precision = 0.571
After 28 training epoch, loss on training batch is 1.973524
2019-06-22 12:00:10.266475: precision = 0.579
After 29 training epoch, loss on training batch is 1.963902
2019-06-22 12:01:07.943836: precision = 0.582
After 30 training epoch, loss on training batch is 1.845086
2019-06-22 12:02:05.521596: precision = 0.569
After 31 training epoch, loss on training batch is 1.674689
2019-06-22 12:03:02.945276: precision = 0.567
After 32 training epoch, loss on training batch is 1.844961
2019-06-22 12:04:00.601025: precision = 0.574
After 33 training epoch, loss on training batch is 1.873143
2019-06-22 12:04:58.237044: precision = 0.590
After 34 training epoch, loss on training batch is 1.818219
2019-06-22 12:05:55.857912: precision = 0.577
After 35 training epoch, loss on training batch is 1.676731
2019-06-22 12:06:53.399844: precision = 0.575
After 36 training epoch, loss on training batch is 1.566692
2019-06-22 12:07:51.216251: precision = 0.577
After 37 training epoch, loss on training batch is 1.623825
2019-06-22 12:08:48.818659: precision = 0.587
After 38 training epoch, loss on training batch is 1.609129
2019-06-22 12:09:46.289140: precision = 0.595
After 39 training epoch, loss on training batch is 1.402591
2019-06-22 12:10:43.710780: precision = 0.590
After 40 training epoch, loss on training batch is 1.533372
2019-06-22 12:11:41.552846: precision = 0.588
After 41 training epoch, loss on training batch is 1.278443
2019-06-22 12:12:39.073674: precision = 0.592
After 42 training epoch, loss on training batch is 1.140161
2019-06-22 12:13:36.622854: precision = 0.603
After 43 training epoch, loss on training batch is 1.321059
2019-06-22 12:14:34.299258: precision = 0.600
After 44 training epoch, loss on training batch is 1.197211
2019-06-22 12:15:31.936780: precision = 0.604
After 45 training epoch, loss on training batch is 1.229920
2019-06-22 12:16:29.644452: precision = 0.611
After 46 training epoch, loss on training batch is 1.565427
2019-06-22 12:17:27.377906: precision = 0.612
After 47 training epoch, loss on training batch is 1.267299
2019-06-22 12:18:24.844285: precision = 0.602
After 48 training epoch, loss on training batch is 1.016951
2019-06-22 12:19:22.490856: precision = 0.608
After 49 training epoch, loss on training batch is 0.971113
2019-06-22 12:20:19.790608: precision = 0.610
After 50 training epoch, loss on training batch is 1.041700
2019-06-22 12:21:17.571421: precision = 0.596
After 1 training epoch, loss on training batch is 3.912219
2019-06-22 12:23:03.242814: precision = 0.286
After 2 training epoch, loss on training batch is 3.447790
2019-06-22 12:24:42.269399: precision = 0.352
After 3 training epoch, loss on training batch is 3.115561
2019-06-22 12:26:21.245751: precision = 0.397
After 4 training epoch, loss on training batch is 2.931060
2019-06-22 12:28:00.345499: precision = 0.450
After 5 training epoch, loss on training batch is 2.693213
2019-06-22 12:29:39.323888: precision = 0.474
After 6 training epoch, loss on training batch is 2.559590
2019-06-22 12:31:18.297757: precision = 0.485
After 7 training epoch, loss on training batch is 2.674880
2019-06-22 12:32:57.238399: precision = 0.497
After 8 training epoch, loss on training batch is 2.587212
2019-06-22 12:34:36.154800: precision = 0.512
After 9 training epoch, loss on training batch is 2.471487
2019-06-22 12:36:15.063552: precision = 0.532
After 10 training epoch, loss on training batch is 2.557916
2019-06-22 12:37:54.126209: precision = 0.541
After 11 training epoch, loss on training batch is 2.209116
2019-06-22 12:39:33.167081: precision = 0.538
After 12 training epoch, loss on training batch is 2.210013
2019-06-22 12:41:12.100116: precision = 0.557
After 13 training epoch, loss on training batch is 2.177623
2019-06-22 12:42:51.013382: precision = 0.560
After 14 training epoch, loss on training batch is 2.377973
2019-06-22 12:44:29.782162: precision = 0.574
After 15 training epoch, loss on training batch is 2.391464
2019-06-22 12:46:08.543995: precision = 0.586
After 16 training epoch, loss on training batch is 2.493752
2019-06-22 12:47:47.493131: precision = 0.594
After 17 training epoch, loss on training batch is 2.359290
2019-06-22 12:49:26.362864: precision = 0.603
After 18 training epoch, loss on training batch is 2.165588
2019-06-22 12:51:05.068073: precision = 0.607
After 19 training epoch, loss on training batch is 2.114841
2019-06-22 12:52:43.994435: precision = 0.604
After 20 training epoch, loss on training batch is 2.063433
2019-06-22 12:54:22.998583: precision = 0.594
After 21 training epoch, loss on training batch is 2.121781
2019-06-22 12:56:01.955496: precision = 0.596
After 22 training epoch, loss on training batch is 1.910180
2019-06-22 12:57:40.822021: precision = 0.596
After 23 training epoch, loss on training batch is 1.888401
2019-06-22 12:59:20.040038: precision = 0.600
After 24 training epoch, loss on training batch is 1.676238
2019-06-22 13:00:59.025800: precision = 0.599
After 25 training epoch, loss on training batch is 1.593729
2019-06-22 13:02:38.060277: precision = 0.608
After 26 training epoch, loss on training batch is 1.490217
2019-06-22 13:04:16.758936: precision = 0.607
After 27 training epoch, loss on training batch is 1.505565
2019-06-22 13:05:55.746521: precision = 0.620
After 28 training epoch, loss on training batch is 1.470889
2019-06-22 13:07:34.787264: precision = 0.624
After 29 training epoch, loss on training batch is 1.493695
2019-06-22 13:09:13.572046: precision = 0.632
After 30 training epoch, loss on training batch is 1.423655
2019-06-22 13:10:52.599065: precision = 0.637
After 31 training epoch, loss on training batch is 1.468364
2019-06-22 13:12:31.661032: precision = 0.641
After 32 training epoch, loss on training batch is 1.241783
2019-06-22 13:14:10.795042: precision = 0.636
After 33 training epoch, loss on training batch is 1.388041
2019-06-22 13:15:49.615834: precision = 0.631
After 34 training epoch, loss on training batch is 1.230495
2019-06-22 13:17:28.589725: precision = 0.632
After 35 training epoch, loss on training batch is 1.121871
2019-06-22 13:19:07.392629: precision = 0.629
After 36 training epoch, loss on training batch is 0.908149
2019-06-22 13:20:46.512868: precision = 0.623
After 37 training epoch, loss on training batch is 0.945552
2019-06-22 13:22:25.585873: precision = 0.626
After 38 training epoch, loss on training batch is 0.997333
2019-06-22 13:24:04.731993: precision = 0.622
After 39 training epoch, loss on training batch is 0.889457
2019-06-22 13:25:43.543117: precision = 0.628
After 40 training epoch, loss on training batch is 0.838641
2019-06-22 13:27:22.297926: precision = 0.638
After 41 training epoch, loss on training batch is 0.839995
2019-06-22 13:29:01.266764: precision = 0.641
After 42 training epoch, loss on training batch is 0.988043
2019-06-22 13:30:40.352771: precision = 0.639
After 43 training epoch, loss on training batch is 0.877500
2019-06-22 13:32:19.363994: precision = 0.629
After 44 training epoch, loss on training batch is 0.829852
2019-06-22 13:33:58.317761: precision = 0.629
After 45 training epoch, loss on training batch is 0.799045
2019-06-22 13:35:37.311862: precision = 0.628
After 46 training epoch, loss on training batch is 0.858086
2019-06-22 13:37:16.420658: precision = 0.631
After 47 training epoch, loss on training batch is 0.979551
2019-06-22 13:38:55.282573: precision = 0.618
After 48 training epoch, loss on training batch is 0.795903
2019-06-22 13:40:34.229518: precision = 0.609
After 49 training epoch, loss on training batch is 0.710818
2019-06-22 13:42:13.358798: precision = 0.624
After 50 training epoch, loss on training batch is 0.648887
2019-06-22 13:43:52.297695: precision = 0.631
After 1 training epoch, loss on training batch is 3.501517
2019-06-22 13:46:32.154625: precision = 0.334
After 2 training epoch, loss on training batch is 3.001858
2019-06-22 13:49:04.751704: precision = 0.402
After 3 training epoch, loss on training batch is 2.932922
2019-06-22 13:51:37.489899: precision = 0.490
After 4 training epoch, loss on training batch is 2.782216
2019-06-22 13:54:10.500516: precision = 0.514
After 5 training epoch, loss on training batch is 3.015162
2019-06-22 13:56:43.811507: precision = 0.535
After 6 training epoch, loss on training batch is 2.703522
2019-06-22 13:59:16.875809: precision = 0.561
After 7 training epoch, loss on training batch is 2.673280
2019-06-22 14:01:49.828341: precision = 0.579
After 8 training epoch, loss on training batch is 2.057842
2019-06-22 14:04:22.991514: precision = 0.598
After 9 training epoch, loss on training batch is 2.095100
2019-06-22 14:06:55.964297: precision = 0.613
After 10 training epoch, loss on training batch is 1.979263
2019-06-22 14:09:28.826540: precision = 0.614
After 11 training epoch, loss on training batch is 1.872292
2019-06-22 14:12:01.741150: precision = 0.613
After 12 training epoch, loss on training batch is 1.951563
2019-06-22 14:14:34.703596: precision = 0.610
After 13 training epoch, loss on training batch is 2.062299
2019-06-22 14:17:07.410929: precision = 0.629
After 14 training epoch, loss on training batch is 2.020530
2019-06-22 14:19:40.171884: precision = 0.633
After 15 training epoch, loss on training batch is 1.676697
2019-06-22 14:22:12.948255: precision = 0.633
After 16 training epoch, loss on training batch is 1.733250
2019-06-22 14:24:46.259180: precision = 0.633
After 17 training epoch, loss on training batch is 1.728185
2019-06-22 14:27:19.122613: precision = 0.638
After 18 training epoch, loss on training batch is 1.839257
2019-06-22 14:29:52.032858: precision = 0.650
After 19 training epoch, loss on training batch is 1.698611
2019-06-22 14:32:25.094375: precision = 0.659
After 20 training epoch, loss on training batch is 1.862226
2019-06-22 14:34:57.943283: precision = 0.665
After 21 training epoch, loss on training batch is 1.636380
2019-06-22 14:37:30.966280: precision = 0.669
After 22 training epoch, loss on training batch is 1.507254
2019-06-22 14:40:03.610387: precision = 0.670
After 23 training epoch, loss on training batch is 1.183929
2019-06-22 14:42:36.452449: precision = 0.678
After 24 training epoch, loss on training batch is 1.215233
2019-06-22 14:45:09.467204: precision = 0.675
After 25 training epoch, loss on training batch is 1.513633
2019-06-22 14:47:42.062652: precision = 0.680
After 26 training epoch, loss on training batch is 1.539093
2019-06-22 14:50:14.942428: precision = 0.684
After 27 training epoch, loss on training batch is 1.433627
2019-06-22 14:52:47.549736: precision = 0.682
After 28 training epoch, loss on training batch is 1.114947
2019-06-22 14:55:20.814423: precision = 0.682
After 29 training epoch, loss on training batch is 0.916268
2019-06-22 14:57:53.684874: precision = 0.677
After 30 training epoch, loss on training batch is 0.922922
2019-06-22 15:00:26.496308: precision = 0.676
After 31 training epoch, loss on training batch is 0.888428
2019-06-22 15:02:59.134362: precision = 0.677
After 32 training epoch, loss on training batch is 0.946043
2019-06-22 15:05:31.769964: precision = 0.679
After 33 training epoch, loss on training batch is 0.977614
2019-06-22 15:08:04.419808: precision = 0.684
After 34 training epoch, loss on training batch is 0.886278
2019-06-22 15:10:37.422994: precision = 0.683
After 35 training epoch, loss on training batch is 0.803048
2019-06-22 15:13:10.697984: precision = 0.689
After 36 training epoch, loss on training batch is 0.901524
2019-06-22 15:15:44.111836: precision = 0.685
After 37 training epoch, loss on training batch is 1.168154
2019-06-22 15:18:16.802383: precision = 0.674
After 38 training epoch, loss on training batch is 1.028855
2019-06-22 15:20:49.938501: precision = 0.668
After 39 training epoch, loss on training batch is 1.119515
2019-06-22 15:23:22.302649: precision = 0.677
After 40 training epoch, loss on training batch is 0.995173
2019-06-22 15:25:55.141246: precision = 0.666
After 41 training epoch, loss on training batch is 0.714477
2019-06-22 15:28:27.751280: precision = 0.650
After 42 training epoch, loss on training batch is 0.693908
2019-06-22 15:31:00.466217: precision = 0.663
After 43 training epoch, loss on training batch is 0.667567
2019-06-22 15:33:33.261251: precision = 0.663
After 44 training epoch, loss on training batch is 0.682016
2019-06-22 15:36:06.233058: precision = 0.668
After 45 training epoch, loss on training batch is 0.515953
2019-06-22 15:38:39.063317: precision = 0.668
After 46 training epoch, loss on training batch is 0.491836
2019-06-22 15:41:11.677926: precision = 0.652
After 47 training epoch, loss on training batch is 0.454138
2019-06-22 15:43:44.754338: precision = 0.655
After 48 training epoch, loss on training batch is 0.469752
2019-06-22 15:46:17.663089: precision = 0.667
After 49 training epoch, loss on training batch is 0.580721
2019-06-22 15:48:50.910818: precision = 0.673
After 50 training epoch, loss on training batch is 0.725154
2019-06-22 15:51:24.055824: precision = 0.673
After 1 training epoch, loss on training batch is 3.459197
2019-06-22 15:55:10.022147: precision = 0.374
After 2 training epoch, loss on training batch is 2.993021
2019-06-22 15:58:49.809145: precision = 0.464
After 3 training epoch, loss on training batch is 2.746509
2019-06-22 16:02:29.797485: precision = 0.506
After 4 training epoch, loss on training batch is 2.476220
2019-06-22 16:06:09.656815: precision = 0.550
After 5 training epoch, loss on training batch is 2.196848
2019-06-22 16:09:49.807727: precision = 0.581
After 6 training epoch, loss on training batch is 2.171833
2019-06-22 16:13:29.844903: precision = 0.596
After 7 training epoch, loss on training batch is 2.598449
2019-06-22 16:17:09.926503: precision = 0.600
After 8 training epoch, loss on training batch is 2.323999
2019-06-22 16:20:50.024821: precision = 0.600
After 9 training epoch, loss on training batch is 2.215130
2019-06-22 16:24:29.703126: precision = 0.626
After 10 training epoch, loss on training batch is 2.073586
2019-06-22 16:28:09.811821: precision = 0.631
After 11 training epoch, loss on training batch is 1.718538
2019-06-22 16:31:49.069071: precision = 0.647
After 12 training epoch, loss on training batch is 1.552828
2019-06-22 16:35:29.180485: precision = 0.653
After 13 training epoch, loss on training batch is 1.650770
2019-06-22 16:39:09.026216: precision = 0.646
After 14 training epoch, loss on training batch is 1.503285
2019-06-22 16:42:49.015025: precision = 0.659
After 15 training epoch, loss on training batch is 1.554860
2019-06-22 16:46:28.990658: precision = 0.673
After 16 training epoch, loss on training batch is 1.539096
2019-06-22 16:50:08.662124: precision = 0.676
After 17 training epoch, loss on training batch is 1.260436
2019-06-22 16:53:48.671664: precision = 0.674
After 18 training epoch, loss on training batch is 1.520319
2019-06-22 16:57:28.622761: precision = 0.674
After 19 training epoch, loss on training batch is 1.020396
2019-06-22 17:01:08.510098: precision = 0.673
After 20 training epoch, loss on training batch is 1.534737
2019-06-22 17:04:48.545140: precision = 0.689
After 21 training epoch, loss on training batch is 1.083735
2019-06-22 17:08:28.624630: precision = 0.692
After 22 training epoch, loss on training batch is 1.057407
2019-06-22 17:12:08.874378: precision = 0.695
After 23 training epoch, loss on training batch is 1.001423
2019-06-22 17:15:49.037282: precision = 0.699
After 24 training epoch, loss on training batch is 1.022400
2019-06-22 17:19:29.057762: precision = 0.704
After 25 training epoch, loss on training batch is 1.311011
2019-06-22 17:23:08.940306: precision = 0.706
After 26 training epoch, loss on training batch is 1.406369
2019-06-22 17:26:48.501305: precision = 0.702
After 27 training epoch, loss on training batch is 1.155409
2019-06-22 17:30:28.194469: precision = 0.695
After 28 training epoch, loss on training batch is 0.784158
2019-06-22 17:34:07.962669: precision = 0.683
After 29 training epoch, loss on training batch is 0.694382
2019-06-22 17:37:47.568393: precision = 0.690
After 30 training epoch, loss on training batch is 0.992789
2019-06-22 17:41:27.473672: precision = 0.709
After 31 training epoch, loss on training batch is 0.864211
2019-06-22 17:45:07.386751: precision = 0.686
After 32 training epoch, loss on training batch is 0.721604
2019-06-22 17:48:47.334929: precision = 0.691
After 33 training epoch, loss on training batch is 0.876701
2019-06-22 17:52:27.050657: precision = 0.697
After 34 training epoch, loss on training batch is 0.572013
2019-06-22 17:56:06.958525: precision = 0.702
After 35 training epoch, loss on training batch is 0.838142
2019-06-22 17:59:46.639637: precision = 0.691
After 36 training epoch, loss on training batch is 0.683110
2019-06-22 18:03:26.562901: precision = 0.679
After 37 training epoch, loss on training batch is 0.818478
2019-06-22 18:07:07.010216: precision = 0.691
After 38 training epoch, loss on training batch is 0.623772
2019-06-22 18:10:47.412517: precision = 0.686
After 39 training epoch, loss on training batch is 0.516733
2019-06-22 18:14:27.448349: precision = 0.697
After 40 training epoch, loss on training batch is 0.468265
2019-06-22 18:18:07.326137: precision = 0.700
After 41 training epoch, loss on training batch is 0.324471
2019-06-22 18:21:47.206966: precision = 0.695
After 42 training epoch, loss on training batch is 0.441040
2019-06-22 18:25:26.870491: precision = 0.698
After 43 training epoch, loss on training batch is 0.588824
2019-06-22 18:29:06.634071: precision = 0.686
After 44 training epoch, loss on training batch is 0.487718
2019-06-22 18:32:46.504442: precision = 0.693
After 45 training epoch, loss on training batch is 0.455147
2019-06-22 18:36:26.012554: precision = 0.698
After 46 training epoch, loss on training batch is 0.569784
2019-06-22 18:40:05.599000: precision = 0.707
After 47 training epoch, loss on training batch is 0.396472
2019-06-22 18:43:45.272170: precision = 0.708
After 48 training epoch, loss on training batch is 0.199606
2019-06-22 18:47:24.948438: precision = 0.696
After 49 training epoch, loss on training batch is 0.297895
2019-06-22 18:51:04.802049: precision = 0.687
After 50 training epoch, loss on training batch is 0.259828
2019-06-22 18:54:45.033728: precision = 0.699
After 1 training epoch, loss on training batch is 3.279387
2019-06-22 18:59:36.618327: precision = 0.396
After 2 training epoch, loss on training batch is 2.505262
2019-06-22 19:04:24.998463: precision = 0.490
After 3 training epoch, loss on training batch is 2.617262
2019-06-22 19:09:13.855819: precision = 0.540
After 4 training epoch, loss on training batch is 2.356251
2019-06-22 19:14:02.341502: precision = 0.581
After 5 training epoch, loss on training batch is 2.087436
2019-06-22 19:18:50.807443: precision = 0.609
After 6 training epoch, loss on training batch is 2.313877
2019-06-22 19:23:39.475479: precision = 0.626
After 7 training epoch, loss on training batch is 2.050705
2019-06-22 19:28:28.077928: precision = 0.638
After 8 training epoch, loss on training batch is 1.725691
2019-06-22 19:33:16.602075: precision = 0.659
After 9 training epoch, loss on training batch is 2.068459
2019-06-22 19:38:04.989075: precision = 0.671
After 10 training epoch, loss on training batch is 2.020174
2019-06-22 19:42:53.121478: precision = 0.681
After 11 training epoch, loss on training batch is 1.938511
2019-06-22 19:47:41.506641: precision = 0.689
After 12 training epoch, loss on training batch is 1.614206
2019-06-22 19:52:29.830350: precision = 0.696
After 13 training epoch, loss on training batch is 1.460656
2019-06-22 19:57:18.568395: precision = 0.705
After 14 training epoch, loss on training batch is 1.285221
2019-06-22 20:02:06.914880: precision = 0.709
After 15 training epoch, loss on training batch is 1.674762
2019-06-22 20:06:54.827388: precision = 0.698
After 16 training epoch, loss on training batch is 1.505355
2019-06-22 20:11:43.130763: precision = 0.701
After 17 training epoch, loss on training batch is 1.238325
2019-06-22 20:16:32.005921: precision = 0.698
After 18 training epoch, loss on training batch is 1.371084
2019-06-22 20:21:20.123894: precision = 0.700
After 19 training epoch, loss on training batch is 1.222869
2019-06-22 20:26:08.510940: precision = 0.706
After 20 training epoch, loss on training batch is 1.257743
2019-06-22 20:30:57.088624: precision = 0.717
After 21 training epoch, loss on training batch is 0.973906
2019-06-22 20:35:45.265052: precision = 0.721
After 22 training epoch, loss on training batch is 1.140676
2019-06-22 20:40:33.347662: precision = 0.722
After 23 training epoch, loss on training batch is 0.816173
2019-06-22 20:45:21.669291: precision = 0.727
After 24 training epoch, loss on training batch is 0.747113
2019-06-22 20:50:10.027095: precision = 0.723
After 25 training epoch, loss on training batch is 0.765629
2019-06-22 20:54:58.358132: precision = 0.715
After 26 training epoch, loss on training batch is 0.731661
2019-06-22 20:59:46.411605: precision = 0.708
After 27 training epoch, loss on training batch is 0.782841
2019-06-22 21:04:34.809987: precision = 0.711
After 28 training epoch, loss on training batch is 0.987505
2019-06-22 21:09:23.443823: precision = 0.701
After 29 training epoch, loss on training batch is 0.671024
2019-06-22 21:14:11.628374: precision = 0.700
After 30 training epoch, loss on training batch is 0.575397
2019-06-22 21:18:59.705872: precision = 0.700
After 31 training epoch, loss on training batch is 0.557081
2019-06-22 21:23:47.553976: precision = 0.713
After 32 training epoch, loss on training batch is 0.662949
2019-06-22 21:28:35.774225: precision = 0.721
After 33 training epoch, loss on training batch is 0.555792
2019-06-22 21:33:23.974759: precision = 0.729
After 34 training epoch, loss on training batch is 0.639974
2019-06-22 21:38:12.303347: precision = 0.739
After 35 training epoch, loss on training batch is 0.700829
2019-06-22 21:43:00.494836: precision = 0.727
After 36 training epoch, loss on training batch is 0.528105
2019-06-22 21:47:48.466593: precision = 0.717
After 37 training epoch, loss on training batch is 0.273491
2019-06-22 21:52:36.609332: precision = 0.715
After 38 training epoch, loss on training batch is 0.341684
2019-06-22 21:57:25.129743: precision = 0.715
After 39 training epoch, loss on training batch is 0.336886
2019-06-22 22:02:13.529455: precision = 0.716
After 40 training epoch, loss on training batch is 0.293963
2019-06-22 22:07:01.727423: precision = 0.721
After 41 training epoch, loss on training batch is 0.396996
2019-06-22 22:11:49.986039: precision = 0.733
After 42 training epoch, loss on training batch is 0.377794
2019-06-22 22:16:38.205647: precision = 0.728
After 43 training epoch, loss on training batch is 0.336686
2019-06-22 22:21:26.365235: precision = 0.713
After 44 training epoch, loss on training batch is 0.278029
2019-06-22 22:26:14.769363: precision = 0.707
After 45 training epoch, loss on training batch is 0.285932
2019-06-22 22:31:02.761503: precision = 0.727
After 46 training epoch, loss on training batch is 0.316224
2019-06-22 22:35:50.952038: precision = 0.717
After 47 training epoch, loss on training batch is 0.247508
2019-06-22 22:40:38.752967: precision = 0.716
After 48 training epoch, loss on training batch is 0.315616
2019-06-22 22:45:26.609940: precision = 0.697
After 49 training epoch, loss on training batch is 0.155996
2019-06-22 22:50:14.348718: precision = 0.714
After 50 training epoch, loss on training batch is 0.197078
2019-06-22 22:55:02.637144: precision = 0.724
After 1 training epoch, loss on training batch is 3.106571
2019-06-22 22:59:54.049972: precision = 0.444
After 2 training epoch, loss on training batch is 2.412126
2019-06-22 23:04:44.763305: precision = 0.529
After 3 training epoch, loss on training batch is 2.625043
2019-06-22 23:09:34.938397: precision = 0.557
After 4 training epoch, loss on training batch is 2.239726
2019-06-22 23:14:25.533184: precision = 0.591
After 5 training epoch, loss on training batch is 1.914638
2019-06-22 23:19:15.800624: precision = 0.616
After 6 training epoch, loss on training batch is 2.200782
2019-06-22 23:24:05.856064: precision = 0.637
After 7 training epoch, loss on training batch is 2.011276
2019-06-22 23:28:55.869314: precision = 0.650
After 8 training epoch, loss on training batch is 1.849346
2019-06-22 23:33:46.337458: precision = 0.655
After 9 training epoch, loss on training batch is 2.191847
2019-06-22 23:38:36.530003: precision = 0.660
After 10 training epoch, loss on training batch is 2.067921
2019-06-22 23:43:26.709588: precision = 0.666
After 11 training epoch, loss on training batch is 1.772613
2019-06-22 23:48:16.968511: precision = 0.678
After 12 training epoch, loss on training batch is 1.568304
2019-06-22 23:53:06.984501: precision = 0.696
After 13 training epoch, loss on training batch is 1.522057
2019-06-22 23:57:56.939402: precision = 0.700
After 14 training epoch, loss on training batch is 1.205792
2019-06-23 00:02:47.406777: precision = 0.709
After 15 training epoch, loss on training batch is 1.450831
2019-06-23 00:07:37.638732: precision = 0.705
After 16 training epoch, loss on training batch is 1.160864
2019-06-23 00:12:27.859648: precision = 0.708
After 17 training epoch, loss on training batch is 1.199279
2019-06-23 00:17:18.174014: precision = 0.716
After 18 training epoch, loss on training batch is 1.395794
2019-06-23 00:22:08.127266: precision = 0.719
After 19 training epoch, loss on training batch is 1.310810
2019-06-23 00:26:58.678491: precision = 0.725
After 20 training epoch, loss on training batch is 1.374584
2019-06-23 00:31:49.165746: precision = 0.725
After 21 training epoch, loss on training batch is 0.952557
2019-06-23 00:36:39.271195: precision = 0.728
After 22 training epoch, loss on training batch is 1.049113
2019-06-23 00:41:29.171898: precision = 0.725
After 23 training epoch, loss on training batch is 0.955492
2019-06-23 00:46:19.542180: precision = 0.733
After 24 training epoch, loss on training batch is 0.803436
2019-06-23 00:51:09.513059: precision = 0.725
After 25 training epoch, loss on training batch is 0.769767
2019-06-23 00:55:59.958968: precision = 0.722
After 26 training epoch, loss on training batch is 0.885289
2019-06-23 01:00:50.373584: precision = 0.719
After 27 training epoch, loss on training batch is 0.806217
2019-06-23 01:05:40.353319: precision = 0.706
After 28 training epoch, loss on training batch is 0.817589
2019-06-23 01:10:30.721587: precision = 0.695
After 29 training epoch, loss on training batch is 0.720308
2019-06-23 01:15:21.233583: precision = 0.709
After 30 training epoch, loss on training batch is 0.641142
2019-06-23 01:20:11.150466: precision = 0.714
After 31 training epoch, loss on training batch is 0.723330
2019-06-23 01:25:00.979510: precision = 0.708
After 32 training epoch, loss on training batch is 0.691624
2019-06-23 01:29:50.696923: precision = 0.708
After 33 training epoch, loss on training batch is 0.699490
2019-06-23 01:34:41.029843: precision = 0.710
After 34 training epoch, loss on training batch is 0.636318
2019-06-23 01:39:31.168205: precision = 0.715
After 35 training epoch, loss on training batch is 0.914771
2019-06-23 01:44:21.352664: precision = 0.695
After 36 training epoch, loss on training batch is 0.457746
2019-06-23 01:49:11.716964: precision = 0.709
After 37 training epoch, loss on training batch is 0.242238
2019-06-23 01:54:01.451422: precision = 0.709
After 38 training epoch, loss on training batch is 0.241572
2019-06-23 01:58:51.697536: precision = 0.727
After 39 training epoch, loss on training batch is 0.258438
2019-06-23 02:03:41.961485: precision = 0.726
After 40 training epoch, loss on training batch is 0.359628
2019-06-23 02:08:31.895691: precision = 0.726
After 41 training epoch, loss on training batch is 0.324519
2019-06-23 02:13:22.258460: precision = 0.730
After 42 training epoch, loss on training batch is 0.343552
2019-06-23 02:18:12.523794: precision = 0.703
After 43 training epoch, loss on training batch is 0.173752
2019-06-23 02:23:02.575058: precision = 0.699
After 44 training epoch, loss on training batch is 0.280831
2019-06-23 02:27:52.868961: precision = 0.726
After 45 training epoch, loss on training batch is 0.155632
2019-06-23 02:32:42.644548: precision = 0.724
After 46 training epoch, loss on training batch is 0.170975
2019-06-23 02:37:32.770419: precision = 0.725
After 47 training epoch, loss on training batch is 0.094405
2019-06-23 02:42:22.613953: precision = 0.713
After 48 training epoch, loss on training batch is 0.174720
2019-06-23 02:47:12.562719: precision = 0.720
After 49 training epoch, loss on training batch is 0.226361
2019-06-23 02:52:02.438967: precision = 0.712
After 50 training epoch, loss on training batch is 0.143964
2019-06-23 02:56:52.685507: precision = 0.714
After 1 training epoch, loss on training batch is 3.205311
2019-06-23 03:01:43.723808: precision = 0.412
After 2 training epoch, loss on training batch is 2.418725
2019-06-23 03:06:33.242980: precision = 0.510
After 3 training epoch, loss on training batch is 2.599890
2019-06-23 03:11:22.290118: precision = 0.539
After 4 training epoch, loss on training batch is 2.399077
2019-06-23 03:16:11.463000: precision = 0.585
After 5 training epoch, loss on training batch is 1.944253
2019-06-23 03:21:00.932459: precision = 0.608
After 6 training epoch, loss on training batch is 2.224044
2019-06-23 03:25:50.335909: precision = 0.625
After 7 training epoch, loss on training batch is 2.041415
2019-06-23 03:30:39.208171: precision = 0.641
After 8 training epoch, loss on training batch is 1.894640
2019-06-23 03:35:28.429360: precision = 0.656
After 9 training epoch, loss on training batch is 2.251794
2019-06-23 03:40:17.346862: precision = 0.664
After 10 training epoch, loss on training batch is 1.993920
2019-06-23 03:45:06.641941: precision = 0.671
After 11 training epoch, loss on training batch is 1.840104
2019-06-23 03:49:55.794581: precision = 0.682
After 12 training epoch, loss on training batch is 1.740586
2019-06-23 03:54:44.517874: precision = 0.687
After 13 training epoch, loss on training batch is 1.490710
2019-06-23 03:59:33.504599: precision = 0.697
After 14 training epoch, loss on training batch is 1.273326
2019-06-23 04:04:23.048449: precision = 0.702
After 15 training epoch, loss on training batch is 1.706394
2019-06-23 04:09:11.909959: precision = 0.701
After 16 training epoch, loss on training batch is 1.317723
2019-06-23 04:14:01.199920: precision = 0.700
After 17 training epoch, loss on training batch is 1.207400
2019-06-23 04:18:50.458656: precision = 0.696
After 18 training epoch, loss on training batch is 1.489515
2019-06-23 04:23:39.375667: precision = 0.692
After 19 training epoch, loss on training batch is 1.389177
2019-06-23 04:28:28.539264: precision = 0.689
After 20 training epoch, loss on training batch is 1.339557
2019-06-23 04:33:17.739062: precision = 0.699
After 21 training epoch, loss on training batch is 0.915160
2019-06-23 04:38:06.931416: precision = 0.708
After 22 training epoch, loss on training batch is 1.182192
2019-06-23 04:42:56.208599: precision = 0.714
After 23 training epoch, loss on training batch is 1.066757
2019-06-23 04:47:45.863628: precision = 0.715
After 24 training epoch, loss on training batch is 0.868471
2019-06-23 04:52:35.174104: precision = 0.710
After 25 training epoch, loss on training batch is 0.695754
2019-06-23 04:57:24.575343: precision = 0.704
After 26 training epoch, loss on training batch is 0.836785
2019-06-23 05:02:13.950625: precision = 0.696
After 27 training epoch, loss on training batch is 0.821339
2019-06-23 05:07:02.768224: precision = 0.691
After 28 training epoch, loss on training batch is 0.745618
2019-06-23 05:11:51.875549: precision = 0.696
After 29 training epoch, loss on training batch is 0.625169
2019-06-23 05:16:40.631632: precision = 0.705
After 30 training epoch, loss on training batch is 0.507749
2019-06-23 05:21:29.397334: precision = 0.714
After 31 training epoch, loss on training batch is 0.662834
2019-06-23 05:26:18.699006: precision = 0.722
After 32 training epoch, loss on training batch is 0.705006
2019-06-23 05:31:07.596703: precision = 0.719
After 33 training epoch, loss on training batch is 0.740219
2019-06-23 05:35:56.820301: precision = 0.709
After 34 training epoch, loss on training batch is 0.614097
2019-06-23 05:40:45.748310: precision = 0.712
After 35 training epoch, loss on training batch is 0.617388
2019-06-23 05:45:34.630267: precision = 0.712
After 36 training epoch, loss on training batch is 0.506807
2019-06-23 05:50:23.327734: precision = 0.703
After 37 training epoch, loss on training batch is 0.459776
2019-06-23 05:55:12.240210: precision = 0.698
After 38 training epoch, loss on training batch is 0.459068
2019-06-23 06:00:01.315500: precision = 0.703
After 39 training epoch, loss on training batch is 0.285185
2019-06-23 06:04:50.594893: precision = 0.707
After 40 training epoch, loss on training batch is 0.330632
2019-06-23 06:09:39.810173: precision = 0.709
After 41 training epoch, loss on training batch is 0.278879
2019-06-23 06:14:28.787310: precision = 0.716
After 42 training epoch, loss on training batch is 0.298874
2019-06-23 06:19:17.732710: precision = 0.713
After 43 training epoch, loss on training batch is 0.294673
2019-06-23 06:24:06.308498: precision = 0.714
After 44 training epoch, loss on training batch is 0.259982
2019-06-23 06:28:55.319843: precision = 0.712
After 45 training epoch, loss on training batch is 0.300885
2019-06-23 06:33:44.354106: precision = 0.700
After 46 training epoch, loss on training batch is 0.364407
2019-06-23 06:38:33.273461: precision = 0.697
After 47 training epoch, loss on training batch is 0.136819
2019-06-23 06:43:22.346204: precision = 0.698
After 48 training epoch, loss on training batch is 0.163558
2019-06-23 06:48:11.220298: precision = 0.704
After 49 training epoch, loss on training batch is 0.161076
2019-06-23 06:53:00.072368: precision = 0.705
After 50 training epoch, loss on training batch is 0.258650
2019-06-23 06:57:49.454829: precision = 0.700
After 1 training epoch, loss on training batch is 3.222275
2019-06-23 07:02:39.945250: precision = 0.437
After 2 training epoch, loss on training batch is 2.517301
2019-06-23 07:07:29.265271: precision = 0.492
After 3 training epoch, loss on training batch is 2.598556
2019-06-23 07:12:19.212232: precision = 0.541
After 4 training epoch, loss on training batch is 2.283707
2019-06-23 07:17:08.715979: precision = 0.597
After 5 training epoch, loss on training batch is 1.947412
2019-06-23 07:21:58.482857: precision = 0.622
After 6 training epoch, loss on training batch is 2.192076
2019-06-23 07:26:48.302315: precision = 0.625
After 7 training epoch, loss on training batch is 2.070290
2019-06-23 07:31:37.699312: precision = 0.648
After 8 training epoch, loss on training batch is 1.801931
2019-06-23 07:36:27.531462: precision = 0.650
After 9 training epoch, loss on training batch is 2.052546
2019-06-23 07:41:16.899795: precision = 0.656
After 10 training epoch, loss on training batch is 1.982389
2019-06-23 07:46:06.482132: precision = 0.664
After 11 training epoch, loss on training batch is 1.886126
2019-06-23 07:50:55.902136: precision = 0.680
After 12 training epoch, loss on training batch is 1.599536
2019-06-23 07:55:45.478618: precision = 0.686
After 13 training epoch, loss on training batch is 1.585126
2019-06-23 08:00:34.931499: precision = 0.687
After 14 training epoch, loss on training batch is 1.303272
2019-06-23 08:05:24.135544: precision = 0.687
After 15 training epoch, loss on training batch is 1.514436
2019-06-23 08:10:13.475700: precision = 0.696
After 16 training epoch, loss on training batch is 1.275154
2019-06-23 08:15:03.254712: precision = 0.701
After 17 training epoch, loss on training batch is 1.212397
2019-06-23 08:19:52.610037: precision = 0.703
After 18 training epoch, loss on training batch is 1.318888
2019-06-23 08:24:41.989707: precision = 0.712
After 19 training epoch, loss on training batch is 1.344054
2019-06-23 08:29:31.552776: precision = 0.715
After 20 training epoch, loss on training batch is 1.320269
2019-06-23 08:34:20.825256: precision = 0.722
After 21 training epoch, loss on training batch is 1.021641
2019-06-23 08:39:09.948518: precision = 0.722
After 22 training epoch, loss on training batch is 1.170479
2019-06-23 08:43:59.420996: precision = 0.723
After 23 training epoch, loss on training batch is 0.828548
2019-06-23 08:48:48.946535: precision = 0.719
After 24 training epoch, loss on training batch is 0.667108
2019-06-23 08:53:38.411525: precision = 0.718
After 25 training epoch, loss on training batch is 0.673917
2019-06-23 08:58:27.514350: precision = 0.721
After 26 training epoch, loss on training batch is 0.764841
2019-06-23 09:03:17.139096: precision = 0.719
After 27 training epoch, loss on training batch is 0.834957
2019-06-23 09:08:06.688090: precision = 0.717
After 28 training epoch, loss on training batch is 0.748796
2019-06-23 09:12:56.175017: precision = 0.712
After 29 training epoch, loss on training batch is 0.702303
2019-06-23 09:17:45.533420: precision = 0.712
After 30 training epoch, loss on training batch is 0.450636
2019-06-23 09:22:34.884072: precision = 0.707
After 31 training epoch, loss on training batch is 0.650060
2019-06-23 09:27:24.171208: precision = 0.711
After 32 training epoch, loss on training batch is 0.668848
2019-06-23 09:32:13.231747: precision = 0.706
After 33 training epoch, loss on training batch is 0.666731
2019-06-23 09:37:02.705605: precision = 0.701
After 34 training epoch, loss on training batch is 0.431356
2019-06-23 09:41:52.114948: precision = 0.710
After 35 training epoch, loss on training batch is 0.510290
2019-06-23 09:46:41.185161: precision = 0.716
After 36 training epoch, loss on training batch is 0.754654
2019-06-23 09:51:30.121080: precision = 0.721
After 37 training epoch, loss on training batch is 0.315973
2019-06-23 09:56:19.250077: precision = 0.720
After 38 training epoch, loss on training batch is 0.223935
2019-06-23 10:01:08.418788: precision = 0.726
After 39 training epoch, loss on training batch is 0.316324
2019-06-23 10:05:57.587207: precision = 0.716
After 40 training epoch, loss on training batch is 0.350377
2019-06-23 10:10:47.077886: precision = 0.715
After 41 training epoch, loss on training batch is 0.324364
2019-06-23 10:15:36.207051: precision = 0.709
After 42 training epoch, loss on training batch is 0.361691
2019-06-23 10:20:25.460098: precision = 0.715
After 43 training epoch, loss on training batch is 0.250092
2019-06-23 10:25:15.035610: precision = 0.711
After 44 training epoch, loss on training batch is 0.435135
2019-06-23 10:30:04.204686: precision = 0.732
After 45 training epoch, loss on training batch is 0.321476
2019-06-23 10:34:53.515341: precision = 0.717
After 46 training epoch, loss on training batch is 0.209192
2019-06-23 10:39:42.655048: precision = 0.721
After 47 training epoch, loss on training batch is 0.192621
2019-06-23 10:44:31.830514: precision = 0.716
After 48 training epoch, loss on training batch is 0.201179
2019-06-23 10:49:20.886040: precision = 0.715
After 49 training epoch, loss on training batch is 0.185492
2019-06-23 10:54:09.593366: precision = 0.725
After 50 training epoch, loss on training batch is 0.117589
2019-06-23 10:58:59.091756: precision = 0.720
After 1 training epoch, loss on training batch is 3.157528
2019-06-23 11:03:47.416431: precision = 0.433
After 2 training epoch, loss on training batch is 2.402939
2019-06-23 11:08:34.696453: precision = 0.500
After 3 training epoch, loss on training batch is 2.579953
2019-06-23 11:13:21.464498: precision = 0.549
After 4 training epoch, loss on training batch is 2.436522
2019-06-23 11:18:08.474713: precision = 0.586
After 5 training epoch, loss on training batch is 1.962488
2019-06-23 11:22:55.762834: precision = 0.613
After 6 training epoch, loss on training batch is 2.327363
2019-06-23 11:27:42.471113: precision = 0.621
After 7 training epoch, loss on training batch is 2.118689
2019-06-23 11:32:29.465757: precision = 0.642
After 8 training epoch, loss on training batch is 1.881252
2019-06-23 11:37:16.847748: precision = 0.653
After 9 training epoch, loss on training batch is 2.233684
2019-06-23 11:42:03.665549: precision = 0.667
After 10 training epoch, loss on training batch is 2.172256
2019-06-23 11:46:50.731405: precision = 0.668
After 11 training epoch, loss on training batch is 2.083494
2019-06-23 11:51:37.350073: precision = 0.667
After 12 training epoch, loss on training batch is 1.672988
2019-06-23 11:56:24.718951: precision = 0.674
After 13 training epoch, loss on training batch is 1.552561
2019-06-23 12:01:11.914282: precision = 0.684
After 14 training epoch, loss on training batch is 1.286541
2019-06-23 12:05:58.842963: precision = 0.687
After 15 training epoch, loss on training batch is 1.523309
2019-06-23 12:10:45.915596: precision = 0.694
After 16 training epoch, loss on training batch is 1.268079
2019-06-23 12:15:32.792632: precision = 0.702
After 17 training epoch, loss on training batch is 1.183808
2019-06-23 12:20:19.844319: precision = 0.706
After 18 training epoch, loss on training batch is 1.275578
2019-06-23 12:25:06.523784: precision = 0.715
After 19 training epoch, loss on training batch is 1.288937
2019-06-23 12:29:53.384010: precision = 0.720
After 20 training epoch, loss on training batch is 1.180038
2019-06-23 12:34:40.892882: precision = 0.729
After 21 training epoch, loss on training batch is 0.864264
2019-06-23 12:39:27.489781: precision = 0.729
After 22 training epoch, loss on training batch is 1.030073
2019-06-23 12:44:14.399736: precision = 0.735
After 23 training epoch, loss on training batch is 0.938334
2019-06-23 12:49:01.410880: precision = 0.730
After 24 training epoch, loss on training batch is 0.732521
2019-06-23 12:53:48.824844: precision = 0.730
After 25 training epoch, loss on training batch is 0.671600
2019-06-23 12:58:35.598121: precision = 0.726
After 26 training epoch, loss on training batch is 0.628726
2019-06-23 13:03:22.456611: precision = 0.728
After 27 training epoch, loss on training batch is 0.765530
2019-06-23 13:08:09.235892: precision = 0.736
After 28 training epoch, loss on training batch is 0.752545
2019-06-23 13:12:56.082062: precision = 0.725
After 29 training epoch, loss on training batch is 0.886702
2019-06-23 13:17:43.082753: precision = 0.713
After 30 training epoch, loss on training batch is 0.570376
2019-06-23 13:22:29.585187: precision = 0.700
After 31 training epoch, loss on training batch is 0.590644
2019-06-23 13:27:16.240142: precision = 0.693
After 32 training epoch, loss on training batch is 0.729362
2019-06-23 13:32:03.301273: precision = 0.692
After 33 training epoch, loss on training batch is 0.610541
2019-06-23 13:36:50.395752: precision = 0.701
After 34 training epoch, loss on training batch is 0.583662
2019-06-23 13:41:37.387300: precision = 0.709
After 35 training epoch, loss on training batch is 0.597926
2019-06-23 13:46:24.156217: precision = 0.711
After 36 training epoch, loss on training batch is 0.467645
2019-06-23 13:51:11.099383: precision = 0.721
After 37 training epoch, loss on training batch is 0.234762
2019-06-23 13:55:57.877747: precision = 0.722
After 38 training epoch, loss on training batch is 0.265268
2019-06-23 14:00:45.258127: precision = 0.723
After 39 training epoch, loss on training batch is 0.379489
2019-06-23 14:05:32.038753: precision = 0.732
After 40 training epoch, loss on training batch is 0.353955
2019-06-23 14:10:18.648925: precision = 0.728
After 41 training epoch, loss on training batch is 0.326639
2019-06-23 14:15:05.532960: precision = 0.722
After 42 training epoch, loss on training batch is 0.248374
2019-06-23 14:19:52.244353: precision = 0.724
After 43 training epoch, loss on training batch is 0.292742
2019-06-23 14:24:39.060165: precision = 0.718
After 44 training epoch, loss on training batch is 0.229144
2019-06-23 14:29:25.942774: precision = 0.725
After 45 training epoch, loss on training batch is 0.443136
2019-06-23 14:34:12.577796: precision = 0.721
After 46 training epoch, loss on training batch is 0.243643
2019-06-23 14:38:59.651918: precision = 0.714
After 47 training epoch, loss on training batch is 0.167498
2019-06-23 14:43:46.618887: precision = 0.716
After 48 training epoch, loss on training batch is 0.312706
2019-06-23 14:48:34.019736: precision = 0.710
After 49 training epoch, loss on training batch is 0.224931
2019-06-23 14:53:20.896809: precision = 0.705
After 50 training epoch, loss on training batch is 0.170757
2019-06-23 14:58:07.790892: precision = 0.703
After 1 training epoch, loss on training batch is 3.293105
2019-06-23 15:02:56.342134: precision = 0.417
After 2 training epoch, loss on training batch is 2.383167
2019-06-23 15:07:43.646458: precision = 0.522
After 3 training epoch, loss on training batch is 2.587258
2019-06-23 15:12:30.761556: precision = 0.574
After 4 training epoch, loss on training batch is 2.320468
2019-06-23 15:17:18.460646: precision = 0.593
After 5 training epoch, loss on training batch is 2.018613
2019-06-23 15:22:06.042118: precision = 0.620
After 6 training epoch, loss on training batch is 2.233123
2019-06-23 15:26:53.660870: precision = 0.639
After 7 training epoch, loss on training batch is 2.049062
2019-06-23 15:31:41.400340: precision = 0.651
After 8 training epoch, loss on training batch is 1.960761
2019-06-23 15:36:28.725196: precision = 0.656
After 9 training epoch, loss on training batch is 2.111022
2019-06-23 15:41:16.145574: precision = 0.667
After 10 training epoch, loss on training batch is 1.956277
2019-06-23 15:46:03.930549: precision = 0.666
After 11 training epoch, loss on training batch is 1.818281
2019-06-23 15:50:51.118875: precision = 0.682
After 12 training epoch, loss on training batch is 1.554582
2019-06-23 15:55:38.876944: precision = 0.685
After 13 training epoch, loss on training batch is 1.424610
2019-06-23 16:00:26.153567: precision = 0.696
After 14 training epoch, loss on training batch is 1.282813
2019-06-23 16:05:13.102682: precision = 0.704
After 15 training epoch, loss on training batch is 1.507711
2019-06-23 16:10:00.412090: precision = 0.705
After 16 training epoch, loss on training batch is 1.365855
2019-06-23 16:14:47.792126: precision = 0.707
After 17 training epoch, loss on training batch is 1.339283
2019-06-23 16:19:35.357084: precision = 0.708
After 18 training epoch, loss on training batch is 1.393716
2019-06-23 16:24:22.798967: precision = 0.706
After 19 training epoch, loss on training batch is 1.264457
2019-06-23 16:29:10.060667: precision = 0.711
After 20 training epoch, loss on training batch is 1.181198
2019-06-23 16:33:57.796645: precision = 0.715
After 21 training epoch, loss on training batch is 0.891322
2019-06-23 16:38:45.251073: precision = 0.716
After 22 training epoch, loss on training batch is 1.108436
2019-06-23 16:43:32.436760: precision = 0.718
After 23 training epoch, loss on training batch is 0.995703
2019-06-23 16:48:19.052131: precision = 0.715
After 24 training epoch, loss on training batch is 0.836587
2019-06-23 16:53:05.955576: precision = 0.717
After 25 training epoch, loss on training batch is 0.585004
2019-06-23 16:57:52.793333: precision = 0.719
After 26 training epoch, loss on training batch is 0.677043
2019-06-23 17:02:39.881493: precision = 0.717
After 27 training epoch, loss on training batch is 0.843565
2019-06-23 17:07:26.742957: precision = 0.717
After 28 training epoch, loss on training batch is 0.954206
2019-06-23 17:12:13.919003: precision = 0.695
After 29 training epoch, loss on training batch is 0.894697
2019-06-23 17:17:01.238659: precision = 0.685
After 30 training epoch, loss on training batch is 0.605225
2019-06-23 17:21:48.972591: precision = 0.679
After 31 training epoch, loss on training batch is 0.717135
2019-06-23 17:26:36.380118: precision = 0.703
After 32 training epoch, loss on training batch is 0.729502
2019-06-23 17:31:23.314122: precision = 0.719
After 33 training epoch, loss on training batch is 0.733251
2019-06-23 17:36:10.334411: precision = 0.722
After 34 training epoch, loss on training batch is 0.588184
2019-06-23 17:40:57.596036: precision = 0.714
After 35 training epoch, loss on training batch is 0.563216
2019-06-23 17:45:44.512046: precision = 0.719
After 36 training epoch, loss on training batch is 0.481188
2019-06-23 17:50:32.048908: precision = 0.708
After 37 training epoch, loss on training batch is 0.392158
2019-06-23 17:55:19.087039: precision = 0.706
After 38 training epoch, loss on training batch is 0.291924
2019-06-23 18:00:06.430897: precision = 0.696
After 39 training epoch, loss on training batch is 0.443184
2019-06-23 18:04:54.097114: precision = 0.711
After 40 training epoch, loss on training batch is 0.449405
2019-06-23 18:09:41.389984: precision = 0.716
After 41 training epoch, loss on training batch is 0.589430
2019-06-23 18:14:28.567809: precision = 0.710
After 42 training epoch, loss on training batch is 0.522245
2019-06-23 18:19:16.084198: precision = 0.703
After 43 training epoch, loss on training batch is 0.292750
2019-06-23 18:24:02.811151: precision = 0.733
After 44 training epoch, loss on training batch is 0.351900
2019-06-23 18:28:49.967682: precision = 0.731
After 45 training epoch, loss on training batch is 0.262001
2019-06-23 18:33:37.210432: precision = 0.728
After 46 training epoch, loss on training batch is 0.283654
2019-06-23 18:38:24.373844: precision = 0.726
After 47 training epoch, loss on training batch is 0.198178
2019-06-23 18:43:11.310174: precision = 0.713
After 48 training epoch, loss on training batch is 0.177072
2019-06-23 18:47:58.363476: precision = 0.715
After 49 training epoch, loss on training batch is 0.187719
2019-06-23 18:52:45.476181: precision = 0.715
After 50 training epoch, loss on training batch is 0.275592
2019-06-23 18:57:33.009054: precision = 0.699
After 1 training epoch, loss on training batch is 3.166285
2019-06-23 19:02:37.916895: precision = 0.401
After 2 training epoch, loss on training batch is 2.437594
2019-06-23 19:07:41.575440: precision = 0.472
After 3 training epoch, loss on training batch is 2.622004
2019-06-23 19:12:45.225451: precision = 0.556
After 4 training epoch, loss on training batch is 2.241819
2019-06-23 19:17:48.890524: precision = 0.573
After 5 training epoch, loss on training batch is 2.026322
2019-06-23 19:22:52.451006: precision = 0.614
After 6 training epoch, loss on training batch is 2.254579
2019-06-23 19:27:56.081267: precision = 0.626
After 7 training epoch, loss on training batch is 2.069757
2019-06-23 19:32:59.755508: precision = 0.651
After 8 training epoch, loss on training batch is 1.982849
2019-06-23 19:38:03.387927: precision = 0.661
After 9 training epoch, loss on training batch is 2.184980
2019-06-23 19:43:07.038770: precision = 0.668
After 10 training epoch, loss on training batch is 2.095011
2019-06-23 19:48:10.638493: precision = 0.679
After 11 training epoch, loss on training batch is 2.038102
2019-06-23 19:53:14.288695: precision = 0.676
After 12 training epoch, loss on training batch is 1.732125
2019-06-23 19:58:17.883663: precision = 0.672
After 13 training epoch, loss on training batch is 1.652452
2019-06-23 20:03:21.452840: precision = 0.678
After 14 training epoch, loss on training batch is 1.278692
2019-06-23 20:08:25.062577: precision = 0.692
After 15 training epoch, loss on training batch is 1.616212
2019-06-23 20:13:28.585908: precision = 0.702
After 16 training epoch, loss on training batch is 1.436417
2019-06-23 20:18:32.156095: precision = 0.708
After 17 training epoch, loss on training batch is 1.242468
2019-06-23 20:23:35.655222: precision = 0.709
After 18 training epoch, loss on training batch is 1.410992
2019-06-23 20:28:39.274866: precision = 0.704
After 19 training epoch, loss on training batch is 1.424300
2019-06-23 20:33:42.785396: precision = 0.715
After 20 training epoch, loss on training batch is 1.385335
2019-06-23 20:38:46.306097: precision = 0.718
After 21 training epoch, loss on training batch is 1.035892
2019-06-23 20:43:49.882934: precision = 0.717
After 22 training epoch, loss on training batch is 1.083625
2019-06-23 20:48:53.456348: precision = 0.722
After 23 training epoch, loss on training batch is 0.861464
2019-06-23 20:53:56.948613: precision = 0.727
After 24 training epoch, loss on training batch is 0.755904
2019-06-23 20:59:00.486881: precision = 0.732
After 25 training epoch, loss on training batch is 0.673040
2019-06-23 21:04:04.048280: precision = 0.727
After 26 training epoch, loss on training batch is 0.763445
2019-06-23 21:09:07.559678: precision = 0.719
After 27 training epoch, loss on training batch is 0.838204
2019-06-23 21:14:11.037548: precision = 0.698
After 28 training epoch, loss on training batch is 0.823461
2019-06-23 21:19:14.585102: precision = 0.684
After 29 training epoch, loss on training batch is 0.630985
2019-06-23 21:24:17.988605: precision = 0.690
After 30 training epoch, loss on training batch is 0.592342
2019-06-23 21:29:21.552932: precision = 0.707
After 31 training epoch, loss on training batch is 0.712487
2019-06-23 21:34:24.977743: precision = 0.718
After 32 training epoch, loss on training batch is 0.711946
2019-06-23 21:39:28.444440: precision = 0.724
After 33 training epoch, loss on training batch is 0.726489
2019-06-23 21:44:31.927019: precision = 0.728
After 34 training epoch, loss on training batch is 0.668223
2019-06-23 21:49:35.345775: precision = 0.722
After 35 training epoch, loss on training batch is 0.578316
2019-06-23 21:54:38.796761: precision = 0.711
After 36 training epoch, loss on training batch is 0.521862
2019-06-23 21:59:42.169401: precision = 0.724
After 37 training epoch, loss on training batch is 0.209783
2019-06-23 22:04:45.637575: precision = 0.718
After 38 training epoch, loss on training batch is 0.264014
2019-06-23 22:09:49.017527: precision = 0.719
After 39 training epoch, loss on training batch is 0.329805
2019-06-23 22:14:52.426590: precision = 0.708
After 40 training epoch, loss on training batch is 0.351134
2019-06-23 22:19:55.850778: precision = 0.710
After 41 training epoch, loss on training batch is 0.417166
2019-06-23 22:24:59.288537: precision = 0.709
After 42 training epoch, loss on training batch is 0.269066
2019-06-23 22:30:02.612715: precision = 0.707
After 43 training epoch, loss on training batch is 0.208942
2019-06-23 22:35:06.027748: precision = 0.724
After 44 training epoch, loss on training batch is 0.255839
2019-06-23 22:40:09.507075: precision = 0.721
After 45 training epoch, loss on training batch is 0.272976
2019-06-23 22:45:12.874736: precision = 0.724
After 46 training epoch, loss on training batch is 0.242075
2019-06-23 22:50:16.191287: precision = 0.713
After 47 training epoch, loss on training batch is 0.302909
2019-06-23 22:55:19.505944: precision = 0.712
After 48 training epoch, loss on training batch is 0.218047
2019-06-23 23:00:22.829169: precision = 0.700
After 49 training epoch, loss on training batch is 0.150628
2019-06-23 23:05:26.159594: precision = 0.694
After 50 training epoch, loss on training batch is 0.188781
2019-06-23 23:10:29.548941: precision = 0.710
After 1 training epoch, loss on training batch is 3.252591
2019-06-23 23:15:34.156701: precision = 0.415
After 2 training epoch, loss on training batch is 2.515680
2019-06-23 23:20:37.226892: precision = 0.512
After 3 training epoch, loss on training batch is 2.612960
2019-06-23 23:25:40.258010: precision = 0.559
After 4 training epoch, loss on training batch is 2.238353
2019-06-23 23:30:43.402990: precision = 0.582
After 5 training epoch, loss on training batch is 2.040580
2019-06-23 23:35:46.447708: precision = 0.618
After 6 training epoch, loss on training batch is 2.335621
2019-06-23 23:40:49.480269: precision = 0.630
After 7 training epoch, loss on training batch is 2.068169
2019-06-23 23:45:52.480529: precision = 0.647
After 8 training epoch, loss on training batch is 1.849757
2019-06-23 23:50:55.464522: precision = 0.658
After 9 training epoch, loss on training batch is 2.134406
2019-06-23 23:55:58.543494: precision = 0.666
After 10 training epoch, loss on training batch is 2.158100
2019-06-24 00:01:01.573441: precision = 0.672
After 11 training epoch, loss on training batch is 2.012414
2019-06-24 00:06:04.599421: precision = 0.689
After 12 training epoch, loss on training batch is 1.605075
2019-06-24 00:11:07.667458: precision = 0.694
After 13 training epoch, loss on training batch is 1.610229
2019-06-24 00:16:10.676481: precision = 0.696
After 14 training epoch, loss on training batch is 1.255403
2019-06-24 00:21:13.678726: precision = 0.701
After 15 training epoch, loss on training batch is 1.541720
2019-06-24 00:26:16.756236: precision = 0.703
After 16 training epoch, loss on training batch is 1.419319
2019-06-24 00:31:19.849314: precision = 0.705
After 17 training epoch, loss on training batch is 1.240274
2019-06-24 00:36:22.877185: precision = 0.707
After 18 training epoch, loss on training batch is 1.479024
2019-06-24 00:41:25.903935: precision = 0.710
After 19 training epoch, loss on training batch is 1.338764
2019-06-24 00:46:28.804894: precision = 0.719
After 20 training epoch, loss on training batch is 1.348804
2019-06-24 00:51:31.750839: precision = 0.727
After 21 training epoch, loss on training batch is 1.008792
2019-06-24 00:56:34.766591: precision = 0.718
After 22 training epoch, loss on training batch is 1.059625
2019-06-24 01:01:37.707665: precision = 0.725
After 23 training epoch, loss on training batch is 0.878686
2019-06-24 01:06:40.586013: precision = 0.724
After 24 training epoch, loss on training batch is 0.723480
2019-06-24 01:11:43.535572: precision = 0.722
After 25 training epoch, loss on training batch is 0.694322
2019-06-24 01:16:46.521991: precision = 0.722
After 26 training epoch, loss on training batch is 0.773598
2019-06-24 01:21:49.422885: precision = 0.728
After 27 training epoch, loss on training batch is 0.650362
2019-06-24 01:26:52.358518: precision = 0.729
After 28 training epoch, loss on training batch is 0.682177
2019-06-24 01:31:55.222646: precision = 0.726
After 29 training epoch, loss on training batch is 0.803028
2019-06-24 01:36:58.109679: precision = 0.711
After 30 training epoch, loss on training batch is 0.643945
2019-06-24 01:42:01.061164: precision = 0.698
After 31 training epoch, loss on training batch is 0.609101
2019-06-24 01:47:03.969292: precision = 0.713
After 32 training epoch, loss on training batch is 0.706736
2019-06-24 01:52:06.849451: precision = 0.717
After 33 training epoch, loss on training batch is 0.653698
2019-06-24 01:57:09.731127: precision = 0.717
After 34 training epoch, loss on training batch is 0.607639
2019-06-24 02:02:12.604985: precision = 0.725
After 35 training epoch, loss on training batch is 0.566200
2019-06-24 02:07:15.546000: precision = 0.729
After 36 training epoch, loss on training batch is 0.537621
2019-06-24 02:12:18.439455: precision = 0.725
After 37 training epoch, loss on training batch is 0.365924
2019-06-24 02:17:21.274605: precision = 0.732
After 38 training epoch, loss on training batch is 0.438660
2019-06-24 02:22:24.146475: precision = 0.720
After 39 training epoch, loss on training batch is 0.435728
2019-06-24 02:27:27.006221: precision = 0.702
After 40 training epoch, loss on training batch is 0.408072
2019-06-24 02:32:29.881999: precision = 0.713
After 41 training epoch, loss on training batch is 0.348121
2019-06-24 02:37:32.724044: precision = 0.726
After 42 training epoch, loss on training batch is 0.285754
2019-06-24 02:42:35.654466: precision = 0.727
After 43 training epoch, loss on training batch is 0.286679
2019-06-24 02:47:38.477834: precision = 0.708
After 44 training epoch, loss on training batch is 0.377148
2019-06-24 02:52:41.271653: precision = 0.708
After 45 training epoch, loss on training batch is 0.274411
2019-06-24 02:57:44.109381: precision = 0.712
After 46 training epoch, loss on training batch is 0.167486
2019-06-24 03:02:46.952601: precision = 0.712
After 47 training epoch, loss on training batch is 0.263765
2019-06-24 03:07:49.772282: precision = 0.732
After 48 training epoch, loss on training batch is 0.383637
2019-06-24 03:12:52.614903: precision = 0.729
After 49 training epoch, loss on training batch is 0.238912
2019-06-24 03:17:55.342688: precision = 0.721
After 50 training epoch, loss on training batch is 0.150596
2019-06-24 03:22:58.142447: precision = 0.719
After 1 training epoch, loss on training batch is 3.203386
2019-06-24 03:27:58.484291: precision = 0.428
After 2 training epoch, loss on training batch is 2.450946
2019-06-24 03:32:57.554931: precision = 0.503
After 3 training epoch, loss on training batch is 2.522901
2019-06-24 03:37:56.706906: precision = 0.553
After 4 training epoch, loss on training batch is 2.252649
2019-06-24 03:42:55.700225: precision = 0.588
After 5 training epoch, loss on training batch is 1.976777
2019-06-24 03:47:54.686838: precision = 0.623
After 6 training epoch, loss on training batch is 2.119396
2019-06-24 03:52:53.844112: precision = 0.633
After 7 training epoch, loss on training batch is 2.033383
2019-06-24 03:57:52.895064: precision = 0.649
After 8 training epoch, loss on training batch is 1.843726
2019-06-24 04:02:51.915496: precision = 0.662
After 9 training epoch, loss on training batch is 1.984069
2019-06-24 04:07:50.969074: precision = 0.675
After 10 training epoch, loss on training batch is 1.889600
2019-06-24 04:12:50.011026: precision = 0.685
After 11 training epoch, loss on training batch is 1.850707
2019-06-24 04:17:49.067756: precision = 0.697
After 12 training epoch, loss on training batch is 1.530159
2019-06-24 04:22:48.110106: precision = 0.699
After 13 training epoch, loss on training batch is 1.526815
2019-06-24 04:27:47.100976: precision = 0.699
After 14 training epoch, loss on training batch is 1.231649
2019-06-24 04:32:46.216874: precision = 0.709
After 15 training epoch, loss on training batch is 1.573713
2019-06-24 04:37:45.305423: precision = 0.708
After 16 training epoch, loss on training batch is 1.406990
2019-06-24 04:42:44.364862: precision = 0.705
After 17 training epoch, loss on training batch is 1.187420
2019-06-24 04:47:43.311385: precision = 0.708
After 18 training epoch, loss on training batch is 1.379624
2019-06-24 04:52:42.200959: precision = 0.711
After 19 training epoch, loss on training batch is 1.336475
2019-06-24 04:57:41.102840: precision = 0.722
After 20 training epoch, loss on training batch is 1.286729
2019-06-24 05:02:40.060139: precision = 0.724
After 21 training epoch, loss on training batch is 1.068006
2019-06-24 05:07:39.103339: precision = 0.719
After 22 training epoch, loss on training batch is 1.114857
2019-06-24 05:12:38.160951: precision = 0.723
After 23 training epoch, loss on training batch is 0.816800
2019-06-24 05:17:37.135341: precision = 0.728
After 24 training epoch, loss on training batch is 0.720785
2019-06-24 05:22:36.227108: precision = 0.732
After 25 training epoch, loss on training batch is 0.752129
2019-06-24 05:27:35.277009: precision = 0.738
After 26 training epoch, loss on training batch is 0.797254
2019-06-24 05:32:34.284673: precision = 0.727
After 27 training epoch, loss on training batch is 0.954907
2019-06-24 05:37:33.260299: precision = 0.705
After 28 training epoch, loss on training batch is 0.876654
2019-06-24 05:42:32.216908: precision = 0.700
After 29 training epoch, loss on training batch is 0.718550
2019-06-24 05:47:31.178862: precision = 0.705
After 30 training epoch, loss on training batch is 0.559842
2019-06-24 05:52:30.086328: precision = 0.718
After 31 training epoch, loss on training batch is 0.728731
2019-06-24 05:57:29.052557: precision = 0.726
After 32 training epoch, loss on training batch is 0.690924
2019-06-24 06:02:28.023329: precision = 0.724
After 33 training epoch, loss on training batch is 0.584998
2019-06-24 06:07:26.879412: precision = 0.726
After 34 training epoch, loss on training batch is 0.451003
2019-06-24 06:12:25.782227: precision = 0.732
After 35 training epoch, loss on training batch is 0.425452
2019-06-24 06:17:24.744084: precision = 0.732
After 36 training epoch, loss on training batch is 0.431194
2019-06-24 06:22:23.656487: precision = 0.723
After 37 training epoch, loss on training batch is 0.264167
2019-06-24 06:27:22.607450: precision = 0.719
After 38 training epoch, loss on training batch is 0.224405
2019-06-24 06:32:21.522854: precision = 0.717
After 39 training epoch, loss on training batch is 0.236783
2019-06-24 06:37:20.505685: precision = 0.723
After 40 training epoch, loss on training batch is 0.283846
2019-06-24 06:42:19.464515: precision = 0.724
After 41 training epoch, loss on training batch is 0.313773
2019-06-24 06:47:18.362272: precision = 0.716
After 42 training epoch, loss on training batch is 0.249162
2019-06-24 06:52:17.268845: precision = 0.718
After 43 training epoch, loss on training batch is 0.206567
2019-06-24 06:57:16.128505: precision = 0.725
After 44 training epoch, loss on training batch is 0.266907
2019-06-24 07:02:14.963995: precision = 0.730
After 45 training epoch, loss on training batch is 0.416011
2019-06-24 07:07:13.824408: precision = 0.726
After 46 training epoch, loss on training batch is 0.153705
2019-06-24 07:12:12.820113: precision = 0.713
After 47 training epoch, loss on training batch is 0.151099
2019-06-24 07:17:11.664164: precision = 0.720
After 48 training epoch, loss on training batch is 0.161730
2019-06-24 07:22:10.498124: precision = 0.726
After 49 training epoch, loss on training batch is 0.224235
2019-06-24 07:27:09.328180: precision = 0.717
After 50 training epoch, loss on training batch is 0.209111
2019-06-24 07:32:08.268887: precision = 0.716
After 1 training epoch, loss on training batch is 3.355012
2019-06-24 07:37:07.875617: precision = 0.429
After 2 training epoch, loss on training batch is 2.556544
2019-06-24 07:42:06.180259: precision = 0.487
After 3 training epoch, loss on training batch is 2.623445
2019-06-24 07:47:04.426392: precision = 0.533
After 4 training epoch, loss on training batch is 2.237352
2019-06-24 07:52:02.705065: precision = 0.577
After 5 training epoch, loss on training batch is 1.936825
2019-06-24 07:57:00.984274: precision = 0.610
After 6 training epoch, loss on training batch is 2.318026
2019-06-24 08:01:59.222034: precision = 0.631
After 7 training epoch, loss on training batch is 2.041140
2019-06-24 08:06:57.391479: precision = 0.645
After 8 training epoch, loss on training batch is 1.720520
2019-06-24 08:11:55.694856: precision = 0.655
After 9 training epoch, loss on training batch is 2.099431
2019-06-24 08:16:53.986532: precision = 0.673
After 10 training epoch, loss on training batch is 1.976453
2019-06-24 08:21:52.128687: precision = 0.685
After 11 training epoch, loss on training batch is 1.881095
2019-06-24 08:26:50.307491: precision = 0.679
After 12 training epoch, loss on training batch is 1.574789
2019-06-24 08:31:48.460696: precision = 0.679
After 13 training epoch, loss on training batch is 1.555993
2019-06-24 08:36:46.690577: precision = 0.690
After 14 training epoch, loss on training batch is 1.337456
2019-06-24 08:41:44.951414: precision = 0.700
After 15 training epoch, loss on training batch is 1.632023
2019-06-24 08:46:43.123316: precision = 0.698
After 16 training epoch, loss on training batch is 1.348039
2019-06-24 08:51:41.254698: precision = 0.705
After 17 training epoch, loss on training batch is 1.235731
2019-06-24 08:56:39.459598: precision = 0.706
After 18 training epoch, loss on training batch is 1.357822
2019-06-24 09:01:37.700022: precision = 0.709
After 19 training epoch, loss on training batch is 1.342659
2019-06-24 09:06:35.913279: precision = 0.713
After 20 training epoch, loss on training batch is 1.397599
2019-06-24 09:11:34.158320: precision = 0.716
After 21 training epoch, loss on training batch is 0.950997
2019-06-24 09:16:32.397482: precision = 0.720
After 22 training epoch, loss on training batch is 1.046510
2019-06-24 09:21:30.567799: precision = 0.721
After 23 training epoch, loss on training batch is 0.849245
2019-06-24 09:26:28.690906: precision = 0.720
After 24 training epoch, loss on training batch is 0.789214
2019-06-24 09:31:26.879750: precision = 0.723
After 25 training epoch, loss on training batch is 0.750497
2019-06-24 09:36:25.018677: precision = 0.733
After 26 training epoch, loss on training batch is 0.733348
2019-06-24 09:41:23.141970: precision = 0.735
After 27 training epoch, loss on training batch is 0.725052
2019-06-24 09:46:21.200287: precision = 0.730
After 28 training epoch, loss on training batch is 0.783660
2019-06-24 09:51:19.301023: precision = 0.714
After 29 training epoch, loss on training batch is 0.667654
2019-06-24 09:56:17.488738: precision = 0.704
After 30 training epoch, loss on training batch is 0.622902
2019-06-24 10:01:15.594951: precision = 0.699
After 31 training epoch, loss on training batch is 0.598959
2019-06-24 10:06:13.690970: precision = 0.708
After 32 training epoch, loss on training batch is 0.571972
2019-06-24 10:11:11.708799: precision = 0.721
After 33 training epoch, loss on training batch is 0.531959
2019-06-24 10:16:09.858822: precision = 0.727
After 34 training epoch, loss on training batch is 0.531674
2019-06-24 10:21:08.043800: precision = 0.727
After 35 training epoch, loss on training batch is 0.803250
2019-06-24 10:26:06.176054: precision = 0.713
After 36 training epoch, loss on training batch is 0.638431
2019-06-24 10:31:04.244937: precision = 0.704
After 37 training epoch, loss on training batch is 0.350680
2019-06-24 10:36:02.303599: precision = 0.704
After 38 training epoch, loss on training batch is 0.402575
2019-06-24 10:41:00.431024: precision = 0.710
After 39 training epoch, loss on training batch is 0.317525
2019-06-24 10:45:58.482199: precision = 0.720
After 40 training epoch, loss on training batch is 0.349360
2019-06-24 10:50:56.497713: precision = 0.710
After 41 training epoch, loss on training batch is 0.409580
2019-06-24 10:55:54.686049: precision = 0.700
After 42 training epoch, loss on training batch is 0.464594
2019-06-24 11:00:52.740646: precision = 0.678
After 43 training epoch, loss on training batch is 0.322825
2019-06-24 11:05:50.832321: precision = 0.683
After 44 training epoch, loss on training batch is 0.673753
2019-06-24 11:10:48.832577: precision = 0.715
After 45 training epoch, loss on training batch is 0.350651
2019-06-24 11:15:46.844138: precision = 0.720
After 46 training epoch, loss on training batch is 0.447785
2019-06-24 11:20:44.855953: precision = 0.717
After 47 training epoch, loss on training batch is 0.268392
2019-06-24 11:25:42.921709: precision = 0.717
After 48 training epoch, loss on training batch is 0.451535
2019-06-24 11:30:40.928737: precision = 0.694
After 49 training epoch, loss on training batch is 0.211721
2019-06-24 11:35:38.963956: precision = 0.692
After 50 training epoch, loss on training batch is 0.090621
2019-06-24 11:40:36.956333: precision = 0.704
After 1 training epoch, loss on training batch is 3.271929
2019-06-24 11:45:30.917169: precision = 0.415
After 2 training epoch, loss on training batch is 2.484278
2019-06-24 11:50:23.114751: precision = 0.504
After 3 training epoch, loss on training batch is 2.582374
2019-06-24 11:55:15.382800: precision = 0.529
After 4 training epoch, loss on training batch is 2.271302
2019-06-24 12:00:07.694466: precision = 0.567
After 5 training epoch, loss on training batch is 2.154614
2019-06-24 12:04:59.965891: precision = 0.610
After 6 training epoch, loss on training batch is 2.353621
2019-06-24 12:09:52.224559: precision = 0.612
After 7 training epoch, loss on training batch is 2.076733
2019-06-24 12:14:44.602980: precision = 0.639
After 8 training epoch, loss on training batch is 1.902226
2019-06-24 12:19:37.018334: precision = 0.641
After 9 training epoch, loss on training batch is 2.233340
2019-06-24 12:24:29.538077: precision = 0.660
After 10 training epoch, loss on training batch is 2.158338
2019-06-24 12:29:21.814727: precision = 0.663
After 11 training epoch, loss on training batch is 2.081534
2019-06-24 12:34:14.035366: precision = 0.675
After 12 training epoch, loss on training batch is 1.617037
2019-06-24 12:39:06.183052: precision = 0.679
After 13 training epoch, loss on training batch is 1.613671
2019-06-24 12:43:58.200256: precision = 0.694
After 14 training epoch, loss on training batch is 1.343360
2019-06-24 12:48:50.489242: precision = 0.702
After 15 training epoch, loss on training batch is 1.563892
2019-06-24 12:53:42.847426: precision = 0.701
After 16 training epoch, loss on training batch is 1.258287
2019-06-24 12:58:35.008825: precision = 0.708
After 17 training epoch, loss on training batch is 1.051019
2019-06-24 13:03:27.165103: precision = 0.711
After 18 training epoch, loss on training batch is 1.474248
2019-06-24 13:08:19.535905: precision = 0.716
After 19 training epoch, loss on training batch is 1.414922
2019-06-24 13:13:11.726900: precision = 0.715
After 20 training epoch, loss on training batch is 1.310325
2019-06-24 13:18:03.954236: precision = 0.721
After 21 training epoch, loss on training batch is 0.849922
2019-06-24 13:22:56.234540: precision = 0.724
After 22 training epoch, loss on training batch is 0.986286
2019-06-24 13:27:48.473741: precision = 0.726
After 23 training epoch, loss on training batch is 0.828155
2019-06-24 13:32:40.660003: precision = 0.735
After 24 training epoch, loss on training batch is 0.805270
2019-06-24 13:37:33.005260: precision = 0.735
After 25 training epoch, loss on training batch is 0.653061
2019-06-24 13:42:25.227619: precision = 0.728
After 26 training epoch, loss on training batch is 0.809317
2019-06-24 13:47:17.275593: precision = 0.714
After 27 training epoch, loss on training batch is 0.701596
2019-06-24 13:52:09.434387: precision = 0.703
After 28 training epoch, loss on training batch is 0.711571
2019-06-24 13:57:01.479612: precision = 0.699
After 29 training epoch, loss on training batch is 0.681432
2019-06-24 14:01:53.960775: precision = 0.711
After 30 training epoch, loss on training batch is 0.583621
2019-06-24 14:06:46.101641: precision = 0.708
After 31 training epoch, loss on training batch is 0.534000
2019-06-24 14:11:38.191884: precision = 0.708
After 32 training epoch, loss on training batch is 0.523074
2019-06-24 14:16:30.243555: precision = 0.717
After 33 training epoch, loss on training batch is 0.476921
2019-06-24 14:21:22.222556: precision = 0.726
After 34 training epoch, loss on training batch is 0.378551
2019-06-24 14:26:14.312369: precision = 0.727
After 35 training epoch, loss on training batch is 0.376067
2019-06-24 14:31:06.309438: precision = 0.724
After 36 training epoch, loss on training batch is 0.452182
2019-06-24 14:35:58.310774: precision = 0.724
After 37 training epoch, loss on training batch is 0.319250
2019-06-24 14:40:50.369581: precision = 0.717
After 38 training epoch, loss on training batch is 0.409346
2019-06-24 14:45:42.466172: precision = 0.703
After 39 training epoch, loss on training batch is 0.293731
2019-06-24 14:50:34.832867: precision = 0.712
After 40 training epoch, loss on training batch is 0.363127
2019-06-24 14:55:26.746650: precision = 0.713
After 41 training epoch, loss on training batch is 0.423611
2019-06-24 15:00:18.824163: precision = 0.699
After 42 training epoch, loss on training batch is 0.223416
2019-06-24 15:05:10.994169: precision = 0.705
After 43 training epoch, loss on training batch is 0.178779
2019-06-24 15:10:03.072360: precision = 0.714
After 44 training epoch, loss on training batch is 0.376095
2019-06-24 15:14:54.902418: precision = 0.719
After 45 training epoch, loss on training batch is 0.425245
2019-06-24 15:19:46.839179: precision = 0.708
After 46 training epoch, loss on training batch is 0.145013
2019-06-24 15:24:38.595157: precision = 0.697
After 47 training epoch, loss on training batch is 0.162243
2019-06-24 15:29:30.151313: precision = 0.710
After 48 training epoch, loss on training batch is 0.198376
2019-06-24 15:34:22.225957: precision = 0.717
After 49 training epoch, loss on training batch is 0.266227
2019-06-24 15:39:14.081414: precision = 0.696
After 50 training epoch, loss on training batch is 0.153561
2019-06-24 15:44:06.010378: precision = 0.695
After 1 training epoch, loss on training batch is 3.198356
2019-06-24 15:49:06.723743: precision = 0.426
After 2 training epoch, loss on training batch is 2.524957
2019-06-24 15:54:06.257781: precision = 0.476
After 3 training epoch, loss on training batch is 2.557518
2019-06-24 15:59:05.671318: precision = 0.545
After 4 training epoch, loss on training batch is 2.248577
2019-06-24 16:04:05.222085: precision = 0.594
After 5 training epoch, loss on training batch is 1.925526
2019-06-24 16:09:04.681552: precision = 0.621
After 6 training epoch, loss on training batch is 2.278780
2019-06-24 16:14:04.141757: precision = 0.635
After 7 training epoch, loss on training batch is 2.097489
2019-06-24 16:19:03.799754: precision = 0.644
After 8 training epoch, loss on training batch is 1.824368
2019-06-24 16:24:03.556249: precision = 0.646
After 9 training epoch, loss on training batch is 2.132653
2019-06-24 16:29:03.225429: precision = 0.657
After 10 training epoch, loss on training batch is 1.989796
2019-06-24 16:34:02.947764: precision = 0.667
After 11 training epoch, loss on training batch is 1.887992
2019-06-24 16:39:02.725169: precision = 0.677
After 12 training epoch, loss on training batch is 1.544560
2019-06-24 16:44:02.363582: precision = 0.688
After 13 training epoch, loss on training batch is 1.569277
2019-06-24 16:49:02.085287: precision = 0.700
After 14 training epoch, loss on training batch is 1.346247
2019-06-24 16:54:01.918836: precision = 0.698
After 15 training epoch, loss on training batch is 1.447742
2019-06-24 16:59:01.615047: precision = 0.705
After 16 training epoch, loss on training batch is 1.222446
2019-06-24 17:04:01.181963: precision = 0.703
After 17 training epoch, loss on training batch is 1.263113
2019-06-24 17:09:00.910572: precision = 0.698
After 18 training epoch, loss on training batch is 1.256865
2019-06-24 17:14:00.742694: precision = 0.703
After 19 training epoch, loss on training batch is 1.263988
2019-06-24 17:19:00.427143: precision = 0.706
After 20 training epoch, loss on training batch is 1.283726
2019-06-24 17:24:00.081889: precision = 0.709
After 21 training epoch, loss on training batch is 0.827544
2019-06-24 17:28:59.887433: precision = 0.706
After 22 training epoch, loss on training batch is 0.963625
2019-06-24 17:33:59.512532: precision = 0.714
After 23 training epoch, loss on training batch is 0.910640
2019-06-24 17:38:59.085310: precision = 0.712
After 24 training epoch, loss on training batch is 0.728735
2019-06-24 17:43:58.809026: precision = 0.715
After 25 training epoch, loss on training batch is 0.696032
2019-06-24 17:48:58.481572: precision = 0.721
After 26 training epoch, loss on training batch is 0.916178
2019-06-24 17:53:58.190177: precision = 0.708
After 27 training epoch, loss on training batch is 0.728851
2019-06-24 17:58:57.806829: precision = 0.698
After 28 training epoch, loss on training batch is 0.807300
2019-06-24 18:03:57.441299: precision = 0.674
After 29 training epoch, loss on training batch is 0.739869
2019-06-24 18:08:57.153461: precision = 0.687
After 30 training epoch, loss on training batch is 0.659339
2019-06-24 18:13:56.782737: precision = 0.700
After 31 training epoch, loss on training batch is 0.772881
2019-06-24 18:18:56.432802: precision = 0.706
After 32 training epoch, loss on training batch is 0.710033
2019-06-24 18:23:56.069259: precision = 0.710
After 33 training epoch, loss on training batch is 0.581922
2019-06-24 18:28:55.746753: precision = 0.704
After 34 training epoch, loss on training batch is 0.420054
2019-06-24 18:33:55.340030: precision = 0.714
After 35 training epoch, loss on training batch is 0.496851
2019-06-24 18:38:54.986444: precision = 0.713
After 36 training epoch, loss on training batch is 0.478279
2019-06-24 18:43:54.597852: precision = 0.723
After 37 training epoch, loss on training batch is 0.315675
2019-06-24 18:48:54.183888: precision = 0.714
After 38 training epoch, loss on training batch is 0.286400
2019-06-24 18:53:53.866274: precision = 0.705
After 39 training epoch, loss on training batch is 0.249946
2019-06-24 18:58:53.505076: precision = 0.715
After 40 training epoch, loss on training batch is 0.334047
2019-06-24 19:03:53.122043: precision = 0.707
After 41 training epoch, loss on training batch is 0.335865
2019-06-24 19:08:52.840222: precision = 0.703
After 42 training epoch, loss on training batch is 0.308891
2019-06-24 19:13:52.380869: precision = 0.700
After 43 training epoch, loss on training batch is 0.240957
2019-06-24 19:18:51.965846: precision = 0.705
After 44 training epoch, loss on training batch is 0.332295
2019-06-24 19:23:51.546397: precision = 0.711
After 45 training epoch, loss on training batch is 0.355573
2019-06-24 19:28:51.108627: precision = 0.707
After 46 training epoch, loss on training batch is 0.287835
2019-06-24 19:33:50.762169: precision = 0.717
After 47 training epoch, loss on training batch is 0.216618
2019-06-24 19:38:50.225396: precision = 0.704
After 48 training epoch, loss on training batch is 0.265726
2019-06-24 19:43:49.886012: precision = 0.686
After 49 training epoch, loss on training batch is 0.117256
2019-06-24 19:48:49.422954: precision = 0.694
After 50 training epoch, loss on training batch is 0.111576
2019-06-24 19:53:49.003918: precision = 0.700
After 1 training epoch, loss on training batch is 3.317807
2019-06-24 19:58:50.898819: precision = 0.412
After 2 training epoch, loss on training batch is 2.534011
2019-06-24 20:03:51.389980: precision = 0.506
After 3 training epoch, loss on training batch is 2.580728
2019-06-24 20:08:51.946304: precision = 0.561
After 4 training epoch, loss on training batch is 2.237445
2019-06-24 20:13:52.411938: precision = 0.598
After 5 training epoch, loss on training batch is 2.021731
2019-06-24 20:18:52.903302: precision = 0.608
After 6 training epoch, loss on training batch is 2.288394
2019-06-24 20:23:53.375010: precision = 0.611
After 7 training epoch, loss on training batch is 2.182715
2019-06-24 20:28:53.915681: precision = 0.639
After 8 training epoch, loss on training batch is 1.862600
2019-06-24 20:33:54.516626: precision = 0.652
After 9 training epoch, loss on training batch is 2.139526
2019-06-24 20:38:55.003174: precision = 0.668
After 10 training epoch, loss on training batch is 2.059302
2019-06-24 20:43:55.444478: precision = 0.673
After 11 training epoch, loss on training batch is 1.924642
2019-06-24 20:48:55.845913: precision = 0.676
After 12 training epoch, loss on training batch is 1.640849
2019-06-24 20:53:56.370338: precision = 0.686
After 13 training epoch, loss on training batch is 1.447090
2019-06-24 20:58:56.865253: precision = 0.696
After 14 training epoch, loss on training batch is 1.316420
2019-06-24 21:03:57.463624: precision = 0.703
After 15 training epoch, loss on training batch is 1.596419
2019-06-24 21:08:57.916911: precision = 0.704
After 16 training epoch, loss on training batch is 1.359572
2019-06-24 21:13:58.355186: precision = 0.697
After 17 training epoch, loss on training batch is 1.201811
2019-06-24 21:18:58.869684: precision = 0.696
After 18 training epoch, loss on training batch is 1.473948
2019-06-24 21:23:59.187736: precision = 0.705
After 19 training epoch, loss on training batch is 1.363666
2019-06-24 21:28:59.618876: precision = 0.707
After 20 training epoch, loss on training batch is 1.358594
2019-06-24 21:34:00.095715: precision = 0.712
After 21 training epoch, loss on training batch is 0.932385
2019-06-24 21:39:00.590066: precision = 0.722
After 22 training epoch, loss on training batch is 1.042622
2019-06-24 21:44:01.133782: precision = 0.730
After 23 training epoch, loss on training batch is 0.836597
2019-06-24 21:49:01.556116: precision = 0.735
After 24 training epoch, loss on training batch is 0.808766
2019-06-24 21:54:01.922280: precision = 0.731
After 25 training epoch, loss on training batch is 0.638262
2019-06-24 21:59:02.354466: precision = 0.731
After 26 training epoch, loss on training batch is 0.843289
2019-06-24 22:04:02.713992: precision = 0.728
After 27 training epoch, loss on training batch is 0.956876
2019-06-24 22:09:03.024651: precision = 0.718
After 28 training epoch, loss on training batch is 0.796037
2019-06-24 22:14:03.420585: precision = 0.714
After 29 training epoch, loss on training batch is 0.654819
2019-06-24 22:19:03.743398: precision = 0.715
After 30 training epoch, loss on training batch is 0.621746
2019-06-24 22:24:04.058761: precision = 0.716
After 31 training epoch, loss on training batch is 0.571082
2019-06-24 22:29:04.376747: precision = 0.717
After 32 training epoch, loss on training batch is 0.653187
2019-06-24 22:34:04.747418: precision = 0.719
After 33 training epoch, loss on training batch is 0.633765
2019-06-24 22:39:05.021954: precision = 0.718
After 34 training epoch, loss on training batch is 0.624193
2019-06-24 22:44:05.258489: precision = 0.720
After 35 training epoch, loss on training batch is 0.541721
2019-06-24 22:49:05.529417: precision = 0.723
After 36 training epoch, loss on training batch is 0.404093
2019-06-24 22:54:05.892483: precision = 0.719
After 37 training epoch, loss on training batch is 0.307100
2019-06-24 22:59:06.145813: precision = 0.714
After 38 training epoch, loss on training batch is 0.469305
2019-06-24 23:04:06.296243: precision = 0.723
After 39 training epoch, loss on training batch is 0.436399
2019-06-24 23:09:06.522494: precision = 0.702
After 40 training epoch, loss on training batch is 0.424545
2019-06-24 23:14:06.744001: precision = 0.717
After 41 training epoch, loss on training batch is 0.396815
2019-06-24 23:19:07.014886: precision = 0.711
After 42 training epoch, loss on training batch is 0.312624
2019-06-24 23:24:07.242453: precision = 0.728
After 43 training epoch, loss on training batch is 0.271958
2019-06-24 23:29:07.523963: precision = 0.724
After 44 training epoch, loss on training batch is 0.373153
2019-06-24 23:34:07.800274: precision = 0.715
After 45 training epoch, loss on training batch is 0.375623
2019-06-24 23:39:08.185286: precision = 0.692
After 46 training epoch, loss on training batch is 0.245219
2019-06-24 23:44:08.432060: precision = 0.714
After 47 training epoch, loss on training batch is 0.286014
2019-06-24 23:49:08.633147: precision = 0.728
After 48 training epoch, loss on training batch is 0.178721
2019-06-24 23:54:08.825691: precision = 0.723
After 49 training epoch, loss on training batch is 0.275926
2019-06-24 23:59:09.023324: precision = 0.725
After 50 training epoch, loss on training batch is 0.178890
2019-06-25 00:04:09.280084: precision = 0.714
After 1 training epoch, loss on training batch is 3.191769
2019-06-25 00:08:57.328359: precision = 0.415
After 2 training epoch, loss on training batch is 2.335656
2019-06-25 00:13:44.121007: precision = 0.487
After 3 training epoch, loss on training batch is 2.497706
2019-06-25 00:18:30.889479: precision = 0.533
After 4 training epoch, loss on training batch is 2.296060
2019-06-25 00:23:17.720400: precision = 0.596
After 5 training epoch, loss on training batch is 2.065807
2019-06-25 00:28:04.363360: precision = 0.611
After 6 training epoch, loss on training batch is 2.275172
2019-06-25 00:32:51.242426: precision = 0.611
After 7 training epoch, loss on training batch is 1.955256
2019-06-25 00:37:38.084725: precision = 0.642
After 8 training epoch, loss on training batch is 1.875751
2019-06-25 00:42:24.812823: precision = 0.653
After 9 training epoch, loss on training batch is 2.111482
2019-06-25 00:47:11.632447: precision = 0.665
After 10 training epoch, loss on training batch is 1.950318
2019-06-25 00:51:58.551082: precision = 0.670
After 11 training epoch, loss on training batch is 1.908076
2019-06-25 00:56:45.014645: precision = 0.676
After 12 training epoch, loss on training batch is 1.533474
2019-06-25 01:01:31.861909: precision = 0.688
After 13 training epoch, loss on training batch is 1.502156
2019-06-25 01:06:18.724887: precision = 0.700
After 14 training epoch, loss on training batch is 1.302274
2019-06-25 01:11:05.414818: precision = 0.709
After 15 training epoch, loss on training batch is 1.557137
2019-06-25 01:15:52.186397: precision = 0.705
After 16 training epoch, loss on training batch is 1.305245
2019-06-25 01:20:39.097807: precision = 0.705
After 17 training epoch, loss on training batch is 1.186154
2019-06-25 01:25:26.006669: precision = 0.707
After 18 training epoch, loss on training batch is 1.378599
2019-06-25 01:30:12.866487: precision = 0.704
After 19 training epoch, loss on training batch is 1.303834
2019-06-25 01:34:59.360223: precision = 0.717
After 20 training epoch, loss on training batch is 1.338099
2019-06-25 01:39:46.081177: precision = 0.723
After 21 training epoch, loss on training batch is 1.027583
2019-06-25 01:44:32.686737: precision = 0.726
After 22 training epoch, loss on training batch is 1.070985
2019-06-25 01:49:19.110115: precision = 0.728
After 23 training epoch, loss on training batch is 0.859309
2019-06-25 01:54:06.032886: precision = 0.731
After 24 training epoch, loss on training batch is 0.747082
2019-06-25 01:58:52.911003: precision = 0.728
After 25 training epoch, loss on training batch is 0.734656
2019-06-25 02:03:39.634017: precision = 0.716
After 26 training epoch, loss on training batch is 0.904550
2019-06-25 02:08:26.571086: precision = 0.714
After 27 training epoch, loss on training batch is 0.903098
2019-06-25 02:13:13.057082: precision = 0.713
After 28 training epoch, loss on training batch is 0.806748
2019-06-25 02:17:59.856476: precision = 0.718
After 29 training epoch, loss on training batch is 0.833967
2019-06-25 02:22:46.417745: precision = 0.719
After 30 training epoch, loss on training batch is 0.622913
2019-06-25 02:27:33.252213: precision = 0.716
After 31 training epoch, loss on training batch is 0.691422
2019-06-25 02:32:19.913739: precision = 0.724
After 32 training epoch, loss on training batch is 0.669860
2019-06-25 02:37:06.518937: precision = 0.723
After 33 training epoch, loss on training batch is 0.609543
2019-06-25 02:41:52.876333: precision = 0.712
After 34 training epoch, loss on training batch is 0.524597
2019-06-25 02:46:39.365997: precision = 0.714
After 35 training epoch, loss on training batch is 0.819084
2019-06-25 02:51:25.843565: precision = 0.721
After 36 training epoch, loss on training batch is 0.380228
2019-06-25 02:56:12.594587: precision = 0.721
After 37 training epoch, loss on training batch is 0.344889
2019-06-25 03:00:58.976540: precision = 0.729
After 38 training epoch, loss on training batch is 0.484385
2019-06-25 03:05:45.461557: precision = 0.720
After 39 training epoch, loss on training batch is 0.258395
2019-06-25 03:10:32.153855: precision = 0.722
After 40 training epoch, loss on training batch is 0.274181
2019-06-25 03:15:18.543819: precision = 0.720
After 41 training epoch, loss on training batch is 0.319804
2019-06-25 03:20:05.024077: precision = 0.721
After 42 training epoch, loss on training batch is 0.274624
2019-06-25 03:24:51.575601: precision = 0.726
After 43 training epoch, loss on training batch is 0.402345
2019-06-25 03:29:38.129208: precision = 0.731
After 44 training epoch, loss on training batch is 0.403586
2019-06-25 03:34:24.479531: precision = 0.719
After 45 training epoch, loss on training batch is 0.292352
2019-06-25 03:39:10.992940: precision = 0.703
After 46 training epoch, loss on training batch is 0.324446
2019-06-25 03:43:57.445628: precision = 0.733
After 47 training epoch, loss on training batch is 0.282523
2019-06-25 03:48:44.116668: precision = 0.733
After 48 training epoch, loss on training batch is 0.234831
2019-06-25 03:53:30.742009: precision = 0.728
After 49 training epoch, loss on training batch is 0.110576
2019-06-25 03:58:17.410064: precision = 0.735
After 50 training epoch, loss on training batch is 0.238756
2019-06-25 04:03:03.843437: precision = 0.728
[[0.10476762820512821, 0.17668269230769232, 0.2527043269230769, 0.2192508012820513, 0.27784455128205127, 0.31490384615384615, 0.32792467948717946, 0.32632211538461536, 0.3323317307692308, 0.35006009615384615, 0.34385016025641024, 0.3584735576923077, 0.37489983974358976, 0.36268028846153844, 0.39122596153846156, 0.40244391025641024, 0.4131610576923077, 0.4110576923076923, 0.4011418269230769, 0.40735176282051283, 0.4199719551282051, 0.4140625, 0.422275641025641, 0.4130608974358974, 0.4522235576923077, 0.4360977564102564, 0.46654647435897434, 0.45783253205128205, 0.46654647435897434, 0.47095352564102566, 0.46854967948717946, 0.46534455128205127, 0.46113782051282054, 0.47345753205128205, 0.4777644230769231, 0.4723557692307692, 0.4622395833333333, 0.4717548076923077, 0.4785657051282051, 0.4658453525641026, 0.49549278846153844, 0.4866786858974359, 0.49589342948717946, 0.4872796474358974, 0.472255608974359, 0.4866786858974359, 0.5115184294871795, 0.5010016025641025, 0.49819711538461536, 0.4946915064102564], [0.19501201923076922, 0.2665264423076923, 0.30358573717948717, 0.3573717948717949, 0.3847155448717949, 0.39623397435897434, 0.42007211538461536, 0.3996394230769231, 0.4127604166666667, 0.46304086538461536, 0.4985977564102564, 0.49829727564102566, 0.5015024038461539, 0.5239383012820513, 0.5261418269230769, 0.5162259615384616, 0.5266426282051282, 0.5403645833333334, 0.5487780448717948, 0.5583934294871795, 0.5481770833333334, 0.5502804487179487, 0.5607972756410257, 0.5536858974358975, 0.5381610576923077, 0.5494791666666666, 0.5708133012820513, 0.5792267628205128, 0.5816306089743589, 0.5692107371794872, 0.5667067307692307, 0.5741185897435898, 0.58984375, 0.5768229166666666, 0.575020032051282, 0.5771233974358975, 0.5866386217948718, 0.5950520833333334, 0.5903445512820513, 0.5875400641025641, 0.5923477564102564, 0.6032652243589743, 0.6000600961538461, 0.6044671474358975, 0.6110777243589743, 0.6120793269230769, 0.6024639423076923, 0.6083733974358975, 0.6098758012820513, 0.5963541666666666], [0.28575721153846156, 0.3515625, 0.39733573717948717, 0.449619391025641, 0.4736578525641026, 0.4846754807692308, 0.49719551282051283, 0.5123197115384616, 0.5322516025641025, 0.5407652243589743, 0.5378605769230769, 0.5567908653846154, 0.5598958333333334, 0.57421875, 0.5863381410256411, 0.5942508012820513, 0.6025641025641025, 0.6073717948717948, 0.6041666666666666, 0.5942508012820513, 0.5961538461538461, 0.5958533653846154, 0.5999599358974359, 0.5992588141025641, 0.6084735576923077, 0.6072716346153846, 0.6200921474358975, 0.6244991987179487, 0.6323116987179487, 0.6373197115384616, 0.6410256410256411, 0.6362179487179487, 0.6312099358974359, 0.6323116987179487, 0.6285056089743589, 0.6227964743589743, 0.6260016025641025, 0.6215945512820513, 0.6280048076923077, 0.6377203525641025, 0.6412259615384616, 0.6392227564102564, 0.6286057692307693, 0.6292067307692307, 0.6280048076923077, 0.6310096153846154, 0.6180889423076923, 0.6089743589743589, 0.624198717948718, 0.6307091346153846], [0.33373397435897434, 0.401943108974359, 0.4896834935897436, 0.5142227564102564, 0.5349559294871795, 0.5612980769230769, 0.5786258012820513, 0.59765625, 0.6126802884615384, 0.6144831730769231, 0.6134815705128205, 0.6099759615384616, 0.6293068910256411, 0.6331129807692307, 0.6334134615384616, 0.6327123397435898, 0.6382211538461539, 0.6500400641025641, 0.6588541666666666, 0.6654647435897436, 0.6690705128205128, 0.6697716346153846, 0.6778846153846154, 0.6747796474358975, 0.6803886217948718, 0.6841947115384616, 0.6819911858974359, 0.6816907051282052, 0.6774839743589743, 0.6760817307692307, 0.6773838141025641, 0.6790865384615384, 0.68359375, 0.6828926282051282, 0.6890024038461539, 0.6851963141025641, 0.6737780448717948, 0.6683693910256411, 0.6769831730769231, 0.6660657051282052, 0.6495392628205128, 0.6627604166666666, 0.6629607371794872, 0.6681690705128205, 0.6675681089743589, 0.6517427884615384, 0.6550480769230769, 0.6670673076923077, 0.6733774038461539, 0.6727764423076923], [0.37439903846153844, 0.4636418269230769, 0.5061097756410257, 0.549979967948718, 0.5811298076923077, 0.5955528846153846, 0.5998597756410257, 0.5995592948717948, 0.6255008012820513, 0.6312099358974359, 0.6474358974358975, 0.6528445512820513, 0.6456330128205128, 0.6585536858974359, 0.6734775641025641, 0.6764823717948718, 0.6737780448717948, 0.6744791666666666, 0.6730769230769231, 0.6889022435897436, 0.6923076923076923, 0.6946113782051282, 0.6994190705128205, 0.7043269230769231, 0.7059294871794872, 0.7024238782051282, 0.6950120192307693, 0.6828926282051282, 0.6903044871794872, 0.7091346153846154, 0.6855969551282052, 0.6907051282051282, 0.6972155448717948, 0.7016225961538461, 0.6907051282051282, 0.6792868589743589, 0.6907051282051282, 0.6855969551282052, 0.6969150641025641, 0.700020032051282, 0.6946113782051282, 0.6976161858974359, 0.6862980769230769, 0.6928084935897436, 0.6980168269230769, 0.7066306089743589, 0.7081330128205128, 0.696113782051282, 0.6870993589743589, 0.6988181089743589], [0.3964342948717949, 0.4898838141025641, 0.5396634615384616, 0.5807291666666666, 0.6088741987179487, 0.6260016025641025, 0.6378205128205128, 0.6591546474358975, 0.6705729166666666, 0.6810897435897436, 0.6894030448717948, 0.6963141025641025, 0.7050280448717948, 0.7085336538461539, 0.6975160256410257, 0.7010216346153846, 0.6977163461538461, 0.6998197115384616, 0.7059294871794872, 0.7170472756410257, 0.7210536858974359, 0.7216546474358975, 0.7268629807692307, 0.7227564102564102, 0.7154447115384616, 0.7080328525641025, 0.7112379807692307, 0.7011217948717948, 0.6995192307692307, 0.700020032051282, 0.7128405448717948, 0.7206530448717948, 0.7292668269230769, 0.7394831730769231, 0.7269631410256411, 0.7165464743589743, 0.7153445512820513, 0.7152443910256411, 0.7158453525641025, 0.7214543269230769, 0.7334735576923077, 0.7279647435897436, 0.7133413461538461, 0.7071314102564102, 0.7267628205128205, 0.7167467948717948, 0.7155448717948718, 0.6972155448717948, 0.7142427884615384, 0.7240584935897436], [0.44360977564102566, 0.5288461538461539, 0.5568910256410257, 0.5912459935897436, 0.6161858974358975, 0.6369190705128205, 0.6502403846153846, 0.6550480769230769, 0.6595552884615384, 0.6657652243589743, 0.6777844551282052, 0.6956129807692307, 0.7003205128205128, 0.7087339743589743, 0.7054286858974359, 0.7075320512820513, 0.7158453525641025, 0.7194511217948718, 0.7251602564102564, 0.7254607371794872, 0.7282652243589743, 0.7253605769230769, 0.7326722756410257, 0.7251602564102564, 0.7221554487179487, 0.7189503205128205, 0.7058293269230769, 0.6954126602564102, 0.7091346153846154, 0.7143429487179487, 0.707832532051282, 0.7077323717948718, 0.7096354166666666, 0.7149439102564102, 0.6949118589743589, 0.7085336538461539, 0.7094350961538461, 0.7272636217948718, 0.7262620192307693, 0.725761217948718, 0.7295673076923077, 0.703125, 0.6989182692307693, 0.7260616987179487, 0.7240584935897436, 0.7245592948717948, 0.7131410256410257, 0.7202524038461539, 0.7118389423076923, 0.7136418269230769], [0.41195913461538464, 0.5104166666666666, 0.5388621794871795, 0.5853365384615384, 0.6075721153846154, 0.6245993589743589, 0.6411258012820513, 0.6560496794871795, 0.6644631410256411, 0.6705729166666666, 0.6816907051282052, 0.6867988782051282, 0.6966145833333334, 0.7018229166666666, 0.7013221153846154, 0.7002203525641025, 0.6955128205128205, 0.6916065705128205, 0.6891025641025641, 0.6990184294871795, 0.7082331730769231, 0.7143429487179487, 0.7154447115384616, 0.7098357371794872, 0.7044270833333334, 0.6963141025641025, 0.690604967948718, 0.6960136217948718, 0.7050280448717948, 0.7139423076923077, 0.7216546474358975, 0.7188501602564102, 0.7086338141025641, 0.7115384615384616, 0.7123397435897436, 0.703125, 0.6981169871794872, 0.7028245192307693, 0.7071314102564102, 0.7085336538461539, 0.7162459935897436, 0.7126402243589743, 0.7137419871794872, 0.7115384615384616, 0.6999198717948718, 0.6972155448717948, 0.6983173076923077, 0.7041266025641025, 0.7048277243589743, 0.700020032051282], [0.4371995192307692, 0.49198717948717946, 0.5408653846153846, 0.5969551282051282, 0.6223958333333334, 0.625, 0.6479366987179487, 0.6500400641025641, 0.6557491987179487, 0.6641626602564102, 0.6801883012820513, 0.6863982371794872, 0.686698717948718, 0.6867988782051282, 0.6955128205128205, 0.7007211538461539, 0.703125, 0.7115384615384616, 0.7150440705128205, 0.7220552884615384, 0.7221554487179487, 0.7233573717948718, 0.7185496794871795, 0.7176482371794872, 0.7212540064102564, 0.7189503205128205, 0.7166466346153846, 0.7121394230769231, 0.711738782051282, 0.7071314102564102, 0.7108373397435898, 0.7063301282051282, 0.7014222756410257, 0.7099358974358975, 0.7157451923076923, 0.7209535256410257, 0.7202524038461539, 0.725761217948718, 0.7161458333333334, 0.7154447115384616, 0.7089342948717948, 0.7151442307692307, 0.7106370192307693, 0.7315705128205128, 0.7167467948717948, 0.7212540064102564, 0.7159455128205128, 0.7152443910256411, 0.7245592948717948, 0.7197516025641025], [0.43279246794871795, 0.5, 0.5489783653846154, 0.5863381410256411, 0.6128806089743589, 0.6211939102564102, 0.6416266025641025, 0.653145032051282, 0.6670673076923077, 0.6683693910256411, 0.6669671474358975, 0.6737780448717948, 0.6838942307692307, 0.6865985576923077, 0.6944110576923077, 0.7024238782051282, 0.7063301282051282, 0.7149439102564102, 0.719551282051282, 0.7293669871794872, 0.7294671474358975, 0.7348758012820513, 0.7295673076923077, 0.7295673076923077, 0.7260616987179487, 0.7276642628205128, 0.7358774038461539, 0.7253605769230769, 0.7130408653846154, 0.6997195512820513, 0.6931089743589743, 0.6923076923076923, 0.7013221153846154, 0.7092347756410257, 0.7112379807692307, 0.7205528846153846, 0.7221554487179487, 0.7231570512820513, 0.7320713141025641, 0.7275641025641025, 0.721854967948718, 0.7244591346153846, 0.7180488782051282, 0.7246594551282052, 0.7209535256410257, 0.7136418269230769, 0.7163461538461539, 0.7098357371794872, 0.7047275641025641, 0.7026241987179487], [0.41716746794871795, 0.5219350961538461, 0.5739182692307693, 0.5930488782051282, 0.6196915064102564, 0.6393229166666666, 0.6508413461538461, 0.65625, 0.6669671474358975, 0.6660657051282052, 0.6820913461538461, 0.6851963141025641, 0.6955128205128205, 0.7040264423076923, 0.7045272435897436, 0.7071314102564102, 0.7077323717948718, 0.706229967948718, 0.7114383012820513, 0.7149439102564102, 0.7157451923076923, 0.7184495192307693, 0.7149439102564102, 0.7169471153846154, 0.7185496794871795, 0.7170472756410257, 0.7174479166666666, 0.6951121794871795, 0.6849959935897436, 0.6792868589743589, 0.7025240384615384, 0.7189503205128205, 0.7223557692307693, 0.714042467948718, 0.7192508012820513, 0.707832532051282, 0.7063301282051282, 0.6957131410256411, 0.7114383012820513, 0.715645032051282, 0.7099358974358975, 0.7028245192307693, 0.7327724358974359, 0.7313701923076923, 0.7277644230769231, 0.7258613782051282, 0.7134415064102564, 0.7152443910256411, 0.7146434294871795, 0.6987179487179487], [0.40084134615384615, 0.4717548076923077, 0.5560897435897436, 0.5732171474358975, 0.614082532051282, 0.6260016025641025, 0.6507411858974359, 0.6608573717948718, 0.6677684294871795, 0.6785857371794872, 0.6762820512820513, 0.6721754807692307, 0.6776842948717948, 0.6915064102564102, 0.7020232371794872, 0.7076322115384616, 0.7087339743589743, 0.7038261217948718, 0.7150440705128205, 0.7181490384615384, 0.7174479166666666, 0.7222556089743589, 0.727363782051282, 0.7315705128205128, 0.7267628205128205, 0.7194511217948718, 0.698417467948718, 0.6842948717948718, 0.6904046474358975, 0.7073317307692307, 0.7182491987179487, 0.7240584935897436, 0.7278645833333334, 0.7220552884615384, 0.7106370192307693, 0.7237580128205128, 0.7175480769230769, 0.7193509615384616, 0.7079326923076923, 0.7095352564102564, 0.7085336538461539, 0.7066306089743589, 0.7236578525641025, 0.7209535256410257, 0.7235576923076923, 0.7130408653846154, 0.7122395833333334, 0.6996193910256411, 0.6936097756410257, 0.7104366987179487], [0.4147636217948718, 0.5122195512820513, 0.55859375, 0.5821314102564102, 0.617988782051282, 0.629707532051282, 0.6471354166666666, 0.6580528846153846, 0.6655649038461539, 0.6716746794871795, 0.6886017628205128, 0.6935096153846154, 0.6959134615384616, 0.7009214743589743, 0.703125, 0.7045272435897436, 0.7072315705128205, 0.7104366987179487, 0.7189503205128205, 0.7266626602564102, 0.7184495192307693, 0.7251602564102564, 0.7244591346153846, 0.721854967948718, 0.7217548076923077, 0.7283653846153846, 0.7294671474358975, 0.7255608974358975, 0.7105368589743589, 0.6981169871794872, 0.7129407051282052, 0.7171474358974359, 0.7168469551282052, 0.7251602564102564, 0.7292668269230769, 0.7251602564102564, 0.7318709935897436, 0.7196514423076923, 0.7016225961538461, 0.7134415064102564, 0.7264623397435898, 0.7274639423076923, 0.7084334935897436, 0.7079326923076923, 0.7122395833333334, 0.7123397435897436, 0.7317708333333334, 0.7291666666666666, 0.7205528846153846, 0.7188501602564102], [0.4277844551282051, 0.5028044871794872, 0.5529847756410257, 0.5877403846153846, 0.6231971153846154, 0.6333133012820513, 0.6490384615384616, 0.6623597756410257, 0.6754807692307693, 0.6854967948717948, 0.6970152243589743, 0.6994190705128205, 0.6987179487179487, 0.7086338141025641, 0.707832532051282, 0.7051282051282052, 0.7082331730769231, 0.7108373397435898, 0.7224559294871795, 0.7243589743589743, 0.7194511217948718, 0.723457532051282, 0.7275641025641025, 0.7322716346153846, 0.7381810897435898, 0.7271634615384616, 0.7053285256410257, 0.6995192307692307, 0.7048277243589743, 0.7182491987179487, 0.7256610576923077, 0.7235576923076923, 0.7261618589743589, 0.7322716346153846, 0.7320713141025641, 0.72265625, 0.7192508012820513, 0.7173477564102564, 0.7225560897435898, 0.7242588141025641, 0.7163461538461539, 0.7183493589743589, 0.7245592948717948, 0.73046875, 0.7255608974358975, 0.7127403846153846, 0.7201522435897436, 0.7263621794871795, 0.7173477564102564, 0.7159455128205128], [0.42948717948717946, 0.48677884615384615, 0.5334535256410257, 0.5766225961538461, 0.6095753205128205, 0.6312099358974359, 0.64453125, 0.6548477564102564, 0.6734775641025641, 0.6845953525641025, 0.6785857371794872, 0.6789863782051282, 0.6896033653846154, 0.7004206730769231, 0.6977163461538461, 0.7050280448717948, 0.7058293269230769, 0.7088341346153846, 0.7130408653846154, 0.7158453525641025, 0.7203525641025641, 0.7213541666666666, 0.7196514423076923, 0.7227564102564102, 0.7326722756410257, 0.7350761217948718, 0.7299679487179487, 0.714042467948718, 0.7043269230769231, 0.6994190705128205, 0.7080328525641025, 0.7208533653846154, 0.7266626602564102, 0.7269631410256411, 0.7132411858974359, 0.703926282051282, 0.7035256410256411, 0.7099358974358975, 0.7196514423076923, 0.7095352564102564, 0.700020032051282, 0.6781850961538461, 0.6834935897435898, 0.7149439102564102, 0.7200520833333334, 0.7168469551282052, 0.7167467948717948, 0.6944110576923077, 0.6921073717948718, 0.7042267628205128], [0.4153645833333333, 0.5041065705128205, 0.5286458333333334, 0.5669070512820513, 0.6102764423076923, 0.6115785256410257, 0.6393229166666666, 0.6411258012820513, 0.6597556089743589, 0.6634615384615384, 0.6746794871794872, 0.6790865384615384, 0.6939102564102564, 0.702323717948718, 0.7014222756410257, 0.7080328525641025, 0.7112379807692307, 0.7157451923076923, 0.7154447115384616, 0.7211538461538461, 0.7241586538461539, 0.7263621794871795, 0.7345753205128205, 0.7346754807692307, 0.7279647435897436, 0.7136418269230769, 0.7025240384615384, 0.6985176282051282, 0.7106370192307693, 0.7077323717948718, 0.7081330128205128, 0.7171474358974359, 0.7260616987179487, 0.7265625, 0.7239583333333334, 0.7239583333333334, 0.7174479166666666, 0.7025240384615384, 0.7121394230769231, 0.7131410256410257, 0.6988181089743589, 0.7048277243589743, 0.7136418269230769, 0.7194511217948718, 0.7080328525641025, 0.6974158653846154, 0.7097355769230769, 0.7170472756410257, 0.696113782051282, 0.6951121794871795], [0.42628205128205127, 0.4755608974358974, 0.5451722756410257, 0.5935496794871795, 0.6213942307692307, 0.6352163461538461, 0.6443309294871795, 0.6461338141025641, 0.6566506410256411, 0.6669671474358975, 0.6770833333333334, 0.688301282051282, 0.700020032051282, 0.6979166666666666, 0.7045272435897436, 0.7033253205128205, 0.698417467948718, 0.7032251602564102, 0.706229967948718, 0.7089342948717948, 0.7056290064102564, 0.7143429487179487, 0.7124399038461539, 0.7151442307692307, 0.7209535256410257, 0.707832532051282, 0.6976161858974359, 0.6744791666666666, 0.6868990384615384, 0.6996193910256411, 0.706229967948718, 0.7102363782051282, 0.7035256410256411, 0.7144431089743589, 0.7134415064102564, 0.7229567307692307, 0.714042467948718, 0.7054286858974359, 0.7151442307692307, 0.7074318910256411, 0.703125, 0.700020032051282, 0.7046274038461539, 0.7113381410256411, 0.7073317307692307, 0.7168469551282052, 0.7042267628205128, 0.6856971153846154, 0.6943108974358975, 0.700020032051282], [0.41225961538461536, 0.5060096153846154, 0.5610977564102564, 0.5981570512820513, 0.6077724358974359, 0.6108774038461539, 0.6393229166666666, 0.6521434294871795, 0.6676682692307693, 0.6725761217948718, 0.6760817307692307, 0.6860977564102564, 0.6958133012820513, 0.7026241987179487, 0.703926282051282, 0.6970152243589743, 0.6962139423076923, 0.7051282051282052, 0.7071314102564102, 0.7122395833333334, 0.7215544871794872, 0.729667467948718, 0.7348758012820513, 0.731270032051282, 0.731270032051282, 0.7283653846153846, 0.7175480769230769, 0.7138421474358975, 0.7152443910256411, 0.7164463141025641, 0.7174479166666666, 0.7186498397435898, 0.7183493589743589, 0.7201522435897436, 0.723457532051282, 0.7192508012820513, 0.7143429487179487, 0.7227564102564102, 0.7022235576923077, 0.7174479166666666, 0.7111378205128205, 0.7281650641025641, 0.7243589743589743, 0.7147435897435898, 0.6923076923076923, 0.7144431089743589, 0.7276642628205128, 0.7233573717948718, 0.7251602564102564, 0.7136418269230769], [0.4149639423076923, 0.48717948717948717, 0.5332532051282052, 0.5956530448717948, 0.6108774038461539, 0.6107772435897436, 0.6419270833333334, 0.653145032051282, 0.6650641025641025, 0.6697716346153846, 0.6764823717948718, 0.6882011217948718, 0.6997195512820513, 0.7088341346153846, 0.7046274038461539, 0.7053285256410257, 0.7071314102564102, 0.703926282051282, 0.7165464743589743, 0.7225560897435898, 0.7256610576923077, 0.7283653846153846, 0.7306690705128205, 0.7278645833333334, 0.7158453525641025, 0.7135416666666666, 0.7133413461538461, 0.7182491987179487, 0.7185496794871795, 0.7162459935897436, 0.7240584935897436, 0.7225560897435898, 0.7120392628205128, 0.7135416666666666, 0.7213541666666666, 0.7205528846153846, 0.7292668269230769, 0.7196514423076923, 0.7222556089743589, 0.7196514423076923, 0.7207532051282052, 0.7263621794871795, 0.7311698717948718, 0.7188501602564102, 0.7032251602564102, 0.7331730769230769, 0.7331730769230769, 0.7283653846153846, 0.7353766025641025, 0.7281650641025641]]
Evaluater: loading data
After 1 training epoch, loss on training batch is 1448.168701
2019-06-25 04:03:15.814494: precision = 0.100
After 2 training epoch, loss on training batch is 702.083557
2019-06-25 04:03:20.761140: precision = 0.100
After 3 training epoch, loss on training batch is 271.684509
2019-06-25 04:03:25.708905: precision = 0.100
After 4 training epoch, loss on training batch is 88.531509
2019-06-25 04:03:30.657662: precision = 0.100
After 5 training epoch, loss on training batch is 130.577911
2019-06-25 04:03:35.614036: precision = 0.100
After 6 training epoch, loss on training batch is 62.111374
2019-06-25 04:03:40.568997: precision = 0.100
After 7 training epoch, loss on training batch is 85.565269
2019-06-25 04:03:45.520877: precision = 0.100
After 8 training epoch, loss on training batch is 36.148903
2019-06-25 04:03:50.478084: precision = 0.100
After 9 training epoch, loss on training batch is 18.651512
2019-06-25 04:03:55.435970: precision = 0.100
After 10 training epoch, loss on training batch is 60.960480
2019-06-25 04:04:00.397673: precision = 0.100
After 11 training epoch, loss on training batch is 24.736609
2019-06-25 04:04:05.354632: precision = 0.100
After 12 training epoch, loss on training batch is 20.654533
2019-06-25 04:04:10.312892: precision = 0.100
After 13 training epoch, loss on training batch is 33.543488
2019-06-25 04:04:15.267297: precision = 0.100
After 14 training epoch, loss on training batch is 23.555048
2019-06-25 04:04:20.223458: precision = 0.100
After 15 training epoch, loss on training batch is 22.587986
2019-06-25 04:04:25.180907: precision = 0.100
After 16 training epoch, loss on training batch is 12.229576
2019-06-25 04:04:30.136514: precision = 0.100
After 17 training epoch, loss on training batch is 18.077803
2019-06-25 04:04:35.087926: precision = 0.100
After 18 training epoch, loss on training batch is 8.095899
2019-06-25 04:04:40.042933: precision = 0.100
After 19 training epoch, loss on training batch is 5.327591
2019-06-25 04:04:44.999718: precision = 0.100
After 20 training epoch, loss on training batch is 5.201011
2019-06-25 04:04:49.956985: precision = 0.100
After 21 training epoch, loss on training batch is 4.911949
2019-06-25 04:04:54.915453: precision = 0.100
After 22 training epoch, loss on training batch is 4.933773
2019-06-25 04:04:59.870322: precision = 0.100
After 23 training epoch, loss on training batch is 4.876714
2019-06-25 04:05:04.833426: precision = 0.100
After 24 training epoch, loss on training batch is 5.019387
2019-06-25 04:05:09.789157: precision = 0.100
After 25 training epoch, loss on training batch is 4.702770
2019-06-25 04:05:14.746075: precision = 0.100
After 26 training epoch, loss on training batch is 4.635169
2019-06-25 04:05:19.706556: precision = 0.100
After 27 training epoch, loss on training batch is 4.821435
2019-06-25 04:05:24.659710: precision = 0.100
After 28 training epoch, loss on training batch is 5.118058
2019-06-25 04:05:29.618291: precision = 0.100
After 29 training epoch, loss on training batch is 5.129920
2019-06-25 04:05:34.576279: precision = 0.100
After 30 training epoch, loss on training batch is 4.947775
2019-06-25 04:05:39.532399: precision = 0.100
After 31 training epoch, loss on training batch is 4.873760
2019-06-25 04:05:44.490822: precision = 0.100
After 32 training epoch, loss on training batch is 4.939293
2019-06-25 04:05:49.446166: precision = 0.100
After 33 training epoch, loss on training batch is 4.728387
2019-06-25 04:05:54.397805: precision = 0.100
After 34 training epoch, loss on training batch is 4.910327
2019-06-25 04:05:59.354828: precision = 0.100
After 35 training epoch, loss on training batch is 5.146826
2019-06-25 04:06:04.316320: precision = 0.100
After 36 training epoch, loss on training batch is 5.295554
2019-06-25 04:06:09.272470: precision = 0.100
After 37 training epoch, loss on training batch is 4.929353
2019-06-25 04:06:14.231248: precision = 0.100
After 38 training epoch, loss on training batch is 4.643842
2019-06-25 04:06:19.187504: precision = 0.100
After 39 training epoch, loss on training batch is 5.019247
2019-06-25 04:06:24.145515: precision = 0.100
After 40 training epoch, loss on training batch is 4.889329
2019-06-25 04:06:29.100233: precision = 0.100
After 41 training epoch, loss on training batch is 5.165720
2019-06-25 04:06:34.058778: precision = 0.100
After 42 training epoch, loss on training batch is 4.823903
2019-06-25 04:06:39.017607: precision = 0.100
After 43 training epoch, loss on training batch is 4.788444
2019-06-25 04:06:43.976483: precision = 0.100
After 44 training epoch, loss on training batch is 4.986374
2019-06-25 04:06:48.931037: precision = 0.100
After 45 training epoch, loss on training batch is 4.800577
2019-06-25 04:06:53.889007: precision = 0.100
After 46 training epoch, loss on training batch is 5.261777
2019-06-25 04:06:58.844562: precision = 0.100
After 47 training epoch, loss on training batch is 4.914411
2019-06-25 04:07:03.806646: precision = 0.100
After 48 training epoch, loss on training batch is 5.015604
2019-06-25 04:07:08.763422: precision = 0.100
After 49 training epoch, loss on training batch is 4.741118
2019-06-25 04:07:13.720953: precision = 0.100
After 50 training epoch, loss on training batch is 5.272372
2019-06-25 04:07:18.674429: precision = 0.100
After 1 training epoch, loss on training batch is 166.138763
2019-06-25 04:07:34.988714: precision = 0.100
After 2 training epoch, loss on training batch is 55.749962
2019-06-25 04:07:44.085952: precision = 0.100
After 3 training epoch, loss on training batch is 44.536495
2019-06-25 04:07:53.178051: precision = 0.100
After 4 training epoch, loss on training batch is 36.034904
2019-06-25 04:08:02.269121: precision = 0.100
After 5 training epoch, loss on training batch is 16.808245
2019-06-25 04:08:11.341205: precision = 0.100
After 6 training epoch, loss on training batch is 4.933353
2019-06-25 04:08:20.427787: precision = 0.100
After 7 training epoch, loss on training batch is 5.236869
2019-06-25 04:08:29.520430: precision = 0.100
After 8 training epoch, loss on training batch is 5.132515
2019-06-25 04:08:38.604751: precision = 0.100
After 9 training epoch, loss on training batch is 5.053855
2019-06-25 04:08:47.687752: precision = 0.100
After 10 training epoch, loss on training batch is 4.488747
2019-06-25 04:08:56.782668: precision = 0.156
After 11 training epoch, loss on training batch is 5.054304
2019-06-25 04:09:05.874685: precision = 0.191
After 12 training epoch, loss on training batch is 4.321912
2019-06-25 04:09:14.961131: precision = 0.212
After 13 training epoch, loss on training batch is 4.046965
2019-06-25 04:09:24.057454: precision = 0.247
After 14 training epoch, loss on training batch is 4.275795
2019-06-25 04:09:33.161785: precision = 0.235
After 15 training epoch, loss on training batch is 3.947416
2019-06-25 04:09:42.259354: precision = 0.248
After 16 training epoch, loss on training batch is 3.972592
2019-06-25 04:09:51.339794: precision = 0.263
After 17 training epoch, loss on training batch is 3.972012
2019-06-25 04:10:00.422351: precision = 0.283
After 18 training epoch, loss on training batch is 4.292859
2019-06-25 04:10:09.511452: precision = 0.282
After 19 training epoch, loss on training batch is 3.700186
2019-06-25 04:10:18.593797: precision = 0.271
After 20 training epoch, loss on training batch is 3.419871
2019-06-25 04:10:27.673881: precision = 0.289
After 21 training epoch, loss on training batch is 3.437586
2019-06-25 04:10:36.768978: precision = 0.289
After 22 training epoch, loss on training batch is 3.529765
2019-06-25 04:10:45.853893: precision = 0.361
After 23 training epoch, loss on training batch is 3.509526
2019-06-25 04:10:54.935269: precision = 0.350
After 24 training epoch, loss on training batch is 3.109935
2019-06-25 04:11:04.030423: precision = 0.344
After 25 training epoch, loss on training batch is 3.138472
2019-06-25 04:11:13.118584: precision = 0.342
After 26 training epoch, loss on training batch is 3.514740
2019-06-25 04:11:22.203056: precision = 0.351
After 27 training epoch, loss on training batch is 3.128137
2019-06-25 04:11:31.272607: precision = 0.351
After 28 training epoch, loss on training batch is 3.073471
2019-06-25 04:11:40.367097: precision = 0.356
After 29 training epoch, loss on training batch is 3.107470
2019-06-25 04:11:49.438644: precision = 0.393
After 30 training epoch, loss on training batch is 3.095250
2019-06-25 04:11:58.522515: precision = 0.392
After 31 training epoch, loss on training batch is 3.420803
2019-06-25 04:12:07.610667: precision = 0.376
After 32 training epoch, loss on training batch is 3.318940
2019-06-25 04:12:16.690334: precision = 0.383
After 33 training epoch, loss on training batch is 3.682408
2019-06-25 04:12:25.770308: precision = 0.373
After 34 training epoch, loss on training batch is 2.988172
2019-06-25 04:12:34.855604: precision = 0.359
After 35 training epoch, loss on training batch is 2.676698
2019-06-25 04:12:43.949491: precision = 0.379
After 36 training epoch, loss on training batch is 2.664204
2019-06-25 04:12:53.049792: precision = 0.361
After 37 training epoch, loss on training batch is 2.899592
2019-06-25 04:13:02.144604: precision = 0.369
After 38 training epoch, loss on training batch is 3.131610
2019-06-25 04:13:11.231450: precision = 0.376
After 39 training epoch, loss on training batch is 2.961540
2019-06-25 04:13:20.334112: precision = 0.359
After 40 training epoch, loss on training batch is 2.895321
2019-06-25 04:13:29.428578: precision = 0.409
After 41 training epoch, loss on training batch is 3.297202
2019-06-25 04:13:38.513687: precision = 0.377
After 42 training epoch, loss on training batch is 3.291825
2019-06-25 04:13:47.621089: precision = 0.417
After 43 training epoch, loss on training batch is 2.888416
2019-06-25 04:13:56.703268: precision = 0.445
After 44 training epoch, loss on training batch is 3.017226
2019-06-25 04:14:05.799112: precision = 0.412
After 45 training epoch, loss on training batch is 3.063404
2019-06-25 04:14:14.891515: precision = 0.399
After 46 training epoch, loss on training batch is 2.389494
2019-06-25 04:14:23.980652: precision = 0.408
After 47 training epoch, loss on training batch is 2.505912
2019-06-25 04:14:33.067401: precision = 0.421
After 48 training epoch, loss on training batch is 2.358138
2019-06-25 04:14:42.173178: precision = 0.421
After 49 training epoch, loss on training batch is 2.366865
2019-06-25 04:14:51.252763: precision = 0.441
After 50 training epoch, loss on training batch is 1.986361
2019-06-25 04:15:00.329429: precision = 0.435
After 1 training epoch, loss on training batch is 95.497665
2019-06-25 04:15:24.175134: precision = 0.100
After 2 training epoch, loss on training batch is 21.276007
2019-06-25 04:15:39.226579: precision = 0.100
After 3 training epoch, loss on training batch is 12.201227
2019-06-25 04:15:54.280873: precision = 0.100
After 4 training epoch, loss on training batch is 4.888966
2019-06-25 04:16:09.337176: precision = 0.100
After 5 training epoch, loss on training batch is 5.606134
2019-06-25 04:16:24.385754: precision = 0.100
After 6 training epoch, loss on training batch is 5.490904
2019-06-25 04:16:39.439236: precision = 0.100
After 7 training epoch, loss on training batch is 5.127733
2019-06-25 04:16:54.490233: precision = 0.100
After 8 training epoch, loss on training batch is 5.597336
2019-06-25 04:17:09.538587: precision = 0.100
After 9 training epoch, loss on training batch is 5.576886
2019-06-25 04:17:24.590090: precision = 0.100
After 10 training epoch, loss on training batch is 5.551222
2019-06-25 04:17:39.643918: precision = 0.100
After 11 training epoch, loss on training batch is 5.526751
2019-06-25 04:17:54.689201: precision = 0.100
After 12 training epoch, loss on training batch is 5.230939
2019-06-25 04:18:09.742280: precision = 0.168
After 13 training epoch, loss on training batch is 4.805858
2019-06-25 04:18:24.798218: precision = 0.174
After 14 training epoch, loss on training batch is 4.254576
2019-06-25 04:18:39.851796: precision = 0.226
After 15 training epoch, loss on training batch is 4.122819
2019-06-25 04:18:54.911599: precision = 0.254
After 16 training epoch, loss on training batch is 3.753855
2019-06-25 04:19:09.975858: precision = 0.283
After 17 training epoch, loss on training batch is 3.922228
2019-06-25 04:19:25.020107: precision = 0.290
After 18 training epoch, loss on training batch is 3.944266
2019-06-25 04:19:40.076514: precision = 0.344
After 19 training epoch, loss on training batch is 3.693939
2019-06-25 04:19:55.119570: precision = 0.367
After 20 training epoch, loss on training batch is 3.888405
2019-06-25 04:20:10.190583: precision = 0.368
After 21 training epoch, loss on training batch is 3.598725
2019-06-25 04:20:25.247247: precision = 0.391
After 22 training epoch, loss on training batch is 3.442136
2019-06-25 04:20:40.302457: precision = 0.369
After 23 training epoch, loss on training batch is 3.180191
2019-06-25 04:20:55.362804: precision = 0.351
After 24 training epoch, loss on training batch is 3.201044
2019-06-25 04:21:10.425559: precision = 0.370
After 25 training epoch, loss on training batch is 2.933135
2019-06-25 04:21:25.480992: precision = 0.405
After 26 training epoch, loss on training batch is 2.940682
2019-06-25 04:21:40.530554: precision = 0.399
After 27 training epoch, loss on training batch is 2.927450
2019-06-25 04:21:55.583889: precision = 0.387
After 28 training epoch, loss on training batch is 3.138620
2019-06-25 04:22:10.643806: precision = 0.417
After 29 training epoch, loss on training batch is 2.873416
2019-06-25 04:22:25.698256: precision = 0.435
After 30 training epoch, loss on training batch is 3.196712
2019-06-25 04:22:40.763975: precision = 0.446
After 31 training epoch, loss on training batch is 2.732976
2019-06-25 04:22:55.817569: precision = 0.456
After 32 training epoch, loss on training batch is 2.666839
2019-06-25 04:23:10.870454: precision = 0.481
After 33 training epoch, loss on training batch is 2.998929
2019-06-25 04:23:25.920096: precision = 0.453
After 34 training epoch, loss on training batch is 2.724016
2019-06-25 04:23:40.968087: precision = 0.460
After 35 training epoch, loss on training batch is 2.670854
2019-06-25 04:23:56.021177: precision = 0.466
After 36 training epoch, loss on training batch is 2.390277
2019-06-25 04:24:11.082628: precision = 0.466
After 37 training epoch, loss on training batch is 2.233118
2019-06-25 04:24:26.126155: precision = 0.460
After 38 training epoch, loss on training batch is 2.088085
2019-06-25 04:24:41.178751: precision = 0.427
After 39 training epoch, loss on training batch is 2.503499
2019-06-25 04:24:56.224296: precision = 0.483
After 40 training epoch, loss on training batch is 3.074241
2019-06-25 04:25:11.278538: precision = 0.495
After 41 training epoch, loss on training batch is 3.067263
2019-06-25 04:25:26.321443: precision = 0.468
After 42 training epoch, loss on training batch is 2.254989
2019-06-25 04:25:41.361060: precision = 0.455
After 43 training epoch, loss on training batch is 1.815007
2019-06-25 04:25:56.399736: precision = 0.472
After 44 training epoch, loss on training batch is 2.109278
2019-06-25 04:26:11.455589: precision = 0.480
After 45 training epoch, loss on training batch is 2.171681
2019-06-25 04:26:26.498401: precision = 0.468
After 46 training epoch, loss on training batch is 2.427014
2019-06-25 04:26:41.550174: precision = 0.487
After 47 training epoch, loss on training batch is 1.771274
2019-06-25 04:26:56.604382: precision = 0.485
After 48 training epoch, loss on training batch is 1.351535
2019-06-25 04:27:11.668060: precision = 0.477
After 49 training epoch, loss on training batch is 1.387150
2019-06-25 04:27:26.715640: precision = 0.478
After 50 training epoch, loss on training batch is 1.446731
2019-06-25 04:27:41.765287: precision = 0.489
After 1 training epoch, loss on training batch is 33.473885
2019-06-25 04:28:14.491629: precision = 0.100
After 2 training epoch, loss on training batch is 5.449473
2019-06-25 04:28:38.068405: precision = 0.100
After 3 training epoch, loss on training batch is 5.574273
2019-06-25 04:29:01.641287: precision = 0.100
After 4 training epoch, loss on training batch is 5.131511
2019-06-25 04:29:25.216210: precision = 0.100
After 5 training epoch, loss on training batch is 5.185828
2019-06-25 04:29:48.798812: precision = 0.100
After 6 training epoch, loss on training batch is 4.839059
2019-06-25 04:30:12.388426: precision = 0.100
After 7 training epoch, loss on training batch is 4.968190
2019-06-25 04:30:35.953263: precision = 0.100
After 8 training epoch, loss on training batch is 4.942097
2019-06-25 04:30:59.531764: precision = 0.100
After 9 training epoch, loss on training batch is 5.359870
2019-06-25 04:31:23.117857: precision = 0.100
After 10 training epoch, loss on training batch is 4.613225
2019-06-25 04:31:46.691019: precision = 0.100
After 11 training epoch, loss on training batch is 4.607188
2019-06-25 04:32:10.228677: precision = 0.100
After 12 training epoch, loss on training batch is 4.605596
2019-06-25 04:32:33.770457: precision = 0.100
After 13 training epoch, loss on training batch is 4.605227
2019-06-25 04:32:57.319232: precision = 0.100
After 14 training epoch, loss on training batch is 4.605480
2019-06-25 04:33:20.853618: precision = 0.100
After 15 training epoch, loss on training batch is 4.606351
2019-06-25 04:33:44.384623: precision = 0.100
After 16 training epoch, loss on training batch is 4.608439
2019-06-25 04:34:07.925192: precision = 0.100
After 17 training epoch, loss on training batch is 4.607025
2019-06-25 04:34:31.474897: precision = 0.100
After 18 training epoch, loss on training batch is 4.606503
2019-06-25 04:34:55.010393: precision = 0.100
After 19 training epoch, loss on training batch is 4.605115
2019-06-25 04:35:18.555003: precision = 0.100
After 20 training epoch, loss on training batch is 4.606053
2019-06-25 04:35:42.089621: precision = 0.100
After 21 training epoch, loss on training batch is 4.604870
2019-06-25 04:36:05.621554: precision = 0.100
After 22 training epoch, loss on training batch is 4.604829
2019-06-25 04:36:29.159069: precision = 0.100
After 23 training epoch, loss on training batch is 4.602504
2019-06-25 04:36:52.705657: precision = 0.100
After 24 training epoch, loss on training batch is 4.604229
2019-06-25 04:37:16.248601: precision = 0.100
After 25 training epoch, loss on training batch is 4.604865
2019-06-25 04:37:39.789146: precision = 0.100
After 26 training epoch, loss on training batch is 4.604515
2019-06-25 04:38:03.317140: precision = 0.100
After 27 training epoch, loss on training batch is 4.606187
2019-06-25 04:38:26.853732: precision = 0.100
After 28 training epoch, loss on training batch is 4.605492
2019-06-25 04:38:50.383669: precision = 0.100
After 29 training epoch, loss on training batch is 4.605742
2019-06-25 04:39:13.920708: precision = 0.100
After 30 training epoch, loss on training batch is 4.604378
2019-06-25 04:39:37.459820: precision = 0.100
After 31 training epoch, loss on training batch is 4.605103
2019-06-25 04:40:00.996730: precision = 0.100
After 32 training epoch, loss on training batch is 4.605815
2019-06-25 04:40:24.531689: precision = 0.100
After 33 training epoch, loss on training batch is 4.606155
2019-06-25 04:40:48.068168: precision = 0.100
After 34 training epoch, loss on training batch is 4.606059
2019-06-25 04:41:11.611472: precision = 0.100
After 35 training epoch, loss on training batch is 4.606999
2019-06-25 04:41:35.140123: precision = 0.100
After 36 training epoch, loss on training batch is 4.605668
2019-06-25 04:41:58.668198: precision = 0.100
After 37 training epoch, loss on training batch is 4.604732
2019-06-25 04:42:22.203841: precision = 0.100
After 38 training epoch, loss on training batch is 4.604801
2019-06-25 04:42:45.734686: precision = 0.100
After 39 training epoch, loss on training batch is 4.604253
2019-06-25 04:43:09.272905: precision = 0.100
After 40 training epoch, loss on training batch is 4.605276
2019-06-25 04:43:32.804030: precision = 0.100
After 41 training epoch, loss on training batch is 4.603889
2019-06-25 04:43:56.344742: precision = 0.100
After 42 training epoch, loss on training batch is 4.604910
2019-06-25 04:44:19.887621: precision = 0.100
After 43 training epoch, loss on training batch is 4.605785
2019-06-25 04:44:43.417948: precision = 0.100
After 44 training epoch, loss on training batch is 4.604176
2019-06-25 04:45:06.952915: precision = 0.100
After 45 training epoch, loss on training batch is 4.604619
2019-06-25 04:45:30.487434: precision = 0.100
After 46 training epoch, loss on training batch is 4.604499
2019-06-25 04:45:54.020998: precision = 0.100
After 47 training epoch, loss on training batch is 4.604610
2019-06-25 04:46:17.560210: precision = 0.100
After 48 training epoch, loss on training batch is 4.605877
2019-06-25 04:46:41.087598: precision = 0.100
After 49 training epoch, loss on training batch is 4.605497
2019-06-25 04:47:04.622117: precision = 0.100
After 50 training epoch, loss on training batch is 4.605617
2019-06-25 04:47:28.152094: precision = 0.100
After 1 training epoch, loss on training batch is 5.592210
2019-06-25 04:48:10.265874: precision = 0.100
After 2 training epoch, loss on training batch is 6.315858
2019-06-25 04:48:44.676786: precision = 0.100
After 3 training epoch, loss on training batch is 4.746296
2019-06-25 04:49:19.131496: precision = 0.100
After 4 training epoch, loss on training batch is 7.081600
2019-06-25 04:49:53.543250: precision = 0.100
After 5 training epoch, loss on training batch is 4.605456
2019-06-25 04:50:27.960053: precision = 0.100
After 6 training epoch, loss on training batch is 4.604444
2019-06-25 04:51:02.357720: precision = 0.100
After 7 training epoch, loss on training batch is 4.602079
2019-06-25 04:51:36.741743: precision = 0.100
After 8 training epoch, loss on training batch is 4.604454
2019-06-25 04:52:11.106664: precision = 0.100
After 9 training epoch, loss on training batch is 4.607579
2019-06-25 04:52:45.483791: precision = 0.100
After 10 training epoch, loss on training batch is 4.605128
2019-06-25 04:53:19.827722: precision = 0.100
After 11 training epoch, loss on training batch is 4.603637
2019-06-25 04:53:54.194602: precision = 0.100
After 12 training epoch, loss on training batch is 4.605693
2019-06-25 04:54:28.525158: precision = 0.100
After 13 training epoch, loss on training batch is 4.604354
2019-06-25 04:55:02.883593: precision = 0.100
After 14 training epoch, loss on training batch is 4.606826
2019-06-25 04:55:37.292664: precision = 0.100
After 15 training epoch, loss on training batch is 4.605046
2019-06-25 04:56:11.647032: precision = 0.100
After 16 training epoch, loss on training batch is 4.605330
2019-06-25 04:56:46.052021: precision = 0.100
After 17 training epoch, loss on training batch is 4.606070
2019-06-25 04:57:20.401114: precision = 0.100
After 18 training epoch, loss on training batch is 4.605917
2019-06-25 04:57:54.832925: precision = 0.100
After 19 training epoch, loss on training batch is 4.603593
2019-06-25 04:58:29.175198: precision = 0.100
After 20 training epoch, loss on training batch is 4.602835
2019-06-25 04:59:03.539247: precision = 0.100
After 21 training epoch, loss on training batch is 4.604769
2019-06-25 04:59:37.928733: precision = 0.100
After 22 training epoch, loss on training batch is 4.606312
2019-06-25 05:00:12.326494: precision = 0.100
After 23 training epoch, loss on training batch is 4.604762
2019-06-25 05:00:46.682654: precision = 0.100
After 24 training epoch, loss on training batch is 4.607228
2019-06-25 05:01:21.046275: precision = 0.100
After 25 training epoch, loss on training batch is 4.603540
2019-06-25 05:01:55.401647: precision = 0.100
After 26 training epoch, loss on training batch is 4.606576
2019-06-25 05:02:29.755966: precision = 0.100
After 27 training epoch, loss on training batch is 4.607597
2019-06-25 05:03:04.161358: precision = 0.100
After 28 training epoch, loss on training batch is 4.603954
2019-06-25 05:03:38.533605: precision = 0.100
After 29 training epoch, loss on training batch is 4.602966
2019-06-25 05:04:12.880960: precision = 0.100
After 30 training epoch, loss on training batch is 4.609467
2019-06-25 05:04:47.235266: precision = 0.100
After 31 training epoch, loss on training batch is 4.603626
2019-06-25 05:05:21.596294: precision = 0.100
After 32 training epoch, loss on training batch is 4.605316
2019-06-25 05:05:55.967219: precision = 0.100
After 33 training epoch, loss on training batch is 4.607468
2019-06-25 05:06:30.263724: precision = 0.100
After 34 training epoch, loss on training batch is 4.607968
2019-06-25 05:07:04.606226: precision = 0.100
After 35 training epoch, loss on training batch is 4.608230
2019-06-25 05:07:38.957845: precision = 0.100
After 36 training epoch, loss on training batch is 4.609643
2019-06-25 05:08:13.316203: precision = 0.100
After 37 training epoch, loss on training batch is 4.606091
2019-06-25 05:08:47.681644: precision = 0.100
After 38 training epoch, loss on training batch is 4.604162
2019-06-25 05:09:22.034734: precision = 0.100
After 39 training epoch, loss on training batch is 4.605975
2019-06-25 05:09:56.431986: precision = 0.100
After 40 training epoch, loss on training batch is 4.603685
2019-06-25 05:10:30.776427: precision = 0.100
After 41 training epoch, loss on training batch is 4.605124
2019-06-25 05:11:05.150137: precision = 0.100
After 42 training epoch, loss on training batch is 4.606260
2019-06-25 05:11:39.519536: precision = 0.100
After 43 training epoch, loss on training batch is 4.604300
2019-06-25 05:12:13.905414: precision = 0.100
After 44 training epoch, loss on training batch is 4.604970
2019-06-25 05:12:48.218977: precision = 0.100
After 45 training epoch, loss on training batch is 4.605758
2019-06-25 05:13:22.566003: precision = 0.100
After 46 training epoch, loss on training batch is 4.604825
2019-06-25 05:13:56.922970: precision = 0.100
After 47 training epoch, loss on training batch is 4.604978
2019-06-25 05:14:31.279451: precision = 0.100
After 48 training epoch, loss on training batch is 4.606668
2019-06-25 05:15:05.662268: precision = 0.100
After 49 training epoch, loss on training batch is 4.605879
2019-06-25 05:15:40.014517: precision = 0.100
After 50 training epoch, loss on training batch is 4.604539
2019-06-25 05:16:14.394907: precision = 0.100
After 1 training epoch, loss on training batch is 4.754648
2019-06-25 05:17:03.958637: precision = 0.100
After 2 training epoch, loss on training batch is 4.747119
2019-06-25 05:17:48.951310: precision = 0.100
After 3 training epoch, loss on training batch is 4.836190
2019-06-25 05:18:33.922166: precision = 0.100
After 4 training epoch, loss on training batch is 4.641990
2019-06-25 05:19:18.912257: precision = 0.100
After 5 training epoch, loss on training batch is 4.607328
2019-06-25 05:20:03.859909: precision = 0.100
After 6 training epoch, loss on training batch is 4.604959
2019-06-25 05:20:48.813562: precision = 0.100
After 7 training epoch, loss on training batch is 4.605155
2019-06-25 05:21:33.753157: precision = 0.100
After 8 training epoch, loss on training batch is 4.604931
2019-06-25 05:22:18.721783: precision = 0.100
After 9 training epoch, loss on training batch is 4.603520
2019-06-25 05:23:03.585016: precision = 0.100
After 10 training epoch, loss on training batch is 4.605132
2019-06-25 05:23:48.519684: precision = 0.100
After 11 training epoch, loss on training batch is 4.605763
2019-06-25 05:24:33.456108: precision = 0.100
After 12 training epoch, loss on training batch is 4.604866
2019-06-25 05:25:18.382449: precision = 0.100
After 13 training epoch, loss on training batch is 4.606338
2019-06-25 05:26:03.303959: precision = 0.100
After 14 training epoch, loss on training batch is 4.607562
2019-06-25 05:26:48.260664: precision = 0.100
After 15 training epoch, loss on training batch is 4.605941
2019-06-25 05:27:33.192481: precision = 0.100
After 16 training epoch, loss on training batch is 4.607652
2019-06-25 05:28:18.097325: precision = 0.100
After 17 training epoch, loss on training batch is 4.608744
2019-06-25 05:29:03.035670: precision = 0.100
After 18 training epoch, loss on training batch is 4.605301
2019-06-25 05:29:47.887302: precision = 0.100
After 19 training epoch, loss on training batch is 4.607182
2019-06-25 05:30:32.736872: precision = 0.100
After 20 training epoch, loss on training batch is 4.606428
2019-06-25 05:31:17.635349: precision = 0.100
After 21 training epoch, loss on training batch is 4.605607
2019-06-25 05:32:02.524607: precision = 0.100
After 22 training epoch, loss on training batch is 4.605022
2019-06-25 05:32:47.404019: precision = 0.100
After 23 training epoch, loss on training batch is 4.603374
2019-06-25 05:33:32.250996: precision = 0.100
After 24 training epoch, loss on training batch is 4.605631
2019-06-25 05:34:17.114732: precision = 0.100
After 25 training epoch, loss on training batch is 4.604881
2019-06-25 05:35:02.036698: precision = 0.100
After 26 training epoch, loss on training batch is 4.605096
2019-06-25 05:35:46.962463: precision = 0.100
After 27 training epoch, loss on training batch is 4.605667
2019-06-25 05:36:31.904587: precision = 0.100
After 28 training epoch, loss on training batch is 4.603958
2019-06-25 05:37:16.812002: precision = 0.100
After 29 training epoch, loss on training batch is 4.603711
2019-06-25 05:38:01.741356: precision = 0.100
After 30 training epoch, loss on training batch is 4.604150
2019-06-25 05:38:46.601235: precision = 0.100
After 31 training epoch, loss on training batch is 4.602043
2019-06-25 05:39:31.500544: precision = 0.100
After 32 training epoch, loss on training batch is 4.604398
2019-06-25 05:40:16.388292: precision = 0.100
After 33 training epoch, loss on training batch is 4.605301
2019-06-25 05:41:01.307839: precision = 0.100
After 34 training epoch, loss on training batch is 4.607142
2019-06-25 05:41:46.235181: precision = 0.100
After 35 training epoch, loss on training batch is 4.606323
2019-06-25 05:42:31.159205: precision = 0.100
After 36 training epoch, loss on training batch is 4.604731
2019-06-25 05:43:16.062830: precision = 0.100
After 37 training epoch, loss on training batch is 4.603872
2019-06-25 05:44:00.952537: precision = 0.100
After 38 training epoch, loss on training batch is 4.606107
2019-06-25 05:44:45.810296: precision = 0.100
After 39 training epoch, loss on training batch is 4.603665
2019-06-25 05:45:30.773498: precision = 0.100
After 40 training epoch, loss on training batch is 4.607896
2019-06-25 05:46:15.662325: precision = 0.100
After 41 training epoch, loss on training batch is 4.605735
2019-06-25 05:47:00.586752: precision = 0.100
After 42 training epoch, loss on training batch is 4.600278
2019-06-25 05:47:45.480896: precision = 0.100
After 43 training epoch, loss on training batch is 4.606070
2019-06-25 05:48:30.378130: precision = 0.100
After 44 training epoch, loss on training batch is 4.605762
2019-06-25 05:49:15.305592: precision = 0.100
After 45 training epoch, loss on training batch is 4.606761
2019-06-25 05:50:00.283795: precision = 0.100
After 46 training epoch, loss on training batch is 4.605456
2019-06-25 05:50:45.239328: precision = 0.100
After 47 training epoch, loss on training batch is 4.607728
2019-06-25 05:51:30.136061: precision = 0.100
After 48 training epoch, loss on training batch is 4.606838
2019-06-25 05:52:15.103640: precision = 0.100
After 49 training epoch, loss on training batch is 4.607592
2019-06-25 05:53:00.016409: precision = 0.100
After 50 training epoch, loss on training batch is 4.607931
2019-06-25 05:53:44.930564: precision = 0.100
After 1 training epoch, loss on training batch is 10.163352
2019-06-25 05:54:29.750632: precision = 0.100
After 2 training epoch, loss on training batch is 4.731406
2019-06-25 05:55:12.939519: precision = 0.100
After 3 training epoch, loss on training batch is 4.853309
2019-06-25 05:55:56.104357: precision = 0.165
After 4 training epoch, loss on training batch is 3.728180
2019-06-25 05:56:39.272441: precision = 0.282
After 5 training epoch, loss on training batch is 3.582634
2019-06-25 05:57:22.442526: precision = 0.356
After 6 training epoch, loss on training batch is 3.177084
2019-06-25 05:58:05.618057: precision = 0.382
After 7 training epoch, loss on training batch is 2.958315
2019-06-25 05:58:48.789021: precision = 0.406
After 8 training epoch, loss on training batch is 2.684935
2019-06-25 05:59:31.959301: precision = 0.441
After 9 training epoch, loss on training batch is 2.814111
2019-06-25 06:00:15.133433: precision = 0.454
After 10 training epoch, loss on training batch is 2.829705
2019-06-25 06:00:58.305360: precision = 0.494
After 11 training epoch, loss on training batch is 2.726693
2019-06-25 06:01:41.469373: precision = 0.521
After 12 training epoch, loss on training batch is 2.264344
2019-06-25 06:02:24.638263: precision = 0.531
After 13 training epoch, loss on training batch is 2.235173
2019-06-25 06:03:07.813953: precision = 0.551
After 14 training epoch, loss on training batch is 2.217479
2019-06-25 06:03:50.980084: precision = 0.561
After 15 training epoch, loss on training batch is 2.056470
2019-06-25 06:04:34.153784: precision = 0.568
After 16 training epoch, loss on training batch is 1.819523
2019-06-25 06:05:17.323605: precision = 0.552
After 17 training epoch, loss on training batch is 2.084768
2019-06-25 06:06:00.482840: precision = 0.593
After 18 training epoch, loss on training batch is 2.414649
2019-06-25 06:06:43.633218: precision = 0.568
After 19 training epoch, loss on training batch is 2.078644
2019-06-25 06:07:26.794720: precision = 0.600
After 20 training epoch, loss on training batch is 1.623294
2019-06-25 06:08:09.950708: precision = 0.589
After 21 training epoch, loss on training batch is 1.808332
2019-06-25 06:08:53.102939: precision = 0.599
After 22 training epoch, loss on training batch is 1.429366
2019-06-25 06:09:36.273141: precision = 0.595
After 23 training epoch, loss on training batch is 1.426526
2019-06-25 06:10:19.421201: precision = 0.597
After 24 training epoch, loss on training batch is 1.327924
2019-06-25 06:11:02.577343: precision = 0.604
After 25 training epoch, loss on training batch is 1.574184
2019-06-25 06:11:45.726303: precision = 0.607
After 26 training epoch, loss on training batch is 1.115654
2019-06-25 06:12:28.910051: precision = 0.601
After 27 training epoch, loss on training batch is 1.160959
2019-06-25 06:13:12.053662: precision = 0.591
After 28 training epoch, loss on training batch is 1.159091
2019-06-25 06:13:55.197564: precision = 0.598
After 29 training epoch, loss on training batch is 1.451746
2019-06-25 06:14:38.343639: precision = 0.595
After 30 training epoch, loss on training batch is 1.377578
2019-06-25 06:15:21.492324: precision = 0.588
After 31 training epoch, loss on training batch is 1.085513
2019-06-25 06:16:04.633873: precision = 0.579
After 32 training epoch, loss on training batch is 0.663774
2019-06-25 06:16:47.781656: precision = 0.607
After 33 training epoch, loss on training batch is 0.663358
2019-06-25 06:17:30.927949: precision = 0.609
After 34 training epoch, loss on training batch is 0.603751
2019-06-25 06:18:14.064558: precision = 0.601
After 35 training epoch, loss on training batch is 1.035915
2019-06-25 06:18:57.198133: precision = 0.589
After 36 training epoch, loss on training batch is 0.853054
2019-06-25 06:19:40.354125: precision = 0.601
After 37 training epoch, loss on training batch is 0.608719
2019-06-25 06:20:23.503497: precision = 0.591
After 38 training epoch, loss on training batch is 0.581615
2019-06-25 06:21:06.645976: precision = 0.607
After 39 training epoch, loss on training batch is 0.514779
2019-06-25 06:21:49.789443: precision = 0.597
After 40 training epoch, loss on training batch is 0.354029
2019-06-25 06:22:32.918742: precision = 0.617
After 41 training epoch, loss on training batch is 0.847140
2019-06-25 06:23:16.089005: precision = 0.618
After 42 training epoch, loss on training batch is 0.999298
2019-06-25 06:23:59.217931: precision = 0.600
After 43 training epoch, loss on training batch is 0.759676
2019-06-25 06:24:42.349771: precision = 0.599
After 44 training epoch, loss on training batch is 0.382875
2019-06-25 06:25:25.490631: precision = 0.604
After 45 training epoch, loss on training batch is 0.688570
2019-06-25 06:26:08.652292: precision = 0.610
After 46 training epoch, loss on training batch is 0.533091
2019-06-25 06:26:51.783200: precision = 0.616
After 47 training epoch, loss on training batch is 0.809106
2019-06-25 06:27:34.916480: precision = 0.607
After 48 training epoch, loss on training batch is 0.487569
2019-06-25 06:28:18.059718: precision = 0.612
After 49 training epoch, loss on training batch is 0.150169
2019-06-25 06:29:01.195328: precision = 0.606
After 50 training epoch, loss on training batch is 0.469141
2019-06-25 06:29:44.326861: precision = 0.599
After 1 training epoch, loss on training batch is 4.839791
2019-06-25 06:30:30.410872: precision = 0.100
After 2 training epoch, loss on training batch is 5.144783
2019-06-25 06:31:14.837155: precision = 0.100
After 3 training epoch, loss on training batch is 4.394795
2019-06-25 06:31:59.266692: precision = 0.160
After 4 training epoch, loss on training batch is 3.716352
2019-06-25 06:32:43.675209: precision = 0.278
After 5 training epoch, loss on training batch is 3.466858
2019-06-25 06:33:28.113177: precision = 0.339
After 6 training epoch, loss on training batch is 3.248768
2019-06-25 06:34:12.544230: precision = 0.405
After 7 training epoch, loss on training batch is 3.002626
2019-06-25 06:34:56.963076: precision = 0.418
After 8 training epoch, loss on training batch is 2.628899
2019-06-25 06:35:41.385173: precision = 0.447
After 9 training epoch, loss on training batch is 2.829485
2019-06-25 06:36:25.813141: precision = 0.483
After 10 training epoch, loss on training batch is 2.584469
2019-06-25 06:37:10.251409: precision = 0.499
After 11 training epoch, loss on training batch is 2.610809
2019-06-25 06:37:54.676855: precision = 0.504
After 12 training epoch, loss on training batch is 2.626432
2019-06-25 06:38:39.109453: precision = 0.516
After 13 training epoch, loss on training batch is 2.180640
2019-06-25 06:39:23.528338: precision = 0.530
After 14 training epoch, loss on training batch is 2.305080
2019-06-25 06:40:07.961139: precision = 0.534
After 15 training epoch, loss on training batch is 2.101146
2019-06-25 06:40:52.374921: precision = 0.560
After 16 training epoch, loss on training batch is 1.843353
2019-06-25 06:41:36.791001: precision = 0.581
After 17 training epoch, loss on training batch is 2.365844
2019-06-25 06:42:21.212005: precision = 0.572
After 18 training epoch, loss on training batch is 2.154758
2019-06-25 06:43:05.630216: precision = 0.586
After 19 training epoch, loss on training batch is 1.850807
2019-06-25 06:43:50.038851: precision = 0.571
After 20 training epoch, loss on training batch is 1.261655
2019-06-25 06:44:34.454412: precision = 0.586
After 21 training epoch, loss on training batch is 1.705658
2019-06-25 06:45:18.871739: precision = 0.594
After 22 training epoch, loss on training batch is 1.760515
2019-06-25 06:46:03.285742: precision = 0.591
After 23 training epoch, loss on training batch is 1.568992
2019-06-25 06:46:47.697889: precision = 0.587
After 24 training epoch, loss on training batch is 1.547594
2019-06-25 06:47:32.118621: precision = 0.588
After 25 training epoch, loss on training batch is 1.498406
2019-06-25 06:48:16.535147: precision = 0.601
After 26 training epoch, loss on training batch is 1.525935
2019-06-25 06:49:00.936247: precision = 0.600
After 27 training epoch, loss on training batch is 1.194026
2019-06-25 06:49:45.335667: precision = 0.620
After 28 training epoch, loss on training batch is 1.260055
2019-06-25 06:50:29.741418: precision = 0.617
After 29 training epoch, loss on training batch is 1.292916
2019-06-25 06:51:14.154845: precision = 0.590
After 30 training epoch, loss on training batch is 1.235295
2019-06-25 06:51:58.548039: precision = 0.601
After 31 training epoch, loss on training batch is 0.953562
2019-06-25 06:52:42.950672: precision = 0.617
After 32 training epoch, loss on training batch is 0.670986
2019-06-25 06:53:27.352820: precision = 0.607
After 33 training epoch, loss on training batch is 0.779709
2019-06-25 06:54:11.758022: precision = 0.613
After 34 training epoch, loss on training batch is 0.750326
2019-06-25 06:54:56.166806: precision = 0.624
After 35 training epoch, loss on training batch is 0.903016
2019-06-25 06:55:40.573097: precision = 0.613
After 36 training epoch, loss on training batch is 1.077286
2019-06-25 06:56:24.968887: precision = 0.592
After 37 training epoch, loss on training batch is 0.733702
2019-06-25 06:57:09.363297: precision = 0.601
After 38 training epoch, loss on training batch is 0.707244
2019-06-25 06:57:53.776267: precision = 0.615
After 39 training epoch, loss on training batch is 0.619718
2019-06-25 06:58:38.188954: precision = 0.620
After 40 training epoch, loss on training batch is 0.630004
2019-06-25 06:59:22.588675: precision = 0.611
After 41 training epoch, loss on training batch is 0.697091
2019-06-25 07:00:06.996932: precision = 0.623
After 42 training epoch, loss on training batch is 0.786049
2019-06-25 07:00:51.407936: precision = 0.628
After 43 training epoch, loss on training batch is 0.838534
2019-06-25 07:01:35.823326: precision = 0.622
After 44 training epoch, loss on training batch is 1.022681
2019-06-25 07:02:20.218682: precision = 0.626
After 45 training epoch, loss on training batch is 0.703470
2019-06-25 07:03:04.608971: precision = 0.619
After 46 training epoch, loss on training batch is 0.700447
2019-06-25 07:03:49.010082: precision = 0.618
After 47 training epoch, loss on training batch is 0.891505
2019-06-25 07:04:33.404585: precision = 0.618
After 48 training epoch, loss on training batch is 0.738106
2019-06-25 07:05:17.796216: precision = 0.616
After 49 training epoch, loss on training batch is 0.301296
2019-06-25 07:06:02.195209: precision = 0.612
After 50 training epoch, loss on training batch is 0.518450
2019-06-25 07:06:46.584560: precision = 0.613
After 1 training epoch, loss on training batch is 11.326982
2019-06-25 07:07:33.352535: precision = 0.100
After 2 training epoch, loss on training batch is 4.951686
2019-06-25 07:08:18.480116: precision = 0.100
After 3 training epoch, loss on training batch is 5.167385
2019-06-25 07:09:03.627228: precision = 0.100
After 4 training epoch, loss on training batch is 4.656663
2019-06-25 07:09:48.754916: precision = 0.100
After 5 training epoch, loss on training batch is 4.658158
2019-06-25 07:10:33.864529: precision = 0.100
After 6 training epoch, loss on training batch is 4.670305
2019-06-25 07:11:19.037186: precision = 0.100
After 7 training epoch, loss on training batch is 4.615254
2019-06-25 07:12:04.162984: precision = 0.100
After 8 training epoch, loss on training batch is 4.604782
2019-06-25 07:12:49.224091: precision = 0.100
After 9 training epoch, loss on training batch is 4.603547
2019-06-25 07:13:34.310177: precision = 0.100
After 10 training epoch, loss on training batch is 4.605539
2019-06-25 07:14:19.410284: precision = 0.100
After 11 training epoch, loss on training batch is 4.606119
2019-06-25 07:15:04.450345: precision = 0.100
After 12 training epoch, loss on training batch is 4.604999
2019-06-25 07:15:49.459864: precision = 0.100
After 13 training epoch, loss on training batch is 4.606241
2019-06-25 07:16:34.489521: precision = 0.100
After 14 training epoch, loss on training batch is 4.606244
2019-06-25 07:17:19.519712: precision = 0.100
After 15 training epoch, loss on training batch is 4.605505
2019-06-25 07:18:04.569177: precision = 0.100
After 16 training epoch, loss on training batch is 4.608638
2019-06-25 07:18:49.619318: precision = 0.100
After 17 training epoch, loss on training batch is 4.608552
2019-06-25 07:19:34.610400: precision = 0.100
After 18 training epoch, loss on training batch is 4.604681
2019-06-25 07:20:19.633349: precision = 0.100
After 19 training epoch, loss on training batch is 4.607058
2019-06-25 07:21:04.666234: precision = 0.100
After 20 training epoch, loss on training batch is 4.607080
2019-06-25 07:21:49.696840: precision = 0.100
After 21 training epoch, loss on training batch is 4.605619
2019-06-25 07:22:34.767973: precision = 0.100
After 22 training epoch, loss on training batch is 4.604948
2019-06-25 07:23:19.793797: precision = 0.100
After 23 training epoch, loss on training batch is 4.603521
2019-06-25 07:24:04.840636: precision = 0.100
After 24 training epoch, loss on training batch is 4.606009
2019-06-25 07:24:49.908196: precision = 0.100
After 25 training epoch, loss on training batch is 4.603849
2019-06-25 07:25:34.907980: precision = 0.100
After 26 training epoch, loss on training batch is 4.605295
2019-06-25 07:26:19.922762: precision = 0.100
After 27 training epoch, loss on training batch is 4.605362
2019-06-25 07:27:05.025367: precision = 0.100
After 28 training epoch, loss on training batch is 4.604342
2019-06-25 07:27:50.059809: precision = 0.100
After 29 training epoch, loss on training batch is 4.604464
2019-06-25 07:28:35.081081: precision = 0.100
After 30 training epoch, loss on training batch is 4.604428
2019-06-25 07:29:20.112531: precision = 0.100
After 31 training epoch, loss on training batch is 4.602228
2019-06-25 07:30:05.105752: precision = 0.100
After 32 training epoch, loss on training batch is 4.604567
2019-06-25 07:30:50.148222: precision = 0.100
After 33 training epoch, loss on training batch is 4.605206
2019-06-25 07:31:35.240316: precision = 0.100
After 34 training epoch, loss on training batch is 4.606510
2019-06-25 07:32:20.306094: precision = 0.100
After 35 training epoch, loss on training batch is 4.608090
2019-06-25 07:33:05.347846: precision = 0.100
After 36 training epoch, loss on training batch is 4.604913
2019-06-25 07:33:50.430701: precision = 0.100
After 37 training epoch, loss on training batch is 4.603529
2019-06-25 07:34:35.437224: precision = 0.100
After 38 training epoch, loss on training batch is 4.606549
2019-06-25 07:35:20.496168: precision = 0.100
After 39 training epoch, loss on training batch is 4.603313
2019-06-25 07:36:05.541266: precision = 0.100
After 40 training epoch, loss on training batch is 4.608806
2019-06-25 07:36:50.628931: precision = 0.100
After 41 training epoch, loss on training batch is 4.604692
2019-06-25 07:37:35.655843: precision = 0.100
After 42 training epoch, loss on training batch is 4.598815
2019-06-25 07:38:20.661611: precision = 0.100
After 43 training epoch, loss on training batch is 4.605889
2019-06-25 07:39:05.687982: precision = 0.100
After 44 training epoch, loss on training batch is 4.605938
2019-06-25 07:39:50.728359: precision = 0.100
After 45 training epoch, loss on training batch is 4.605807
2019-06-25 07:40:35.787464: precision = 0.100
After 46 training epoch, loss on training batch is 4.605254
2019-06-25 07:41:20.854144: precision = 0.100
After 47 training epoch, loss on training batch is 4.608103
2019-06-25 07:42:05.883932: precision = 0.100
After 48 training epoch, loss on training batch is 4.606915
2019-06-25 07:42:50.937517: precision = 0.100
After 49 training epoch, loss on training batch is 4.607736
2019-06-25 07:43:35.972752: precision = 0.100
After 50 training epoch, loss on training batch is 4.607955
2019-06-25 07:44:21.013880: precision = 0.100
After 1 training epoch, loss on training batch is 4.996849
2019-06-25 07:45:08.117788: precision = 0.100
After 2 training epoch, loss on training batch is 4.978237
2019-06-25 07:45:53.583610: precision = 0.100
After 3 training epoch, loss on training batch is 5.137254
2019-06-25 07:46:39.071630: precision = 0.100
After 4 training epoch, loss on training batch is 4.600488
2019-06-25 07:47:24.551593: precision = 0.100
After 5 training epoch, loss on training batch is 4.632874
2019-06-25 07:48:10.035624: precision = 0.100
After 6 training epoch, loss on training batch is 4.604928
2019-06-25 07:48:55.459296: precision = 0.100
After 7 training epoch, loss on training batch is 4.604792
2019-06-25 07:49:40.879084: precision = 0.100
After 8 training epoch, loss on training batch is 4.604844
2019-06-25 07:50:26.279193: precision = 0.100
After 9 training epoch, loss on training batch is 4.603548
2019-06-25 07:51:11.687154: precision = 0.100
After 10 training epoch, loss on training batch is 4.605619
2019-06-25 07:51:57.086932: precision = 0.100
After 11 training epoch, loss on training batch is 4.605760
2019-06-25 07:52:42.490341: precision = 0.100
After 12 training epoch, loss on training batch is 4.605013
2019-06-25 07:53:27.905571: precision = 0.100
After 13 training epoch, loss on training batch is 4.606403
2019-06-25 07:54:13.315215: precision = 0.100
After 14 training epoch, loss on training batch is 4.605825
2019-06-25 07:54:58.703166: precision = 0.100
After 15 training epoch, loss on training batch is 4.605380
2019-06-25 07:55:44.113892: precision = 0.100
After 16 training epoch, loss on training batch is 4.606954
2019-06-25 07:56:29.516009: precision = 0.100
After 17 training epoch, loss on training batch is 4.607572
2019-06-25 07:57:14.926659: precision = 0.100
After 18 training epoch, loss on training batch is 4.604779
2019-06-25 07:58:00.327866: precision = 0.100
After 19 training epoch, loss on training batch is 4.606287
2019-06-25 07:58:45.725759: precision = 0.100
After 20 training epoch, loss on training batch is 4.606580
2019-06-25 07:59:31.134249: precision = 0.100
After 21 training epoch, loss on training batch is 4.605497
2019-06-25 08:00:16.534666: precision = 0.100
After 22 training epoch, loss on training batch is 4.605166
2019-06-25 08:01:01.941776: precision = 0.100
After 23 training epoch, loss on training batch is 4.603734
2019-06-25 08:01:47.334717: precision = 0.100
After 24 training epoch, loss on training batch is 4.605749
2019-06-25 08:02:32.736897: precision = 0.100
After 25 training epoch, loss on training batch is 4.604813
2019-06-25 08:03:18.138822: precision = 0.100
After 26 training epoch, loss on training batch is 4.605010
2019-06-25 08:04:03.542499: precision = 0.100
After 27 training epoch, loss on training batch is 4.605459
2019-06-25 08:04:48.934561: precision = 0.100
After 28 training epoch, loss on training batch is 4.604417
2019-06-25 08:05:34.342541: precision = 0.100
After 29 training epoch, loss on training batch is 4.604114
2019-06-25 08:06:19.758957: precision = 0.100
After 30 training epoch, loss on training batch is 4.604450
2019-06-25 08:07:05.166040: precision = 0.100
After 31 training epoch, loss on training batch is 4.602665
2019-06-25 08:07:50.559025: precision = 0.100
After 32 training epoch, loss on training batch is 4.604668
2019-06-25 08:08:35.955942: precision = 0.100
After 33 training epoch, loss on training batch is 4.605340
2019-06-25 08:09:21.356685: precision = 0.100
After 34 training epoch, loss on training batch is 4.606413
2019-06-25 08:10:06.760890: precision = 0.100
After 35 training epoch, loss on training batch is 4.606159
2019-06-25 08:10:52.155200: precision = 0.100
After 36 training epoch, loss on training batch is 4.604974
2019-06-25 08:11:37.552229: precision = 0.100
After 37 training epoch, loss on training batch is 4.604060
2019-06-25 08:12:22.958025: precision = 0.100
After 38 training epoch, loss on training batch is 4.605951
2019-06-25 08:13:08.361379: precision = 0.100
After 39 training epoch, loss on training batch is 4.603821
2019-06-25 08:13:53.756984: precision = 0.100
After 40 training epoch, loss on training batch is 4.607498
2019-06-25 08:14:39.164901: precision = 0.100
After 41 training epoch, loss on training batch is 4.605748
2019-06-25 08:15:24.560100: precision = 0.100
After 42 training epoch, loss on training batch is 4.601045
2019-06-25 08:16:09.956772: precision = 0.100
After 43 training epoch, loss on training batch is 4.605810
2019-06-25 08:16:55.360559: precision = 0.100
After 44 training epoch, loss on training batch is 4.605722
2019-06-25 08:17:40.758429: precision = 0.100
After 45 training epoch, loss on training batch is 4.605976
2019-06-25 08:18:26.160123: precision = 0.100
After 46 training epoch, loss on training batch is 4.605442
2019-06-25 08:19:11.566329: precision = 0.100
After 47 training epoch, loss on training batch is 4.607368
2019-06-25 08:19:56.963689: precision = 0.100
After 48 training epoch, loss on training batch is 4.606678
2019-06-25 08:20:42.367482: precision = 0.100
After 49 training epoch, loss on training batch is 4.607462
2019-06-25 08:21:27.761827: precision = 0.100
After 50 training epoch, loss on training batch is 4.607745
2019-06-25 08:22:13.172138: precision = 0.100
After 1 training epoch, loss on training batch is 9.629061
2019-06-25 08:22:59.367039: precision = 0.100
After 2 training epoch, loss on training batch is 5.033074
2019-06-25 08:23:43.217709: precision = 0.100
After 3 training epoch, loss on training batch is 32.876621
2019-06-25 08:24:27.052291: precision = 0.100
After 4 training epoch, loss on training batch is 4.613723
2019-06-25 08:25:10.874700: precision = 0.100
After 5 training epoch, loss on training batch is 4.585235
2019-06-25 08:25:54.704819: precision = 0.100
After 6 training epoch, loss on training batch is 4.604140
2019-06-25 08:26:38.486192: precision = 0.100
After 7 training epoch, loss on training batch is 4.604307
2019-06-25 08:27:22.250184: precision = 0.100
After 8 training epoch, loss on training batch is 4.604726
2019-06-25 08:28:05.997406: precision = 0.100
After 9 training epoch, loss on training batch is 4.603591
2019-06-25 08:28:49.760799: precision = 0.100
After 10 training epoch, loss on training batch is 4.605281
2019-06-25 08:29:33.505924: precision = 0.100
After 11 training epoch, loss on training batch is 4.605632
2019-06-25 08:30:17.261668: precision = 0.100
After 12 training epoch, loss on training batch is 4.605276
2019-06-25 08:31:01.002986: precision = 0.100
After 13 training epoch, loss on training batch is 4.605978
2019-06-25 08:31:44.761779: precision = 0.100
After 14 training epoch, loss on training batch is 4.605666
2019-06-25 08:32:28.505121: precision = 0.100
After 15 training epoch, loss on training batch is 4.605452
2019-06-25 08:33:12.257046: precision = 0.100
After 16 training epoch, loss on training batch is 4.606941
2019-06-25 08:33:56.007666: precision = 0.100
After 17 training epoch, loss on training batch is 4.607664
2019-06-25 08:34:39.770193: precision = 0.100
After 18 training epoch, loss on training batch is 4.604994
2019-06-25 08:35:23.543333: precision = 0.100
After 19 training epoch, loss on training batch is 4.606377
2019-06-25 08:36:07.301360: precision = 0.100
After 20 training epoch, loss on training batch is 4.606550
2019-06-25 08:36:51.046794: precision = 0.100
After 21 training epoch, loss on training batch is 4.605496
2019-06-25 08:37:34.776416: precision = 0.100
After 22 training epoch, loss on training batch is 4.605145
2019-06-25 08:38:18.529815: precision = 0.100
After 23 training epoch, loss on training batch is 4.603717
2019-06-25 08:39:02.288221: precision = 0.100
After 24 training epoch, loss on training batch is 4.605741
2019-06-25 08:39:46.039028: precision = 0.100
After 25 training epoch, loss on training batch is 4.604812
2019-06-25 08:40:29.792678: precision = 0.100
After 26 training epoch, loss on training batch is 4.605010
2019-06-25 08:41:13.543767: precision = 0.100
After 27 training epoch, loss on training batch is 4.605459
2019-06-25 08:41:57.291185: precision = 0.100
After 28 training epoch, loss on training batch is 4.604417
2019-06-25 08:42:41.032455: precision = 0.100
After 29 training epoch, loss on training batch is 4.604114
2019-06-25 08:43:24.770681: precision = 0.100
After 30 training epoch, loss on training batch is 4.604450
2019-06-25 08:44:08.527992: precision = 0.100
After 31 training epoch, loss on training batch is 4.602665
2019-06-25 08:44:52.250669: precision = 0.100
After 32 training epoch, loss on training batch is 4.604667
2019-06-25 08:45:36.001557: precision = 0.100
After 33 training epoch, loss on training batch is 4.605340
2019-06-25 08:46:19.740365: precision = 0.100
After 34 training epoch, loss on training batch is 4.606413
2019-06-25 08:47:03.485126: precision = 0.100
After 35 training epoch, loss on training batch is 4.606159
2019-06-25 08:47:47.222277: precision = 0.100
After 36 training epoch, loss on training batch is 4.604974
2019-06-25 08:48:31.004327: precision = 0.100
After 37 training epoch, loss on training batch is 4.604060
2019-06-25 08:49:14.754361: precision = 0.100
After 38 training epoch, loss on training batch is 4.605951
2019-06-25 08:49:58.493069: precision = 0.100
After 39 training epoch, loss on training batch is 4.603821
2019-06-25 08:50:42.251447: precision = 0.100
After 40 training epoch, loss on training batch is 4.607498
2019-06-25 08:51:26.008823: precision = 0.100
After 41 training epoch, loss on training batch is 4.605748
2019-06-25 08:52:09.764535: precision = 0.100
After 42 training epoch, loss on training batch is 4.601045
2019-06-25 08:52:53.497775: precision = 0.100
After 43 training epoch, loss on training batch is 4.605810
2019-06-25 08:53:37.262885: precision = 0.100
After 44 training epoch, loss on training batch is 4.605722
2019-06-25 08:54:21.002868: precision = 0.100
After 45 training epoch, loss on training batch is 4.605976
2019-06-25 08:55:04.753800: precision = 0.100
After 46 training epoch, loss on training batch is 4.605442
2019-06-25 08:55:48.509583: precision = 0.100
After 47 training epoch, loss on training batch is 4.607368
2019-06-25 08:56:32.273772: precision = 0.100
After 48 training epoch, loss on training batch is 4.606678
2019-06-25 08:57:16.020733: precision = 0.100
After 49 training epoch, loss on training batch is 4.607462
2019-06-25 08:57:59.755703: precision = 0.100
After 50 training epoch, loss on training batch is 4.607745
2019-06-25 08:58:43.507061: precision = 0.100
After 1 training epoch, loss on training batch is 5.058187
2019-06-25 08:59:29.482921: precision = 0.100
After 2 training epoch, loss on training batch is 4.648550
2019-06-25 09:00:13.830758: precision = 0.100
After 3 training epoch, loss on training batch is 4.681732
2019-06-25 09:00:58.168106: precision = 0.100
After 4 training epoch, loss on training batch is 4.644210
2019-06-25 09:01:42.509161: precision = 0.100
After 5 training epoch, loss on training batch is 4.605146
2019-06-25 09:02:26.825793: precision = 0.100
After 6 training epoch, loss on training batch is 4.604793
2019-06-25 09:03:11.104009: precision = 0.100
After 7 training epoch, loss on training batch is 4.604493
2019-06-25 09:03:55.370495: precision = 0.100
After 8 training epoch, loss on training batch is 4.604710
2019-06-25 09:04:39.625875: precision = 0.100
After 9 training epoch, loss on training batch is 4.603203
2019-06-25 09:05:23.885284: precision = 0.100
After 10 training epoch, loss on training batch is 4.605406
2019-06-25 09:06:08.175781: precision = 0.100
After 11 training epoch, loss on training batch is 4.605725
2019-06-25 09:06:52.437342: precision = 0.100
After 12 training epoch, loss on training batch is 4.604944
2019-06-25 09:07:36.691091: precision = 0.100
After 13 training epoch, loss on training batch is 4.606258
2019-06-25 09:08:20.965811: precision = 0.100
After 14 training epoch, loss on training batch is 4.606125
2019-06-25 09:09:05.229921: precision = 0.100
After 15 training epoch, loss on training batch is 4.605870
2019-06-25 09:09:49.461952: precision = 0.100
After 16 training epoch, loss on training batch is 4.607778
2019-06-25 09:10:33.719277: precision = 0.100
After 17 training epoch, loss on training batch is 4.608740
2019-06-25 09:11:17.962214: precision = 0.100
After 18 training epoch, loss on training batch is 4.605482
2019-06-25 09:12:02.237978: precision = 0.100
After 19 training epoch, loss on training batch is 4.607142
2019-06-25 09:12:46.475389: precision = 0.100
After 20 training epoch, loss on training batch is 4.607635
2019-06-25 09:13:30.751174: precision = 0.100
After 21 training epoch, loss on training batch is 4.605417
2019-06-25 09:14:15.014548: precision = 0.100
After 22 training epoch, loss on training batch is 4.604956
2019-06-25 09:14:59.245146: precision = 0.100
After 23 training epoch, loss on training batch is 4.602811
2019-06-25 09:15:43.506385: precision = 0.100
After 24 training epoch, loss on training batch is 4.605375
2019-06-25 09:16:27.764859: precision = 0.100
After 25 training epoch, loss on training batch is 4.604445
2019-06-25 09:17:12.042191: precision = 0.100
After 26 training epoch, loss on training batch is 4.604912
2019-06-25 09:17:56.297365: precision = 0.100
After 27 training epoch, loss on training batch is 4.605717
2019-06-25 09:18:40.553485: precision = 0.100
After 28 training epoch, loss on training batch is 4.604348
2019-06-25 09:19:24.835776: precision = 0.100
After 29 training epoch, loss on training batch is 4.603527
2019-06-25 09:20:09.099064: precision = 0.100
After 30 training epoch, loss on training batch is 4.604315
2019-06-25 09:20:53.334113: precision = 0.100
After 31 training epoch, loss on training batch is 4.602041
2019-06-25 09:21:37.585061: precision = 0.100
After 32 training epoch, loss on training batch is 4.604367
2019-06-25 09:22:21.841171: precision = 0.100
After 33 training epoch, loss on training batch is 4.604916
2019-06-25 09:23:06.088965: precision = 0.100
After 34 training epoch, loss on training batch is 4.606952
2019-06-25 09:23:50.339682: precision = 0.100
After 35 training epoch, loss on training batch is 4.606701
2019-06-25 09:24:34.602403: precision = 0.100
After 36 training epoch, loss on training batch is 4.604898
2019-06-25 09:25:18.851429: precision = 0.100
After 37 training epoch, loss on training batch is 4.603802
2019-06-25 09:26:03.114529: precision = 0.100
After 38 training epoch, loss on training batch is 4.606043
2019-06-25 09:26:47.360603: precision = 0.100
After 39 training epoch, loss on training batch is 4.603720
2019-06-25 09:27:31.586058: precision = 0.100
After 40 training epoch, loss on training batch is 4.607790
2019-06-25 09:28:15.848557: precision = 0.100
After 41 training epoch, loss on training batch is 4.605780
2019-06-25 09:29:00.106718: precision = 0.100
After 42 training epoch, loss on training batch is 4.600594
2019-06-25 09:29:44.372543: precision = 0.100
After 43 training epoch, loss on training batch is 4.605809
2019-06-25 09:30:28.622815: precision = 0.100
After 44 training epoch, loss on training batch is 4.605802
2019-06-25 09:31:12.868574: precision = 0.100
After 45 training epoch, loss on training batch is 4.605947
2019-06-25 09:31:57.114694: precision = 0.100
After 46 training epoch, loss on training batch is 4.605443
2019-06-25 09:32:41.375121: precision = 0.100
After 47 training epoch, loss on training batch is 4.607624
2019-06-25 09:33:25.634842: precision = 0.100
After 48 training epoch, loss on training batch is 4.606821
2019-06-25 09:34:09.897245: precision = 0.100
After 49 training epoch, loss on training batch is 4.607548
2019-06-25 09:34:54.155180: precision = 0.100
After 50 training epoch, loss on training batch is 4.607701
2019-06-25 09:35:38.423657: precision = 0.100
Traceback (most recent call last):
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1323, in _do_call
    return fn(*args)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1302, in _run_fn
    status, run_metadata)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,64,32,32]
	 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](norm2, conv1/weights/read)]]
	 [[Node: Adam/update/_22 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1450_Adam/update", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "evaluater.py", line 489, in <module>
    a = eval.exp(network, 50)
  File "evaluater.py", line 433, in exp
    labels: self.dataset.label[batch_index]})
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 889, in run
    run_metadata_ptr)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1317, in _do_run
    options, run_metadata)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[128,64,32,32]
	 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](norm2, conv1/weights/read)]]
	 [[Node: Adam/update/_22 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1450_Adam/update", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op 'conv1/Conv2D', defined at:
  File "evaluater.py", line 489, in <module>
    a = eval.exp(network, 50)
  File "evaluater.py", line 412, in exp
    train_op, loss, top_k_op, images, labels = self.train(graph_part, cellist)
  File "evaluater.py", line 279, in train
    y = self._inference(images, graph_part, cellist)
  File "evaluater.py", line 194, in _inference
    layer = self._makeconv(inputs[node], cellist[node], node)
  File "evaluater.py", line 105, in _makeconv
    conv = tf.nn.conv2d(inputs, kernel, [1, 1, 1, 1], padding='SAME')
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 631, in conv2d
    data_format=data_format, name=name)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 2956, in create_op
    op_def=op_def)
  File "/home/nas/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,64,32,32]
	 [[Node: conv1/Conv2D = Conv2D[T=DT_FLOAT, data_format="NHWC", padding="SAME", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](norm2, conv1/weights/read)]]
	 [[Node: Adam/update/_22 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1450_Adam/update", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

