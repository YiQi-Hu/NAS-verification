After 19 training steps, loss on training batch is 4.176157
2019-07-04 16:29:27.507101: precision = 0.316
After 38 training steps, loss on training batch is 2.968163
2019-07-04 16:29:29.285148: precision = 0.389
After 57 training steps, loss on training batch is 1.117664
2019-07-04 16:29:31.093274: precision = 0.364
After 76 training steps, loss on training batch is 0.499388
2019-07-04 16:29:32.867475: precision = 0.360
After 95 training steps, loss on training batch is 0.799875
2019-07-04 16:29:34.645530: precision = 0.359
After 114 training steps, loss on training batch is 0.525990
2019-07-04 16:29:36.449107: precision = 0.369
After 133 training steps, loss on training batch is 0.815837
2019-07-04 16:29:38.223551: precision = 0.369
After 152 training steps, loss on training batch is 0.409736
2019-07-04 16:29:40.021237: precision = 0.383
After 171 training steps, loss on training batch is 0.486427
2019-07-04 16:29:41.827964: precision = 0.391
After 190 training steps, loss on training batch is 0.200573
2019-07-04 16:29:43.633325: precision = 0.390
After 209 training steps, loss on training batch is 0.052453
2019-07-04 16:29:45.413527: precision = 0.392
After 228 training steps, loss on training batch is 0.133409
2019-07-04 16:29:47.208383: precision = 0.386
After 247 training steps, loss on training batch is 0.214761
2019-07-04 16:29:48.994812: precision = 0.385
After 266 training steps, loss on training batch is 0.076886
2019-07-04 16:29:50.802528: precision = 0.390
After 285 training steps, loss on training batch is 0.032307
2019-07-04 16:29:52.602183: precision = 0.382
After 304 training steps, loss on training batch is 0.101145
2019-07-04 16:29:54.408741: precision = 0.384
After 323 training steps, loss on training batch is 0.046560
2019-07-04 16:29:56.214051: precision = 0.390
After 342 training steps, loss on training batch is 0.086944
2019-07-04 16:29:58.017516: precision = 0.403
After 361 training steps, loss on training batch is 0.044038
2019-07-04 16:29:59.819876: precision = 0.398
After 380 training steps, loss on training batch is 0.052577
2019-07-04 16:30:01.632778: precision = 0.392
After 399 training steps, loss on training batch is 0.268059
2019-07-04 16:30:03.422498: precision = 0.387
After 418 training steps, loss on training batch is 0.465094
2019-07-04 16:30:05.239724: precision = 0.374
After 437 training steps, loss on training batch is 0.713487
2019-07-04 16:30:07.039216: precision = 0.384
After 456 training steps, loss on training batch is 0.343222
2019-07-04 16:30:08.848725: precision = 0.379
After 475 training steps, loss on training batch is 0.436817
2019-07-04 16:30:10.670051: precision = 0.386
After 494 training steps, loss on training batch is 0.179696
2019-07-04 16:30:12.465910: precision = 0.397
After 513 training steps, loss on training batch is 0.072813
2019-07-04 16:30:14.266562: precision = 0.391
After 532 training steps, loss on training batch is 0.062425
2019-07-04 16:30:16.071908: precision = 0.397
After 551 training steps, loss on training batch is 0.144205
2019-07-04 16:30:17.880717: precision = 0.408
After 570 training steps, loss on training batch is 0.012898
2019-07-04 16:30:19.700156: precision = 0.418
After 589 training steps, loss on training batch is 0.059189
2019-07-04 16:30:21.517182: precision = 0.416
After 608 training steps, loss on training batch is 0.002055
2019-07-04 16:30:23.293407: precision = 0.414
After 627 training steps, loss on training batch is 0.003084
2019-07-04 16:30:25.104028: precision = 0.416
After 646 training steps, loss on training batch is 0.001732
2019-07-04 16:30:26.905623: precision = 0.409
After 665 training steps, loss on training batch is 0.004306
2019-07-04 16:30:28.716513: precision = 0.413
After 684 training steps, loss on training batch is 0.000603
2019-07-04 16:30:30.505379: precision = 0.410
After 703 training steps, loss on training batch is 0.000444
2019-07-04 16:30:32.309821: precision = 0.412
After 722 training steps, loss on training batch is 0.000350
2019-07-04 16:30:34.108338: precision = 0.413
After 741 training steps, loss on training batch is 0.000497
2019-07-04 16:30:35.912744: precision = 0.413
After 760 training steps, loss on training batch is 0.000378
2019-07-04 16:30:37.723446: precision = 0.414
After 779 training steps, loss on training batch is 0.000330
2019-07-04 16:30:39.526635: precision = 0.415
After 798 training steps, loss on training batch is 0.000240
2019-07-04 16:30:41.328520: precision = 0.415
After 817 training steps, loss on training batch is 0.000201
2019-07-04 16:30:43.126944: precision = 0.415
After 836 training steps, loss on training batch is 0.000309
2019-07-04 16:30:44.924150: precision = 0.415
After 855 training steps, loss on training batch is 0.000221
2019-07-04 16:30:46.734211: precision = 0.416
After 874 training steps, loss on training batch is 0.000246
2019-07-04 16:30:48.549321: precision = 0.415
After 893 training steps, loss on training batch is 0.000133
2019-07-04 16:30:50.342012: precision = 0.416
After 912 training steps, loss on training batch is 0.000157
2019-07-04 16:30:52.140605: precision = 0.416
After 931 training steps, loss on training batch is 0.000195
2019-07-04 16:30:53.927312: precision = 0.416
After 58 training steps, loss on training batch is 3.694730
2019-07-04 16:31:08.873406: precision = 0.393
After 116 training steps, loss on training batch is 3.027552
2019-07-04 16:31:12.342641: precision = 0.410
After 174 training steps, loss on training batch is 2.291683
2019-07-04 16:31:15.819903: precision = 0.408
After 232 training steps, loss on training batch is 1.769861
2019-07-04 16:31:19.291255: precision = 0.409
After 290 training steps, loss on training batch is 1.064883
2019-07-04 16:31:22.755209: precision = 0.420
After 348 training steps, loss on training batch is 1.047491
2019-07-04 16:31:26.204425: precision = 0.425
After 406 training steps, loss on training batch is 0.768460
2019-07-04 16:31:29.690667: precision = 0.439
After 464 training steps, loss on training batch is 0.641879
2019-07-04 16:31:33.170220: precision = 0.461
After 522 training steps, loss on training batch is 0.567928
2019-07-04 16:31:36.638621: precision = 0.460
After 580 training steps, loss on training batch is 0.360517
2019-07-04 16:31:40.111471: precision = 0.464
After 638 training steps, loss on training batch is 0.328108
2019-07-04 16:31:43.560233: precision = 0.470
After 696 training steps, loss on training batch is 0.381067
2019-07-04 16:31:47.021123: precision = 0.461
After 754 training steps, loss on training batch is 0.486114
2019-07-04 16:31:50.483235: precision = 0.471
After 812 training steps, loss on training batch is 0.322064
2019-07-04 16:31:53.930460: precision = 0.469
After 870 training steps, loss on training batch is 0.370526
2019-07-04 16:31:57.435644: precision = 0.465
After 928 training steps, loss on training batch is 0.104129
2019-07-04 16:32:00.898535: precision = 0.466
After 986 training steps, loss on training batch is 0.199943
2019-07-04 16:32:04.369670: precision = 0.471
After 1044 training steps, loss on training batch is 0.123260
2019-07-04 16:32:07.845697: precision = 0.467
After 1102 training steps, loss on training batch is 0.152801
2019-07-04 16:32:11.313787: precision = 0.471
After 1160 training steps, loss on training batch is 0.078253
2019-07-04 16:32:14.785111: precision = 0.467
After 1218 training steps, loss on training batch is 0.101076
2019-07-04 16:32:18.255393: precision = 0.471
After 1276 training steps, loss on training batch is 0.074864
2019-07-04 16:32:21.732507: precision = 0.471
After 1334 training steps, loss on training batch is 0.257674
2019-07-04 16:32:25.250154: precision = 0.470
After 1392 training steps, loss on training batch is 0.279956
2019-07-04 16:32:28.703352: precision = 0.476
After 1450 training steps, loss on training batch is 0.193619
2019-07-04 16:32:32.207746: precision = 0.480
After 1508 training steps, loss on training batch is 0.134082
2019-07-04 16:32:35.697730: precision = 0.470
After 1566 training steps, loss on training batch is 0.074017
2019-07-04 16:32:39.204004: precision = 0.474
After 1624 training steps, loss on training batch is 0.118320
2019-07-04 16:32:42.712490: precision = 0.479
After 1682 training steps, loss on training batch is 0.195827
2019-07-04 16:32:46.231964: precision = 0.483
After 1740 training steps, loss on training batch is 0.040521
2019-07-04 16:32:49.735810: precision = 0.487
After 1798 training steps, loss on training batch is 0.108448
2019-07-04 16:32:53.266318: precision = 0.485
After 1856 training steps, loss on training batch is 0.330992
2019-07-04 16:32:56.781854: precision = 0.487
After 1914 training steps, loss on training batch is 0.216034
2019-07-04 16:33:00.326688: precision = 0.481
After 1972 training steps, loss on training batch is 0.100397
2019-07-04 16:33:03.847862: precision = 0.477
After 2030 training steps, loss on training batch is 0.098162
2019-07-04 16:33:07.345248: precision = 0.485
After 2088 training steps, loss on training batch is 0.050989
2019-07-04 16:33:10.845627: precision = 0.481
After 2146 training steps, loss on training batch is 0.117911
2019-07-04 16:33:14.319800: precision = 0.482
After 2204 training steps, loss on training batch is 0.110683
2019-07-04 16:33:17.818913: precision = 0.492
After 2262 training steps, loss on training batch is 0.041185
2019-07-04 16:33:21.312469: precision = 0.489
After 2320 training steps, loss on training batch is 0.167390
2019-07-04 16:33:24.800296: precision = 0.492
After 2378 training steps, loss on training batch is 0.287750
2019-07-04 16:33:28.294271: precision = 0.484
After 2436 training steps, loss on training batch is 0.065369
2019-07-04 16:33:31.803040: precision = 0.500
After 2494 training steps, loss on training batch is 0.100536
2019-07-04 16:33:35.287249: precision = 0.496
After 2552 training steps, loss on training batch is 0.046840
2019-07-04 16:33:38.810589: precision = 0.488
After 2610 training steps, loss on training batch is 0.193923
2019-07-04 16:33:42.298834: precision = 0.488
After 2668 training steps, loss on training batch is 0.174812
2019-07-04 16:33:45.793658: precision = 0.491
After 2726 training steps, loss on training batch is 0.166103
2019-07-04 16:33:49.293912: precision = 0.493
After 2784 training steps, loss on training batch is 0.138570
2019-07-04 16:33:52.788671: precision = 0.488
After 2842 training steps, loss on training batch is 0.072071
2019-07-04 16:33:56.258166: precision = 0.489
After 117 training steps, loss on training batch is 2.819647
2019-07-04 16:34:17.209398: precision = 0.449
After 234 training steps, loss on training batch is 2.050226
2019-07-04 16:34:23.183877: precision = 0.480
After 351 training steps, loss on training batch is 1.565015
2019-07-04 16:34:29.221715: precision = 0.498
After 468 training steps, loss on training batch is 1.028055
2019-07-04 16:34:35.245623: precision = 0.491
After 585 training steps, loss on training batch is 0.826070
2019-07-04 16:34:41.281919: precision = 0.506
After 702 training steps, loss on training batch is 2.816242
2019-07-04 16:34:47.281236: precision = 0.503
After 819 training steps, loss on training batch is 1.145462
2019-07-04 16:34:53.316220: precision = 0.509
After 936 training steps, loss on training batch is 1.029569
2019-07-04 16:34:59.391369: precision = 0.490
After 1053 training steps, loss on training batch is 0.443055
2019-07-04 16:35:05.448636: precision = 0.502
After 1170 training steps, loss on training batch is 0.330571
2019-07-04 16:35:11.496861: precision = 0.518
After 1287 training steps, loss on training batch is 0.194330
2019-07-04 16:35:17.545976: precision = 0.520
After 1404 training steps, loss on training batch is 0.258260
2019-07-04 16:35:23.606520: precision = 0.509
After 1521 training steps, loss on training batch is 0.384397
2019-07-04 16:35:29.696646: precision = 0.531
After 1638 training steps, loss on training batch is 0.281768
2019-07-04 16:35:35.757204: precision = 0.512
After 1755 training steps, loss on training batch is 0.430645
2019-07-04 16:35:41.853003: precision = 0.516
After 1872 training steps, loss on training batch is 0.165129
2019-07-04 16:35:47.866416: precision = 0.532
After 1989 training steps, loss on training batch is 0.193540
2019-07-04 16:35:53.942594: precision = 0.522
After 2106 training steps, loss on training batch is 0.110714
2019-07-04 16:36:00.033112: precision = 0.534
After 2223 training steps, loss on training batch is 0.128773
2019-07-04 16:36:06.097948: precision = 0.535
After 2340 training steps, loss on training batch is 0.220705
2019-07-04 16:36:12.161428: precision = 0.534
After 2457 training steps, loss on training batch is 0.383856
2019-07-04 16:36:18.229374: precision = 0.528
After 2574 training steps, loss on training batch is 0.178805
2019-07-04 16:36:24.333390: precision = 0.531
After 2691 training steps, loss on training batch is 0.102041
2019-07-04 16:36:30.410033: precision = 0.537
After 2808 training steps, loss on training batch is 0.154659
2019-07-04 16:36:36.494656: precision = 0.529
After 2925 training steps, loss on training batch is 0.421735
2019-07-04 16:36:42.566447: precision = 0.526
After 3042 training steps, loss on training batch is 0.178061
2019-07-04 16:36:48.622658: precision = 0.534
After 3159 training steps, loss on training batch is 0.082756
2019-07-04 16:36:54.685222: precision = 0.540
After 3276 training steps, loss on training batch is 0.278547
2019-07-04 16:37:00.775819: precision = 0.536
After 3393 training steps, loss on training batch is 0.042105
2019-07-04 16:37:06.854224: precision = 0.539
After 3510 training steps, loss on training batch is 0.142839
2019-07-04 16:37:12.905004: precision = 0.547
After 3627 training steps, loss on training batch is 0.180769
2019-07-04 16:37:18.969159: precision = 0.544
After 3744 training steps, loss on training batch is 0.075148
2019-07-04 16:37:25.061958: precision = 0.535
After 3861 training steps, loss on training batch is 0.127328
2019-07-04 16:37:31.127424: precision = 0.536
After 3978 training steps, loss on training batch is 0.092311
2019-07-04 16:37:37.201869: precision = 0.535
After 4095 training steps, loss on training batch is 0.027193
2019-07-04 16:37:43.294100: precision = 0.541
After 4212 training steps, loss on training batch is 0.199526
2019-07-04 16:37:49.360379: precision = 0.530
After 4329 training steps, loss on training batch is 0.066926
2019-07-04 16:37:55.418507: precision = 0.531
After 4446 training steps, loss on training batch is 0.061525
2019-07-04 16:38:01.417866: precision = 0.543
After 4563 training steps, loss on training batch is 0.105928
2019-07-04 16:38:07.464899: precision = 0.539
After 4680 training steps, loss on training batch is 0.101690
2019-07-04 16:38:13.501653: precision = 0.538
After 4797 training steps, loss on training batch is 0.131660
2019-07-04 16:38:19.526510: precision = 0.547
After 4914 training steps, loss on training batch is 0.106709
2019-07-04 16:38:25.546070: precision = 0.528
After 5031 training steps, loss on training batch is 0.045872
2019-07-04 16:38:31.594902: precision = 0.553
After 5148 training steps, loss on training batch is 0.029404
2019-07-04 16:38:37.657519: precision = 0.546
After 5265 training steps, loss on training batch is 0.050333
2019-07-04 16:38:43.723255: precision = 0.540
After 5382 training steps, loss on training batch is 0.065888
2019-07-04 16:38:49.827707: precision = 0.544
After 5499 training steps, loss on training batch is 0.088714
2019-07-04 16:38:55.911113: precision = 0.555
After 5616 training steps, loss on training batch is 0.089899
2019-07-04 16:39:01.979881: precision = 0.546
After 5733 training steps, loss on training batch is 0.018607
2019-07-04 16:39:08.097944: precision = 0.545
After 195 training steps, loss on training batch is 2.624737
2019-07-04 16:39:35.021490: precision = 0.482
After 390 training steps, loss on training batch is 2.207406
2019-07-04 16:39:44.370866: precision = 0.524
After 585 training steps, loss on training batch is 1.635552
2019-07-04 16:39:53.738908: precision = 0.528
After 780 training steps, loss on training batch is 2.101380
2019-07-04 16:40:03.103228: precision = 0.553
After 975 training steps, loss on training batch is 1.236434
2019-07-04 16:40:12.484995: precision = 0.553
After 1170 training steps, loss on training batch is 0.803676
2019-07-04 16:40:21.908293: precision = 0.542
After 1365 training steps, loss on training batch is 0.712641
2019-07-04 16:40:31.297860: precision = 0.548
After 1560 training steps, loss on training batch is 0.650749
2019-07-04 16:40:40.682586: precision = 0.549
After 1755 training steps, loss on training batch is 0.511846
2019-07-04 16:40:50.082338: precision = 0.544
After 1950 training steps, loss on training batch is 0.745178
2019-07-04 16:40:59.545367: precision = 0.560
After 2145 training steps, loss on training batch is 0.365532
2019-07-04 16:41:08.975646: precision = 0.562
After 2340 training steps, loss on training batch is 0.242147
2019-07-04 16:41:18.417936: precision = 0.567
After 2535 training steps, loss on training batch is 0.429433
2019-07-04 16:41:27.842172: precision = 0.573
After 2730 training steps, loss on training batch is 0.301618
2019-07-04 16:41:37.275215: precision = 0.573
After 2925 training steps, loss on training batch is 0.424811
2019-07-04 16:41:46.715161: precision = 0.562
After 3120 training steps, loss on training batch is 0.559544
2019-07-04 16:41:56.130756: precision = 0.578
After 3315 training steps, loss on training batch is 0.153144
2019-07-04 16:42:05.549053: precision = 0.572
After 3510 training steps, loss on training batch is 0.156142
2019-07-04 16:42:14.993893: precision = 0.578
After 3705 training steps, loss on training batch is 0.245115
2019-07-04 16:42:24.411156: precision = 0.585
After 3900 training steps, loss on training batch is 0.138076
2019-07-04 16:42:33.851986: precision = 0.587
After 4095 training steps, loss on training batch is 0.375190
2019-07-04 16:42:43.310479: precision = 0.575
After 4290 training steps, loss on training batch is 0.189012
2019-07-04 16:42:52.740235: precision = 0.583
After 4485 training steps, loss on training batch is 0.305596
2019-07-04 16:43:02.187940: precision = 0.587
After 4680 training steps, loss on training batch is 0.228610
2019-07-04 16:43:11.628287: precision = 0.596
After 4875 training steps, loss on training batch is 0.127805
2019-07-04 16:43:21.058671: precision = 0.584
After 5070 training steps, loss on training batch is 0.349545
2019-07-04 16:43:30.479076: precision = 0.592
After 5265 training steps, loss on training batch is 0.330312
2019-07-04 16:43:39.895788: precision = 0.585
After 5460 training steps, loss on training batch is 0.157461
2019-07-04 16:43:49.325031: precision = 0.588
After 5655 training steps, loss on training batch is 0.165668
2019-07-04 16:43:58.745199: precision = 0.588
After 5850 training steps, loss on training batch is 0.237925
2019-07-04 16:44:08.196628: precision = 0.577
After 6045 training steps, loss on training batch is 0.116718
2019-07-04 16:44:17.600926: precision = 0.589
After 6240 training steps, loss on training batch is 0.196303
2019-07-04 16:44:26.984242: precision = 0.588
After 6435 training steps, loss on training batch is 0.050088
2019-07-04 16:44:36.395875: precision = 0.589
After 6630 training steps, loss on training batch is 0.191420
2019-07-04 16:44:45.815961: precision = 0.587
After 6825 training steps, loss on training batch is 0.192229
2019-07-04 16:44:55.216319: precision = 0.588
After 7020 training steps, loss on training batch is 0.060256
2019-07-04 16:45:04.657631: precision = 0.581
After 7215 training steps, loss on training batch is 0.393495
2019-07-04 16:45:14.094317: precision = 0.592
After 7410 training steps, loss on training batch is 0.050117
2019-07-04 16:45:23.505937: precision = 0.589
After 7605 training steps, loss on training batch is 0.175209
2019-07-04 16:45:32.894743: precision = 0.589
After 7800 training steps, loss on training batch is 0.093984
2019-07-04 16:45:42.287274: precision = 0.588
After 7995 training steps, loss on training batch is 0.152490
2019-07-04 16:45:51.704490: precision = 0.587
After 8190 training steps, loss on training batch is 0.011312
2019-07-04 16:46:01.176830: precision = 0.595
After 8385 training steps, loss on training batch is 0.177830
2019-07-04 16:46:10.585982: precision = 0.592
After 8580 training steps, loss on training batch is 0.174015
2019-07-04 16:46:19.995351: precision = 0.585
After 8775 training steps, loss on training batch is 0.094511
2019-07-04 16:46:29.413101: precision = 0.585
After 8970 training steps, loss on training batch is 0.043312
2019-07-04 16:46:38.818706: precision = 0.605
After 9165 training steps, loss on training batch is 0.005040
2019-07-04 16:46:48.233007: precision = 0.604
After 9360 training steps, loss on training batch is 0.029094
2019-07-04 16:46:57.650278: precision = 0.593
After 9555 training steps, loss on training batch is 0.023252
2019-07-04 16:47:07.088192: precision = 0.595
After 292 training steps, loss on training batch is 3.097195
2019-07-04 16:47:40.177663: precision = 0.512
After 584 training steps, loss on training batch is 2.352509
2019-07-04 16:47:53.613705: precision = 0.555
After 876 training steps, loss on training batch is 2.144961
2019-07-04 16:48:07.105227: precision = 0.574
After 1168 training steps, loss on training batch is 1.417533
2019-07-04 16:48:20.644503: precision = 0.571
After 1460 training steps, loss on training batch is 1.743350
2019-07-04 16:48:34.194779: precision = 0.573
After 1752 training steps, loss on training batch is 1.254694
2019-07-04 16:48:47.768375: precision = 0.571
After 2044 training steps, loss on training batch is 0.914385
2019-07-04 16:49:01.388499: precision = 0.600
After 2336 training steps, loss on training batch is 0.796586
2019-07-04 16:49:14.943595: precision = 0.579
After 2628 training steps, loss on training batch is 0.518616
2019-07-04 16:49:28.509068: precision = 0.591
After 2920 training steps, loss on training batch is 0.364194
2019-07-04 16:49:42.049018: precision = 0.594
After 3212 training steps, loss on training batch is 0.390600
2019-07-04 16:49:55.552251: precision = 0.609
After 3504 training steps, loss on training batch is 0.646984
2019-07-04 16:50:09.107651: precision = 0.611
After 3796 training steps, loss on training batch is 0.554188
2019-07-04 16:50:22.651083: precision = 0.614
After 4088 training steps, loss on training batch is 0.209285
2019-07-04 16:50:36.220432: precision = 0.617
After 4380 training steps, loss on training batch is 0.103946
2019-07-04 16:50:49.752497: precision = 0.614
After 4672 training steps, loss on training batch is 0.333617
2019-07-04 16:51:03.278535: precision = 0.610
After 4964 training steps, loss on training batch is 0.318948
2019-07-04 16:51:16.813277: precision = 0.620
After 5256 training steps, loss on training batch is 0.211130
2019-07-04 16:51:30.335920: precision = 0.616
After 5548 training steps, loss on training batch is 0.229885
2019-07-04 16:51:43.851155: precision = 0.620
After 5840 training steps, loss on training batch is 0.046437
2019-07-04 16:51:57.382789: precision = 0.624
After 6132 training steps, loss on training batch is 0.233558
2019-07-04 16:52:10.914327: precision = 0.625
After 6424 training steps, loss on training batch is 0.189215
2019-07-04 16:52:24.458379: precision = 0.630
After 6716 training steps, loss on training batch is 0.144437
2019-07-04 16:52:38.009547: precision = 0.627
After 7008 training steps, loss on training batch is 0.331345
2019-07-04 16:52:51.585672: precision = 0.622
After 7300 training steps, loss on training batch is 0.303995
2019-07-04 16:53:05.153234: precision = 0.629
After 7592 training steps, loss on training batch is 0.198866
2019-07-04 16:53:18.735480: precision = 0.622
After 7884 training steps, loss on training batch is 0.311042
2019-07-04 16:53:32.331746: precision = 0.628
After 8176 training steps, loss on training batch is 0.228254
2019-07-04 16:53:45.953619: precision = 0.625
After 8468 training steps, loss on training batch is 0.124846
2019-07-04 16:53:59.563671: precision = 0.633
After 8760 training steps, loss on training batch is 0.062845
2019-07-04 16:54:13.186027: precision = 0.629
After 9052 training steps, loss on training batch is 0.214104
2019-07-04 16:54:26.762335: precision = 0.625
After 9344 training steps, loss on training batch is 0.186495
2019-07-04 16:54:40.348439: precision = 0.630
After 9636 training steps, loss on training batch is 0.192680
2019-07-04 16:54:53.937802: precision = 0.627
After 9928 training steps, loss on training batch is 0.167831
2019-07-04 16:55:07.524179: precision = 0.630
After 10220 training steps, loss on training batch is 0.140902
2019-07-04 16:55:21.177274: precision = 0.631
After 10512 training steps, loss on training batch is 0.049978
2019-07-04 16:55:34.754991: precision = 0.633
After 10804 training steps, loss on training batch is 0.076677
2019-07-04 16:55:48.326633: precision = 0.622
After 11096 training steps, loss on training batch is 0.140662
2019-07-04 16:56:01.936020: precision = 0.627
After 11388 training steps, loss on training batch is 0.045130
2019-07-04 16:56:15.517450: precision = 0.630
After 11680 training steps, loss on training batch is 0.010160
2019-07-04 16:56:29.091863: precision = 0.631
After 11972 training steps, loss on training batch is 0.050965
2019-07-04 16:56:42.701714: precision = 0.636
After 12264 training steps, loss on training batch is 0.047886
2019-07-04 16:56:56.307648: precision = 0.638
After 12556 training steps, loss on training batch is 0.091854
2019-07-04 16:57:09.890017: precision = 0.633
After 12848 training steps, loss on training batch is 0.025712
2019-07-04 16:57:23.439065: precision = 0.637
After 13140 training steps, loss on training batch is 0.039990
2019-07-04 16:57:37.068692: precision = 0.630
After 13432 training steps, loss on training batch is 0.101101
2019-07-04 16:57:50.704588: precision = 0.635
After 13724 training steps, loss on training batch is 0.148669
2019-07-04 16:58:04.345697: precision = 0.628
After 14016 training steps, loss on training batch is 0.041878
2019-07-04 16:58:17.967474: precision = 0.637
After 14308 training steps, loss on training batch is 0.074433
2019-07-04 16:58:31.538614: precision = 0.635
After 390 training steps, loss on training batch is 2.809323
2019-07-04 16:59:10.055918: precision = 0.534
After 780 training steps, loss on training batch is 2.757874
2019-07-04 16:59:27.883603: precision = 0.584
After 1170 training steps, loss on training batch is 1.717652
2019-07-04 16:59:45.703119: precision = 0.595
After 1560 training steps, loss on training batch is 1.272751
2019-07-04 17:00:03.531446: precision = 0.594
After 1950 training steps, loss on training batch is 1.376882
2019-07-04 17:00:21.373293: precision = 0.614
After 2340 training steps, loss on training batch is 1.152063
2019-07-04 17:00:39.169165: precision = 0.619
After 2730 training steps, loss on training batch is 0.733365
2019-07-04 17:00:56.930290: precision = 0.615
After 3120 training steps, loss on training batch is 0.872540
2019-07-04 17:01:14.832039: precision = 0.621
After 3510 training steps, loss on training batch is 0.832916
2019-07-04 17:01:32.670099: precision = 0.623
After 3900 training steps, loss on training batch is 0.361052
2019-07-04 17:01:50.523423: precision = 0.634
After 4290 training steps, loss on training batch is 0.454577
2019-07-04 17:02:08.443547: precision = 0.638
After 4680 training steps, loss on training batch is 0.342082
2019-07-04 17:02:26.344148: precision = 0.643
After 5070 training steps, loss on training batch is 0.457936
2019-07-04 17:02:44.254700: precision = 0.641
After 5460 training steps, loss on training batch is 0.350083
2019-07-04 17:03:02.124275: precision = 0.652
After 5850 training steps, loss on training batch is 0.176223
2019-07-04 17:03:19.976328: precision = 0.647
After 6240 training steps, loss on training batch is 0.142975
2019-07-04 17:03:37.769598: precision = 0.651
After 6630 training steps, loss on training batch is 0.220487
2019-07-04 17:03:55.611960: precision = 0.654
After 7020 training steps, loss on training batch is 0.277453
2019-07-04 17:04:13.466185: precision = 0.651
After 7410 training steps, loss on training batch is 0.131247
2019-07-04 17:04:31.280849: precision = 0.649
After 7800 training steps, loss on training batch is 0.336743
2019-07-04 17:04:49.177103: precision = 0.649
After 8190 training steps, loss on training batch is 0.099837
2019-07-04 17:05:07.033550: precision = 0.670
After 8580 training steps, loss on training batch is 0.048840
2019-07-04 17:05:24.915723: precision = 0.659
After 8970 training steps, loss on training batch is 0.195419
2019-07-04 17:05:42.830342: precision = 0.661
After 9360 training steps, loss on training batch is 0.131018
2019-07-04 17:06:00.686942: precision = 0.660
After 9750 training steps, loss on training batch is 0.086793
2019-07-04 17:06:18.603554: precision = 0.656
After 10140 training steps, loss on training batch is 0.089869
2019-07-04 17:06:36.471959: precision = 0.663
After 10530 training steps, loss on training batch is 0.249439
2019-07-04 17:06:54.313213: precision = 0.656
After 10920 training steps, loss on training batch is 0.088683
2019-07-04 17:07:12.183283: precision = 0.660
After 11310 training steps, loss on training batch is 0.187215
2019-07-04 17:07:30.037659: precision = 0.659
After 11700 training steps, loss on training batch is 0.062902
2019-07-04 17:07:47.885885: precision = 0.655
After 12090 training steps, loss on training batch is 0.105001
2019-07-04 17:08:05.763980: precision = 0.660
After 12480 training steps, loss on training batch is 0.071388
2019-07-04 17:08:23.695693: precision = 0.658
After 12870 training steps, loss on training batch is 0.153981
2019-07-04 17:08:41.607822: precision = 0.665
After 13260 training steps, loss on training batch is 0.107739
2019-07-04 17:08:59.475056: precision = 0.667
After 13650 training steps, loss on training batch is 0.024692
2019-07-04 17:09:17.304783: precision = 0.660
After 14040 training steps, loss on training batch is 0.058471
2019-07-04 17:09:35.150445: precision = 0.664
After 14430 training steps, loss on training batch is 0.102909
2019-07-04 17:09:53.023417: precision = 0.668
After 14820 training steps, loss on training batch is 0.116193
2019-07-04 17:10:10.949694: precision = 0.660
After 15210 training steps, loss on training batch is 0.054429
2019-07-04 17:10:28.856536: precision = 0.668
After 15600 training steps, loss on training batch is 0.114265
2019-07-04 17:10:46.745595: precision = 0.660
After 15990 training steps, loss on training batch is 0.114920
2019-07-04 17:11:04.612328: precision = 0.661
After 16380 training steps, loss on training batch is 0.080808
2019-07-04 17:11:22.526439: precision = 0.669
After 16770 training steps, loss on training batch is 0.076102
2019-07-04 17:11:40.520689: precision = 0.671
After 17160 training steps, loss on training batch is 0.048435
2019-07-04 17:11:58.467609: precision = 0.661
After 17550 training steps, loss on training batch is 0.150439
2019-07-04 17:12:16.398505: precision = 0.661
After 17940 training steps, loss on training batch is 0.019962
2019-07-04 17:12:34.257030: precision = 0.668
After 18330 training steps, loss on training batch is 0.119591
2019-07-04 17:12:52.110516: precision = 0.665
After 18720 training steps, loss on training batch is 0.091305
2019-07-04 17:13:10.016737: precision = 0.665
After 19110 training steps, loss on training batch is 0.021962
2019-07-04 17:13:27.865339: precision = 0.668
After 390 training steps, loss on training batch is 2.995653
2019-07-04 17:14:08.183233: precision = 0.525
After 780 training steps, loss on training batch is 2.639246
2019-07-04 17:14:26.026845: precision = 0.582
After 1170 training steps, loss on training batch is 1.974634
2019-07-04 17:14:43.846324: precision = 0.593
After 1560 training steps, loss on training batch is 1.300885
2019-07-04 17:15:01.705413: precision = 0.612
After 1950 training steps, loss on training batch is 1.194071
2019-07-04 17:15:19.602047: precision = 0.606
After 2340 training steps, loss on training batch is 1.478152
2019-07-04 17:15:37.452907: precision = 0.613
After 2730 training steps, loss on training batch is 0.878293
2019-07-04 17:15:55.315309: precision = 0.618
After 3120 training steps, loss on training batch is 1.038828
2019-07-04 17:16:13.219469: precision = 0.625
After 3510 training steps, loss on training batch is 0.958921
2019-07-04 17:16:31.179884: precision = 0.622
After 3900 training steps, loss on training batch is 0.609323
2019-07-04 17:16:49.116016: precision = 0.626
After 4290 training steps, loss on training batch is 0.260710
2019-07-04 17:17:07.074766: precision = 0.631
After 4680 training steps, loss on training batch is 0.250604
2019-07-04 17:17:24.946978: precision = 0.636
After 5070 training steps, loss on training batch is 0.285283
2019-07-04 17:17:42.819918: precision = 0.640
After 5460 training steps, loss on training batch is 0.342483
2019-07-04 17:18:00.699962: precision = 0.643
After 5850 training steps, loss on training batch is 0.375675
2019-07-04 17:18:18.528375: precision = 0.643
After 6240 training steps, loss on training batch is 0.121539
2019-07-04 17:18:36.413053: precision = 0.652
After 6630 training steps, loss on training batch is 0.250175
2019-07-04 17:18:54.345909: precision = 0.639
After 7020 training steps, loss on training batch is 0.229399
2019-07-04 17:19:12.269933: precision = 0.650
After 7410 training steps, loss on training batch is 0.136850
2019-07-04 17:19:30.212186: precision = 0.650
After 7800 training steps, loss on training batch is 0.048185
2019-07-04 17:19:48.151094: precision = 0.653
After 8190 training steps, loss on training batch is 0.164820
2019-07-04 17:20:06.064642: precision = 0.642
After 8580 training steps, loss on training batch is 0.135581
2019-07-04 17:20:23.979572: precision = 0.658
After 8970 training steps, loss on training batch is 0.098762
2019-07-04 17:20:41.842263: precision = 0.656
After 9360 training steps, loss on training batch is 0.261740
2019-07-04 17:20:59.687995: precision = 0.656
After 9750 training steps, loss on training batch is 0.261345
2019-07-04 17:21:17.581537: precision = 0.655
After 10140 training steps, loss on training batch is 0.074531
2019-07-04 17:21:35.448106: precision = 0.655
After 10530 training steps, loss on training batch is 0.127365
2019-07-04 17:21:53.376974: precision = 0.659
After 10920 training steps, loss on training batch is 0.226352
2019-07-04 17:22:11.299874: precision = 0.662
After 11310 training steps, loss on training batch is 0.142284
2019-07-04 17:22:29.282778: precision = 0.660
After 11700 training steps, loss on training batch is 0.109799
2019-07-04 17:22:47.205616: precision = 0.657
After 12090 training steps, loss on training batch is 0.081385
2019-07-04 17:23:05.028554: precision = 0.663
After 12480 training steps, loss on training batch is 0.038248
2019-07-04 17:23:22.903292: precision = 0.663
After 12870 training steps, loss on training batch is 0.330006
2019-07-04 17:23:40.821112: precision = 0.661
After 13260 training steps, loss on training batch is 0.036986
2019-07-04 17:23:58.741836: precision = 0.673
After 13650 training steps, loss on training batch is 0.085958
2019-07-04 17:24:16.674282: precision = 0.667
After 14040 training steps, loss on training batch is 0.071679
2019-07-04 17:24:34.549201: precision = 0.664
After 14430 training steps, loss on training batch is 0.257841
2019-07-04 17:24:52.437435: precision = 0.657
After 14820 training steps, loss on training batch is 0.031878
2019-07-04 17:25:10.328536: precision = 0.665
After 15210 training steps, loss on training batch is 0.068073
2019-07-04 17:25:28.219455: precision = 0.663
After 15600 training steps, loss on training batch is 0.257210
2019-07-04 17:25:46.119780: precision = 0.663
After 15990 training steps, loss on training batch is 0.059198
2019-07-04 17:26:03.945734: precision = 0.661
After 16380 training steps, loss on training batch is 0.066568
2019-07-04 17:26:21.766345: precision = 0.668
After 16770 training steps, loss on training batch is 0.109146
2019-07-04 17:26:39.667420: precision = 0.655
After 17160 training steps, loss on training batch is 0.086027
2019-07-04 17:26:57.494754: precision = 0.670
After 17550 training steps, loss on training batch is 0.080180
2019-07-04 17:27:15.344454: precision = 0.670
After 17940 training steps, loss on training batch is 0.084817
2019-07-04 17:27:33.229127: precision = 0.669
After 18330 training steps, loss on training batch is 0.050268
2019-07-04 17:27:51.138423: precision = 0.668
After 18720 training steps, loss on training batch is 0.019054
2019-07-04 17:28:09.069535: precision = 0.672
After 19110 training steps, loss on training batch is 0.018057
2019-07-04 17:28:26.933637: precision = 0.662
After 390 training steps, loss on training batch is 2.843466
2019-07-04 17:29:07.376726: precision = 0.541
After 780 training steps, loss on training batch is 2.603049
2019-07-04 17:29:25.216717: precision = 0.590
After 1170 training steps, loss on training batch is 1.800927
2019-07-04 17:29:42.982808: precision = 0.601
After 1560 training steps, loss on training batch is 1.142782
2019-07-04 17:30:00.767938: precision = 0.605
After 1950 training steps, loss on training batch is 1.176859
2019-07-04 17:30:18.565960: precision = 0.609
After 2340 training steps, loss on training batch is 1.490997
2019-07-04 17:30:36.404661: precision = 0.609
After 2730 training steps, loss on training batch is 0.841937
2019-07-04 17:30:54.273884: precision = 0.616
After 3120 training steps, loss on training batch is 0.859040
2019-07-04 17:31:12.041046: precision = 0.613
After 3510 training steps, loss on training batch is 0.790922
2019-07-04 17:31:29.729721: precision = 0.598
After 3900 training steps, loss on training batch is 0.646125
2019-07-04 17:31:47.493299: precision = 0.624
After 4290 training steps, loss on training batch is 0.332889
2019-07-04 17:32:05.248569: precision = 0.637
After 4680 training steps, loss on training batch is 0.238623
2019-07-04 17:32:23.049647: precision = 0.635
After 5070 training steps, loss on training batch is 0.464663
2019-07-04 17:32:40.909880: precision = 0.639
After 5460 training steps, loss on training batch is 0.399954
2019-07-04 17:32:58.748798: precision = 0.642
After 5850 training steps, loss on training batch is 0.179308
2019-07-04 17:33:16.498371: precision = 0.649
After 6240 training steps, loss on training batch is 0.253178
2019-07-04 17:33:34.277453: precision = 0.649
After 6630 training steps, loss on training batch is 0.370072
2019-07-04 17:33:52.063852: precision = 0.655
After 7020 training steps, loss on training batch is 0.224046
2019-07-04 17:34:09.797454: precision = 0.653
After 7410 training steps, loss on training batch is 0.070714
2019-07-04 17:34:27.545965: precision = 0.653
After 7800 training steps, loss on training batch is 0.253832
2019-07-04 17:34:45.326568: precision = 0.651
After 8190 training steps, loss on training batch is 0.176164
2019-07-04 17:35:03.648377: precision = 0.655
After 8580 training steps, loss on training batch is 0.054981
2019-07-04 17:35:22.297372: precision = 0.654
After 8970 training steps, loss on training batch is 0.107757
2019-07-04 17:35:40.676327: precision = 0.648
After 9360 training steps, loss on training batch is 0.096340
2019-07-04 17:35:58.987927: precision = 0.655
After 9750 training steps, loss on training batch is 0.139200
2019-07-04 17:36:17.170153: precision = 0.662
After 10140 training steps, loss on training batch is 0.090506
2019-07-04 17:36:35.437769: precision = 0.651
After 10530 training steps, loss on training batch is 0.090491
2019-07-04 17:36:53.649593: precision = 0.655
After 10920 training steps, loss on training batch is 0.210824
2019-07-04 17:37:11.752542: precision = 0.650
After 11310 training steps, loss on training batch is 0.139206
2019-07-04 17:37:30.107128: precision = 0.659
After 11700 training steps, loss on training batch is 0.061575
2019-07-04 17:37:48.412767: precision = 0.663
After 12090 training steps, loss on training batch is 0.095755
2019-07-04 17:38:07.881170: precision = 0.661
After 12480 training steps, loss on training batch is 0.164725
2019-07-04 17:38:27.478630: precision = 0.660
After 12870 training steps, loss on training batch is 0.214918
2019-07-04 17:38:47.133175: precision = 0.656
After 13260 training steps, loss on training batch is 0.097833
2019-07-04 17:39:06.768836: precision = 0.664
After 13650 training steps, loss on training batch is 0.034993
2019-07-04 17:39:26.401846: precision = 0.659
After 14040 training steps, loss on training batch is 0.075678
2019-07-04 17:39:46.067462: precision = 0.666
After 14430 training steps, loss on training batch is 0.039962
2019-07-04 17:40:05.709461: precision = 0.665
After 14820 training steps, loss on training batch is 0.009310
2019-07-04 17:40:25.406544: precision = 0.657
After 15210 training steps, loss on training batch is 0.024271
2019-07-04 17:40:45.045945: precision = 0.663
After 15600 training steps, loss on training batch is 0.132342
2019-07-04 17:41:04.666408: precision = 0.665
After 15990 training steps, loss on training batch is 0.039514
2019-07-04 17:41:24.351651: precision = 0.660
After 16380 training steps, loss on training batch is 0.018227
2019-07-04 17:41:43.984155: precision = 0.664
After 16770 training steps, loss on training batch is 0.067550
2019-07-04 17:42:03.532262: precision = 0.662
After 17160 training steps, loss on training batch is 0.097989
2019-07-04 17:42:23.107972: precision = 0.667
After 17550 training steps, loss on training batch is 0.022086
2019-07-04 17:42:42.729513: precision = 0.662
After 17940 training steps, loss on training batch is 0.057507
2019-07-04 17:43:02.298216: precision = 0.664
After 18330 training steps, loss on training batch is 0.072645
2019-07-04 17:43:21.898088: precision = 0.670
After 18720 training steps, loss on training batch is 0.046447
2019-07-04 17:43:41.597445: precision = 0.660
After 19110 training steps, loss on training batch is 0.154382
2019-07-04 17:44:01.283256: precision = 0.661
After 390 training steps, loss on training batch is 2.629793
2019-07-04 17:44:44.152490: precision = 0.526
After 780 training steps, loss on training batch is 2.701502
2019-07-04 17:45:03.762638: precision = 0.579
After 1170 training steps, loss on training batch is 1.914408
2019-07-04 17:45:23.329792: precision = 0.592
After 1560 training steps, loss on training batch is 1.220309
2019-07-04 17:45:42.882460: precision = 0.609
After 1950 training steps, loss on training batch is 1.087256
2019-07-04 17:46:02.465511: precision = 0.613
After 2340 training steps, loss on training batch is 1.090162
2019-07-04 17:46:21.994610: precision = 0.619
After 2730 training steps, loss on training batch is 0.959337
2019-07-04 17:46:41.620608: precision = 0.622
After 3120 training steps, loss on training batch is 0.741744
2019-07-04 17:47:01.202105: precision = 0.635
After 3510 training steps, loss on training batch is 0.757264
2019-07-04 17:47:20.836339: precision = 0.631
After 3900 training steps, loss on training batch is 0.547443
2019-07-04 17:47:40.452877: precision = 0.619
After 4290 training steps, loss on training batch is 0.350809
2019-07-04 17:48:00.090119: precision = 0.627
After 4680 training steps, loss on training batch is 0.280595
2019-07-04 17:48:19.744738: precision = 0.635
After 5070 training steps, loss on training batch is 0.439560
2019-07-04 17:48:39.458625: precision = 0.642
After 5460 training steps, loss on training batch is 0.447130
2019-07-04 17:48:59.147324: precision = 0.641
After 5850 training steps, loss on training batch is 0.306549
2019-07-04 17:49:18.790498: precision = 0.639
After 6240 training steps, loss on training batch is 0.132138
2019-07-04 17:49:38.303475: precision = 0.646
After 6630 training steps, loss on training batch is 0.133337
2019-07-04 17:49:57.962467: precision = 0.645
After 7020 training steps, loss on training batch is 0.372569
2019-07-04 17:50:17.563069: precision = 0.656
After 7410 training steps, loss on training batch is 0.161547
2019-07-04 17:50:37.146424: precision = 0.647
After 7800 training steps, loss on training batch is 0.113264
2019-07-04 17:50:56.794657: precision = 0.652
After 8190 training steps, loss on training batch is 0.214495
2019-07-04 17:51:16.397055: precision = 0.648
After 8580 training steps, loss on training batch is 0.160530
2019-07-04 17:51:36.026192: precision = 0.648
After 8970 training steps, loss on training batch is 0.171874
2019-07-04 17:51:55.695721: precision = 0.649
After 9360 training steps, loss on training batch is 0.195435
2019-07-04 17:52:15.331005: precision = 0.651
After 9750 training steps, loss on training batch is 0.104753
2019-07-04 17:52:34.957932: precision = 0.649
After 10140 training steps, loss on training batch is 0.162787
2019-07-04 17:52:54.539373: precision = 0.657
After 10530 training steps, loss on training batch is 0.262725
2019-07-04 17:53:14.191598: precision = 0.659
After 10920 training steps, loss on training batch is 0.171734
2019-07-04 17:53:33.809554: precision = 0.650
After 11310 training steps, loss on training batch is 0.335690
2019-07-04 17:53:53.487959: precision = 0.656
After 11700 training steps, loss on training batch is 0.110493
2019-07-04 17:54:13.014335: precision = 0.653
After 12090 training steps, loss on training batch is 0.214740
2019-07-04 17:54:32.611921: precision = 0.660
After 12480 training steps, loss on training batch is 0.152057
2019-07-04 17:54:52.197280: precision = 0.659
After 12870 training steps, loss on training batch is 0.209769
2019-07-04 17:55:11.794509: precision = 0.657
After 13260 training steps, loss on training batch is 0.071342
2019-07-04 17:55:31.395624: precision = 0.658
After 13650 training steps, loss on training batch is 0.079850
2019-07-04 17:55:50.970836: precision = 0.660
After 14040 training steps, loss on training batch is 0.069031
2019-07-04 17:56:10.582395: precision = 0.662
After 14430 training steps, loss on training batch is 0.019254
2019-07-04 17:56:30.127727: precision = 0.664
After 14820 training steps, loss on training batch is 0.119014
2019-07-04 17:56:49.743436: precision = 0.649
After 15210 training steps, loss on training batch is 0.099757
2019-07-04 17:57:09.368095: precision = 0.666
After 15600 training steps, loss on training batch is 0.055748
2019-07-04 17:57:28.879188: precision = 0.664
After 15990 training steps, loss on training batch is 0.318303
2019-07-04 17:57:48.520649: precision = 0.661
After 16380 training steps, loss on training batch is 0.045953
2019-07-04 17:58:08.156753: precision = 0.663
After 16770 training steps, loss on training batch is 0.128972
2019-07-04 17:58:27.774256: precision = 0.657
After 17160 training steps, loss on training batch is 0.031391
2019-07-04 17:58:47.333655: precision = 0.655
After 17550 training steps, loss on training batch is 0.068863
2019-07-04 17:59:06.901539: precision = 0.656
After 17940 training steps, loss on training batch is 0.036927
2019-07-04 17:59:26.466481: precision = 0.665
After 18330 training steps, loss on training batch is 0.006070
2019-07-04 17:59:46.087480: precision = 0.664
After 18720 training steps, loss on training batch is 0.006921
2019-07-04 18:00:05.668301: precision = 0.659
After 19110 training steps, loss on training batch is 0.019764
2019-07-04 18:00:25.226626: precision = 0.661
After 390 training steps, loss on training batch is 2.870082
2019-07-04 18:01:07.967954: precision = 0.537
After 780 training steps, loss on training batch is 2.717228
2019-07-04 18:01:27.710168: precision = 0.575
After 1170 training steps, loss on training batch is 1.737275
2019-07-04 18:01:47.458262: precision = 0.603
After 1560 training steps, loss on training batch is 1.137779
2019-07-04 18:02:07.216466: precision = 0.606
After 1950 training steps, loss on training batch is 1.299605
2019-07-04 18:02:26.980871: precision = 0.602
After 2340 training steps, loss on training batch is 1.537925
2019-07-04 18:02:46.735427: precision = 0.611
After 2730 training steps, loss on training batch is 0.701382
2019-07-04 18:03:06.511490: precision = 0.608
After 3120 training steps, loss on training batch is 0.807482
2019-07-04 18:03:26.279434: precision = 0.603
After 3510 training steps, loss on training batch is 0.780108
2019-07-04 18:03:46.033402: precision = 0.618
After 3900 training steps, loss on training batch is 0.341034
2019-07-04 18:04:05.775055: precision = 0.614
After 4290 training steps, loss on training batch is 0.220754
2019-07-04 18:04:25.500118: precision = 0.636
After 4680 training steps, loss on training batch is 0.240113
2019-07-04 18:04:45.228591: precision = 0.641
After 5070 training steps, loss on training batch is 0.402191
2019-07-04 18:05:04.940921: precision = 0.643
After 5460 training steps, loss on training batch is 0.204151
2019-07-04 18:05:24.681040: precision = 0.640
After 5850 training steps, loss on training batch is 0.401744
2019-07-04 18:05:44.404468: precision = 0.645
After 6240 training steps, loss on training batch is 0.102604
2019-07-04 18:06:04.149242: precision = 0.642
After 6630 training steps, loss on training batch is 0.246850
2019-07-04 18:06:23.842075: precision = 0.645
After 7020 training steps, loss on training batch is 0.303287
2019-07-04 18:06:43.580196: precision = 0.658
After 7410 training steps, loss on training batch is 0.212718
2019-07-04 18:07:03.316594: precision = 0.652
After 7800 training steps, loss on training batch is 0.401040
2019-07-04 18:07:23.061267: precision = 0.643
After 8190 training steps, loss on training batch is 0.149274
2019-07-04 18:07:42.765179: precision = 0.653
After 8580 training steps, loss on training batch is 0.107587
2019-07-04 18:08:02.470265: precision = 0.650
After 8970 training steps, loss on training batch is 0.148535
2019-07-04 18:08:22.245169: precision = 0.649
After 9360 training steps, loss on training batch is 0.063523
2019-07-04 18:08:42.007282: precision = 0.654
After 9750 training steps, loss on training batch is 0.063503
2019-07-04 18:09:01.675138: precision = 0.662
After 10140 training steps, loss on training batch is 0.183454
2019-07-04 18:09:21.453973: precision = 0.650
After 10530 training steps, loss on training batch is 0.039603
2019-07-04 18:09:41.186263: precision = 0.656
After 10920 training steps, loss on training batch is 0.121939
2019-07-04 18:10:00.946788: precision = 0.654
After 11310 training steps, loss on training batch is 0.163346
2019-07-04 18:10:20.637208: precision = 0.657
After 11700 training steps, loss on training batch is 0.081752
2019-07-04 18:10:40.376439: precision = 0.658
After 12090 training steps, loss on training batch is 0.237808
2019-07-04 18:11:00.119627: precision = 0.664
After 12480 training steps, loss on training batch is 0.119239
2019-07-04 18:11:19.722899: precision = 0.660
After 12870 training steps, loss on training batch is 0.071236
2019-07-04 18:11:39.422769: precision = 0.664
After 13260 training steps, loss on training batch is 0.112567
2019-07-04 18:11:59.179854: precision = 0.665
After 13650 training steps, loss on training batch is 0.100898
2019-07-04 18:12:18.911708: precision = 0.662
After 14040 training steps, loss on training batch is 0.190353
2019-07-04 18:12:38.676876: precision = 0.654
After 14430 training steps, loss on training batch is 0.175743
2019-07-04 18:12:58.400035: precision = 0.659
After 14820 training steps, loss on training batch is 0.131663
2019-07-04 18:13:18.182656: precision = 0.663
After 15210 training steps, loss on training batch is 0.124039
2019-07-04 18:13:37.969751: precision = 0.665
After 15600 training steps, loss on training batch is 0.057694
2019-07-04 18:13:57.723541: precision = 0.662
After 15990 training steps, loss on training batch is 0.136233
2019-07-04 18:14:17.460692: precision = 0.661
After 16380 training steps, loss on training batch is 0.046033
2019-07-04 18:14:37.180015: precision = 0.656
After 16770 training steps, loss on training batch is 0.067800
2019-07-04 18:14:56.961367: precision = 0.661
After 17160 training steps, loss on training batch is 0.112464
2019-07-04 18:15:16.713063: precision = 0.654
After 17550 training steps, loss on training batch is 0.028782
2019-07-04 18:15:36.434228: precision = 0.665
After 17940 training steps, loss on training batch is 0.078482
2019-07-04 18:15:56.140230: precision = 0.660
After 18330 training steps, loss on training batch is 0.049255
2019-07-04 18:16:15.884428: precision = 0.672
After 18720 training steps, loss on training batch is 0.010831
2019-07-04 18:16:35.639104: precision = 0.665
After 19110 training steps, loss on training batch is 0.132062
2019-07-04 18:16:55.257689: precision = 0.664
After 390 training steps, loss on training batch is 2.764071
2019-07-04 18:17:37.884777: precision = 0.537
After 780 training steps, loss on training batch is 2.630492
2019-07-04 18:17:57.473816: precision = 0.586
After 1170 training steps, loss on training batch is 1.947180
2019-07-04 18:18:16.995284: precision = 0.590
After 1560 training steps, loss on training batch is 1.352898
2019-07-04 18:18:36.557964: precision = 0.595
After 1950 training steps, loss on training batch is 1.124181
2019-07-04 18:18:56.143809: precision = 0.605
After 2340 training steps, loss on training batch is 1.137074
2019-07-04 18:19:15.743479: precision = 0.615
After 2730 training steps, loss on training batch is 0.773939
2019-07-04 18:19:35.336553: precision = 0.615
After 3120 training steps, loss on training batch is 1.016285
2019-07-04 18:19:54.964386: precision = 0.614
After 3510 training steps, loss on training batch is 0.990827
2019-07-04 18:20:14.542509: precision = 0.617
After 3900 training steps, loss on training batch is 0.385958
2019-07-04 18:20:34.128812: precision = 0.622
After 4290 training steps, loss on training batch is 0.307638
2019-07-04 18:20:53.597259: precision = 0.628
After 4680 training steps, loss on training batch is 0.278017
2019-07-04 18:21:13.187782: precision = 0.632
After 5070 training steps, loss on training batch is 0.338416
2019-07-04 18:21:32.763107: precision = 0.644
After 5460 training steps, loss on training batch is 0.334112
2019-07-04 18:21:52.376543: precision = 0.634
After 5850 training steps, loss on training batch is 0.289926
2019-07-04 18:22:12.003205: precision = 0.643
After 6240 training steps, loss on training batch is 0.194501
2019-07-04 18:22:31.565443: precision = 0.651
After 6630 training steps, loss on training batch is 0.160543
2019-07-04 18:22:51.144321: precision = 0.653
After 7020 training steps, loss on training batch is 0.150305
2019-07-04 18:23:10.718125: precision = 0.647
After 7410 training steps, loss on training batch is 0.177146
2019-07-04 18:23:30.236512: precision = 0.648
After 7800 training steps, loss on training batch is 0.194741
2019-07-04 18:23:49.806560: precision = 0.652
After 8190 training steps, loss on training batch is 0.463946
2019-07-04 18:24:09.374088: precision = 0.651
After 8580 training steps, loss on training batch is 0.193039
2019-07-04 18:24:28.920675: precision = 0.652
After 8970 training steps, loss on training batch is 0.085848
2019-07-04 18:24:48.412748: precision = 0.652
After 9360 training steps, loss on training batch is 0.128902
2019-07-04 18:25:07.966116: precision = 0.649
After 9750 training steps, loss on training batch is 0.173071
2019-07-04 18:25:27.525040: precision = 0.656
After 10140 training steps, loss on training batch is 0.148953
2019-07-04 18:25:47.128317: precision = 0.654
After 10530 training steps, loss on training batch is 0.057766
2019-07-04 18:26:06.724513: precision = 0.660
After 10920 training steps, loss on training batch is 0.028146
2019-07-04 18:26:26.326957: precision = 0.656
After 11310 training steps, loss on training batch is 0.296460
2019-07-04 18:26:45.919958: precision = 0.651
After 11700 training steps, loss on training batch is 0.144524
2019-07-04 18:27:05.472550: precision = 0.651
After 12090 training steps, loss on training batch is 0.092179
2019-07-04 18:27:25.036709: precision = 0.665
After 12480 training steps, loss on training batch is 0.087165
2019-07-04 18:27:44.665714: precision = 0.657
After 12870 training steps, loss on training batch is 0.141294
2019-07-04 18:28:04.305513: precision = 0.656
After 13260 training steps, loss on training batch is 0.046859
2019-07-04 18:28:23.817135: precision = 0.659
After 13650 training steps, loss on training batch is 0.031781
2019-07-04 18:28:43.383706: precision = 0.656
After 14040 training steps, loss on training batch is 0.068658
2019-07-04 18:29:02.982133: precision = 0.661
After 14430 training steps, loss on training batch is 0.089164
2019-07-04 18:29:22.610055: precision = 0.659
After 14820 training steps, loss on training batch is 0.044610
2019-07-04 18:29:42.236530: precision = 0.657
After 15210 training steps, loss on training batch is 0.061680
2019-07-04 18:30:01.822820: precision = 0.664
After 15600 training steps, loss on training batch is 0.038035
2019-07-04 18:30:21.375561: precision = 0.657
After 15990 training steps, loss on training batch is 0.142862
2019-07-04 18:30:40.949441: precision = 0.670
After 16380 training steps, loss on training batch is 0.105047
2019-07-04 18:31:00.505086: precision = 0.661
After 16770 training steps, loss on training batch is 0.019833
2019-07-04 18:31:20.077109: precision = 0.647
After 17160 training steps, loss on training batch is 0.055903
2019-07-04 18:31:39.613719: precision = 0.656
After 17550 training steps, loss on training batch is 0.032540
2019-07-04 18:31:59.184842: precision = 0.664
After 17940 training steps, loss on training batch is 0.052382
2019-07-04 18:32:18.712162: precision = 0.658
After 18330 training steps, loss on training batch is 0.085266
2019-07-04 18:32:38.325082: precision = 0.655
After 18720 training steps, loss on training batch is 0.132801
2019-07-04 18:32:57.946370: precision = 0.666
After 19110 training steps, loss on training batch is 0.079349
2019-07-04 18:33:17.518416: precision = 0.662
After 390 training steps, loss on training batch is 2.675562
2019-07-04 18:34:00.373643: precision = 0.533
After 780 training steps, loss on training batch is 2.682812
2019-07-04 18:34:20.103510: precision = 0.586
After 1170 training steps, loss on training batch is 1.820122
2019-07-04 18:34:39.770374: precision = 0.589
After 1560 training steps, loss on training batch is 1.222292
2019-07-04 18:34:59.531423: precision = 0.594
After 1950 training steps, loss on training batch is 1.057104
2019-07-04 18:35:19.265284: precision = 0.608
After 2340 training steps, loss on training batch is 1.087688
2019-07-04 18:35:39.068357: precision = 0.615
After 2730 training steps, loss on training batch is 0.947625
2019-07-04 18:35:58.732831: precision = 0.620
After 3120 training steps, loss on training batch is 0.870049
2019-07-04 18:36:18.493444: precision = 0.621
After 3510 training steps, loss on training batch is 0.798989
2019-07-04 18:36:38.261999: precision = 0.625
After 3900 training steps, loss on training batch is 0.508043
2019-07-04 18:36:58.052868: precision = 0.631
After 4290 training steps, loss on training batch is 0.334331
2019-07-04 18:37:17.863523: precision = 0.637
After 4680 training steps, loss on training batch is 0.376529
2019-07-04 18:37:37.633899: precision = 0.630
After 5070 training steps, loss on training batch is 0.369711
2019-07-04 18:37:57.355136: precision = 0.641
After 5460 training steps, loss on training batch is 0.500681
2019-07-04 18:38:17.090209: precision = 0.650
After 5850 training steps, loss on training batch is 0.379883
2019-07-04 18:38:36.819830: precision = 0.648
After 6240 training steps, loss on training batch is 0.227987
2019-07-04 18:38:56.616121: precision = 0.654
After 6630 training steps, loss on training batch is 0.161885
2019-07-04 18:39:16.329336: precision = 0.640
After 7020 training steps, loss on training batch is 0.219366
2019-07-04 18:39:36.068696: precision = 0.649
After 7410 training steps, loss on training batch is 0.127547
2019-07-04 18:39:55.751171: precision = 0.658
After 7800 training steps, loss on training batch is 0.253413
2019-07-04 18:40:15.487304: precision = 0.652
After 8190 training steps, loss on training batch is 0.118107
2019-07-04 18:40:35.259641: precision = 0.658
After 8580 training steps, loss on training batch is 0.147039
2019-07-04 18:40:55.040311: precision = 0.659
After 8970 training steps, loss on training batch is 0.120798
2019-07-04 18:41:14.837418: precision = 0.657
After 9360 training steps, loss on training batch is 0.147285
2019-07-04 18:41:34.598682: precision = 0.661
After 9750 training steps, loss on training batch is 0.190226
2019-07-04 18:41:54.331929: precision = 0.659
After 10140 training steps, loss on training batch is 0.131752
2019-07-04 18:42:14.099671: precision = 0.660
After 10530 training steps, loss on training batch is 0.189460
2019-07-04 18:42:33.848251: precision = 0.663
After 10920 training steps, loss on training batch is 0.060796
2019-07-04 18:42:53.641111: precision = 0.658
After 11310 training steps, loss on training batch is 0.050249
2019-07-04 18:43:13.422154: precision = 0.664
After 11700 training steps, loss on training batch is 0.117165
2019-07-04 18:43:33.236225: precision = 0.654
After 12090 training steps, loss on training batch is 0.063088
2019-07-04 18:43:52.961856: precision = 0.656
After 12480 training steps, loss on training batch is 0.045335
2019-07-04 18:44:12.740228: precision = 0.666
After 12870 training steps, loss on training batch is 0.072514
2019-07-04 18:44:32.497844: precision = 0.665
After 13260 training steps, loss on training batch is 0.106111
2019-07-04 18:44:52.266188: precision = 0.663
After 13650 training steps, loss on training batch is 0.097130
2019-07-04 18:45:12.025972: precision = 0.662
After 14040 training steps, loss on training batch is 0.135254
2019-07-04 18:45:31.807680: precision = 0.662
After 14430 training steps, loss on training batch is 0.145672
2019-07-04 18:45:51.602246: precision = 0.655
After 14820 training steps, loss on training batch is 0.081292
2019-07-04 18:46:11.348435: precision = 0.663
After 15210 training steps, loss on training batch is 0.055554
2019-07-04 18:46:31.101209: precision = 0.663
After 15600 training steps, loss on training batch is 0.100377
2019-07-04 18:46:50.851476: precision = 0.671
After 15990 training steps, loss on training batch is 0.197851
2019-07-04 18:47:10.617893: precision = 0.663
After 16380 training steps, loss on training batch is 0.037188
2019-07-04 18:47:30.366931: precision = 0.657
After 16770 training steps, loss on training batch is 0.081296
2019-07-04 18:47:50.043237: precision = 0.661
After 17160 training steps, loss on training batch is 0.033169
2019-07-04 18:48:09.810498: precision = 0.662
After 17550 training steps, loss on training batch is 0.104928
2019-07-04 18:48:29.585330: precision = 0.656
After 17940 training steps, loss on training batch is 0.062490
2019-07-04 18:48:49.318782: precision = 0.665
After 18330 training steps, loss on training batch is 0.053860
2019-07-04 18:49:09.108370: precision = 0.666
After 18720 training steps, loss on training batch is 0.092748
2019-07-04 18:49:28.875348: precision = 0.667
After 19110 training steps, loss on training batch is 0.040264
2019-07-04 18:49:48.666742: precision = 0.665
After 390 training steps, loss on training batch is 2.929772
2019-07-04 18:50:31.441554: precision = 0.543
After 780 training steps, loss on training batch is 2.638358
2019-07-04 18:50:51.057739: precision = 0.590
After 1170 training steps, loss on training batch is 1.801776
2019-07-04 18:51:10.698312: precision = 0.608
After 1560 training steps, loss on training batch is 1.216318
2019-07-04 18:51:30.432591: precision = 0.602
After 1950 training steps, loss on training batch is 1.115324
2019-07-04 18:51:50.166614: precision = 0.612
After 2340 training steps, loss on training batch is 1.210417
2019-07-04 18:52:09.915264: precision = 0.618
After 2730 training steps, loss on training batch is 0.862666
2019-07-04 18:52:29.616840: precision = 0.606
After 3120 training steps, loss on training batch is 1.068831
2019-07-04 18:52:49.339191: precision = 0.618
After 3510 training steps, loss on training batch is 0.784868
2019-07-04 18:53:09.051721: precision = 0.632
After 3900 training steps, loss on training batch is 0.465249
2019-07-04 18:53:28.750146: precision = 0.634
After 4290 training steps, loss on training batch is 0.277226
2019-07-04 18:53:48.427985: precision = 0.639
After 4680 training steps, loss on training batch is 0.262253
2019-07-04 18:54:08.202691: precision = 0.650
After 5070 training steps, loss on training batch is 0.188822
2019-07-04 18:54:27.848905: precision = 0.645
After 5460 training steps, loss on training batch is 0.362822
2019-07-04 18:54:47.526332: precision = 0.647
After 5850 training steps, loss on training batch is 0.148161
2019-07-04 18:55:07.221122: precision = 0.655
After 6240 training steps, loss on training batch is 0.107498
2019-07-04 18:55:26.888500: precision = 0.645
After 6630 training steps, loss on training batch is 0.157430
2019-07-04 18:55:46.621817: precision = 0.643
After 7020 training steps, loss on training batch is 0.298071
2019-07-04 18:56:06.308881: precision = 0.653
After 7410 training steps, loss on training batch is 0.145884
2019-07-04 18:56:26.066015: precision = 0.652
After 7800 training steps, loss on training batch is 0.135765
2019-07-04 18:56:45.797763: precision = 0.656
After 8190 training steps, loss on training batch is 0.149718
2019-07-04 18:57:05.552209: precision = 0.649
After 8580 training steps, loss on training batch is 0.100042
2019-07-04 18:57:25.313546: precision = 0.654
After 8970 training steps, loss on training batch is 0.091008
2019-07-04 18:57:45.049744: precision = 0.653
After 9360 training steps, loss on training batch is 0.180982
2019-07-04 18:58:04.761282: precision = 0.659
After 9750 training steps, loss on training batch is 0.130596
2019-07-04 18:58:24.583228: precision = 0.660
After 10140 training steps, loss on training batch is 0.163811
2019-07-04 18:58:44.321655: precision = 0.650
After 10530 training steps, loss on training batch is 0.117133
2019-07-04 18:59:04.035617: precision = 0.656
After 10920 training steps, loss on training batch is 0.121970
2019-07-04 18:59:23.703167: precision = 0.653
After 11310 training steps, loss on training batch is 0.082325
2019-07-04 18:59:43.466567: precision = 0.654
After 11700 training steps, loss on training batch is 0.168266
2019-07-04 19:00:03.225107: precision = 0.662
After 12090 training steps, loss on training batch is 0.233865
2019-07-04 19:00:23.006919: precision = 0.666
After 12480 training steps, loss on training batch is 0.079975
2019-07-04 19:00:42.743707: precision = 0.657
After 12870 training steps, loss on training batch is 0.078694
2019-07-04 19:01:02.471107: precision = 0.658
After 13260 training steps, loss on training batch is 0.176985
2019-07-04 19:01:22.261192: precision = 0.655
After 13650 training steps, loss on training batch is 0.081471
2019-07-04 19:01:42.006050: precision = 0.660
After 14040 training steps, loss on training batch is 0.178597
2019-07-04 19:02:01.735948: precision = 0.656
After 14430 training steps, loss on training batch is 0.047547
2019-07-04 19:02:21.469243: precision = 0.656
After 14820 training steps, loss on training batch is 0.101216
2019-07-04 19:02:41.159381: precision = 0.659
After 15210 training steps, loss on training batch is 0.086152
2019-07-04 19:03:00.796455: precision = 0.664
After 15600 training steps, loss on training batch is 0.033937
2019-07-04 19:03:20.501542: precision = 0.661
After 15990 training steps, loss on training batch is 0.157497
2019-07-04 19:03:40.250810: precision = 0.661
After 16380 training steps, loss on training batch is 0.203068
2019-07-04 19:03:59.975386: precision = 0.660
After 16770 training steps, loss on training batch is 0.039366
2019-07-04 19:04:19.763900: precision = 0.667
After 17160 training steps, loss on training batch is 0.084144
2019-07-04 19:04:39.509606: precision = 0.660
After 17550 training steps, loss on training batch is 0.084887
2019-07-04 19:04:59.286704: precision = 0.664
After 17940 training steps, loss on training batch is 0.120180
2019-07-04 19:05:19.009644: precision = 0.664
After 18330 training steps, loss on training batch is 0.067143
2019-07-04 19:05:38.698408: precision = 0.663
After 18720 training steps, loss on training batch is 0.007263
2019-07-04 19:05:58.407339: precision = 0.668
After 19110 training steps, loss on training batch is 0.045224
2019-07-04 19:06:18.219636: precision = 0.671
After 390 training steps, loss on training batch is 2.971605
2019-07-04 19:07:00.859058: precision = 0.527
After 780 training steps, loss on training batch is 2.507814
2019-07-04 19:07:20.481624: precision = 0.581
After 1170 training steps, loss on training batch is 2.006093
2019-07-04 19:07:40.107871: precision = 0.603
After 1560 training steps, loss on training batch is 1.407010
2019-07-04 19:07:59.712033: precision = 0.601
After 1950 training steps, loss on training batch is 1.189603
2019-07-04 19:08:19.299541: precision = 0.617
After 2340 training steps, loss on training batch is 1.236455
2019-07-04 19:08:38.952214: precision = 0.624
After 2730 training steps, loss on training batch is 0.684741
2019-07-04 19:08:58.608330: precision = 0.614
After 3120 training steps, loss on training batch is 0.975576
2019-07-04 19:09:18.203721: precision = 0.617
After 3510 training steps, loss on training batch is 1.088862
2019-07-04 19:09:37.849066: precision = 0.625
After 3900 training steps, loss on training batch is 0.385624
2019-07-04 19:09:57.492025: precision = 0.633
After 4290 training steps, loss on training batch is 0.333087
2019-07-04 19:10:17.157010: precision = 0.633
After 4680 training steps, loss on training batch is 0.346812
2019-07-04 19:10:36.824275: precision = 0.645
After 5070 training steps, loss on training batch is 0.388837
2019-07-04 19:10:56.404031: precision = 0.644
After 5460 training steps, loss on training batch is 0.264069
2019-07-04 19:11:16.048668: precision = 0.643
After 5850 training steps, loss on training batch is 0.239838
2019-07-04 19:11:35.678342: precision = 0.640
After 6240 training steps, loss on training batch is 0.290159
2019-07-04 19:11:55.316222: precision = 0.641
After 6630 training steps, loss on training batch is 0.112411
2019-07-04 19:12:14.972413: precision = 0.653
After 7020 training steps, loss on training batch is 0.153565
2019-07-04 19:12:34.586635: precision = 0.649
After 7410 training steps, loss on training batch is 0.185676
2019-07-04 19:12:54.177324: precision = 0.652
After 7800 training steps, loss on training batch is 0.278343
2019-07-04 19:13:13.802790: precision = 0.657
After 8190 training steps, loss on training batch is 0.276437
2019-07-04 19:13:33.449696: precision = 0.653
After 8580 training steps, loss on training batch is 0.058413
2019-07-04 19:13:53.043058: precision = 0.657
After 8970 training steps, loss on training batch is 0.094609
2019-07-04 19:14:12.646835: precision = 0.660
After 9360 training steps, loss on training batch is 0.150899
2019-07-04 19:14:32.186263: precision = 0.659
After 9750 training steps, loss on training batch is 0.128789
2019-07-04 19:14:51.730071: precision = 0.661
After 10140 training steps, loss on training batch is 0.255658
2019-07-04 19:15:11.353836: precision = 0.657
After 10530 training steps, loss on training batch is 0.211963
2019-07-04 19:15:30.978663: precision = 0.655
After 10920 training steps, loss on training batch is 0.062647
2019-07-04 19:15:50.624162: precision = 0.653
After 11310 training steps, loss on training batch is 0.108141
2019-07-04 19:16:10.242952: precision = 0.657
After 11700 training steps, loss on training batch is 0.223543
2019-07-04 19:16:29.890243: precision = 0.652
After 12090 training steps, loss on training batch is 0.048655
2019-07-04 19:16:49.574133: precision = 0.649
After 12480 training steps, loss on training batch is 0.041274
2019-07-04 19:17:09.229449: precision = 0.660
After 12870 training steps, loss on training batch is 0.015541
2019-07-04 19:17:28.847306: precision = 0.662
After 13260 training steps, loss on training batch is 0.071313
2019-07-04 19:17:48.515373: precision = 0.657
After 13650 training steps, loss on training batch is 0.073816
2019-07-04 19:18:08.189014: precision = 0.665
After 14040 training steps, loss on training batch is 0.187759
2019-07-04 19:18:27.826498: precision = 0.656
After 14430 training steps, loss on training batch is 0.089964
2019-07-04 19:18:47.323617: precision = 0.662
After 14820 training steps, loss on training batch is 0.241901
2019-07-04 19:19:06.970305: precision = 0.656
After 15210 training steps, loss on training batch is 0.093445
2019-07-04 19:19:26.587920: precision = 0.658
After 15600 training steps, loss on training batch is 0.057657
2019-07-04 19:19:46.242063: precision = 0.663
After 15990 training steps, loss on training batch is 0.029660
2019-07-04 19:20:05.889119: precision = 0.662
After 16380 training steps, loss on training batch is 0.030234
2019-07-04 19:20:25.524225: precision = 0.667
After 16770 training steps, loss on training batch is 0.015695
2019-07-04 19:20:45.191459: precision = 0.661
After 17160 training steps, loss on training batch is 0.045090
2019-07-04 19:21:04.788306: precision = 0.664
After 17550 training steps, loss on training batch is 0.036572
2019-07-04 19:21:24.445992: precision = 0.666
After 17940 training steps, loss on training batch is 0.024031
2019-07-04 19:21:44.102889: precision = 0.671
After 18330 training steps, loss on training batch is 0.142922
2019-07-04 19:22:03.725229: precision = 0.662
After 18720 training steps, loss on training batch is 0.103738
2019-07-04 19:22:23.377494: precision = 0.656
After 19110 training steps, loss on training batch is 0.018908
2019-07-04 19:22:42.952319: precision = 0.664
After 390 training steps, loss on training batch is 2.837476
2019-07-04 19:23:25.456586: precision = 0.536
After 780 training steps, loss on training batch is 2.786725
2019-07-04 19:23:45.148758: precision = 0.576
After 1170 training steps, loss on training batch is 1.891867
2019-07-04 19:24:04.863198: precision = 0.610
After 1560 training steps, loss on training batch is 1.406523
2019-07-04 19:24:24.525856: precision = 0.600
After 1950 training steps, loss on training batch is 1.127727
2019-07-04 19:24:44.190497: precision = 0.601
After 2340 training steps, loss on training batch is 1.198495
2019-07-04 19:25:03.854250: precision = 0.617
After 2730 training steps, loss on training batch is 0.693791
2019-07-04 19:25:23.589080: precision = 0.613
After 3120 training steps, loss on training batch is 0.786784
2019-07-04 19:25:43.241384: precision = 0.609
After 3510 training steps, loss on training batch is 1.029825
2019-07-04 19:26:02.932782: precision = 0.626
After 3900 training steps, loss on training batch is 0.463999
2019-07-04 19:26:22.480432: precision = 0.626
After 4290 training steps, loss on training batch is 0.530701
2019-07-04 19:26:42.104748: precision = 0.629
After 4680 training steps, loss on training batch is 0.370044
2019-07-04 19:27:01.785360: precision = 0.632
After 5070 training steps, loss on training batch is 0.421768
2019-07-04 19:27:21.475231: precision = 0.643
After 5460 training steps, loss on training batch is 0.427249
2019-07-04 19:27:41.126745: precision = 0.640
After 5850 training steps, loss on training batch is 0.183538
2019-07-04 19:28:00.825238: precision = 0.640
After 6240 training steps, loss on training batch is 0.257657
2019-07-04 19:28:20.536192: precision = 0.651
After 6630 training steps, loss on training batch is 0.092772
2019-07-04 19:28:40.237074: precision = 0.654
After 7020 training steps, loss on training batch is 0.719475
2019-07-04 19:28:59.912795: precision = 0.649
After 7410 training steps, loss on training batch is 0.445639
2019-07-04 19:29:19.591698: precision = 0.655
After 7800 training steps, loss on training batch is 0.169410
2019-07-04 19:29:39.290537: precision = 0.651
After 8190 training steps, loss on training batch is 0.272117
2019-07-04 19:29:58.990832: precision = 0.642
After 8580 training steps, loss on training batch is 0.106704
2019-07-04 19:30:18.615556: precision = 0.652
After 8970 training steps, loss on training batch is 0.075215
2019-07-04 19:30:38.285181: precision = 0.655
After 9360 training steps, loss on training batch is 0.215985
2019-07-04 19:30:57.935060: precision = 0.650
After 9750 training steps, loss on training batch is 0.160847
2019-07-04 19:31:17.604456: precision = 0.659
After 10140 training steps, loss on training batch is 0.229468
2019-07-04 19:31:37.267412: precision = 0.662
After 10530 training steps, loss on training batch is 0.084367
2019-07-04 19:31:56.892608: precision = 0.670
After 10920 training steps, loss on training batch is 0.133546
2019-07-04 19:32:16.556846: precision = 0.655
After 11310 training steps, loss on training batch is 0.200492
2019-07-04 19:32:36.222957: precision = 0.658
After 11700 training steps, loss on training batch is 0.082707
2019-07-04 19:32:55.831977: precision = 0.661
After 12090 training steps, loss on training batch is 0.131977
2019-07-04 19:33:15.495920: precision = 0.659
After 12480 training steps, loss on training batch is 0.040934
2019-07-04 19:33:35.183898: precision = 0.663
After 12870 training steps, loss on training batch is 0.055091
2019-07-04 19:33:54.795517: precision = 0.660
After 13260 training steps, loss on training batch is 0.232646
2019-07-04 19:34:14.330293: precision = 0.660
After 13650 training steps, loss on training batch is 0.078372
2019-07-04 19:34:33.985377: precision = 0.664
After 14040 training steps, loss on training batch is 0.059155
2019-07-04 19:34:53.681384: precision = 0.667
After 14430 training steps, loss on training batch is 0.037487
2019-07-04 19:35:13.336140: precision = 0.665
After 14820 training steps, loss on training batch is 0.130660
2019-07-04 19:35:33.045302: precision = 0.667
After 15210 training steps, loss on training batch is 0.055663
2019-07-04 19:35:52.700951: precision = 0.667
After 15600 training steps, loss on training batch is 0.087979
2019-07-04 19:36:12.353815: precision = 0.664
After 15990 training steps, loss on training batch is 0.275338
2019-07-04 19:36:32.021018: precision = 0.657
After 16380 training steps, loss on training batch is 0.035358
2019-07-04 19:36:51.708239: precision = 0.662
After 16770 training steps, loss on training batch is 0.016348
2019-07-04 19:37:11.388679: precision = 0.669
After 17160 training steps, loss on training batch is 0.172068
2019-07-04 19:37:31.057106: precision = 0.666
After 17550 training steps, loss on training batch is 0.081686
2019-07-04 19:37:50.671384: precision = 0.658
After 17940 training steps, loss on training batch is 0.149817
2019-07-04 19:38:10.223615: precision = 0.668
After 18330 training steps, loss on training batch is 0.112820
2019-07-04 19:38:29.842806: precision = 0.664
After 18720 training steps, loss on training batch is 0.068925
2019-07-04 19:38:49.571870: precision = 0.661
After 19110 training steps, loss on training batch is 0.090440
2019-07-04 19:39:09.165585: precision = 0.660
After 390 training steps, loss on training batch is 2.736955
2019-07-04 19:39:51.606524: precision = 0.533
After 780 training steps, loss on training batch is 2.852430
2019-07-04 19:40:11.254144: precision = 0.582
After 1170 training steps, loss on training batch is 1.911432
2019-07-04 19:40:30.882444: precision = 0.594
After 1560 training steps, loss on training batch is 1.242087
2019-07-04 19:40:50.495518: precision = 0.602
After 1950 training steps, loss on training batch is 1.283168
2019-07-04 19:41:10.140904: precision = 0.614
After 2340 training steps, loss on training batch is 1.330620
2019-07-04 19:41:29.753043: precision = 0.614
After 2730 training steps, loss on training batch is 0.764919
2019-07-04 19:41:49.357226: precision = 0.612
After 3120 training steps, loss on training batch is 0.751757
2019-07-04 19:42:09.040085: precision = 0.604
After 3510 training steps, loss on training batch is 0.593480
2019-07-04 19:42:28.726778: precision = 0.609
After 3900 training steps, loss on training batch is 0.575913
2019-07-04 19:42:48.404664: precision = 0.634
After 4290 training steps, loss on training batch is 0.208511
2019-07-04 19:43:08.110692: precision = 0.630
After 4680 training steps, loss on training batch is 0.234298
2019-07-04 19:43:27.765651: precision = 0.636
After 5070 training steps, loss on training batch is 0.312077
2019-07-04 19:43:47.386636: precision = 0.638
After 5460 training steps, loss on training batch is 0.631144
2019-07-04 19:44:06.996162: precision = 0.640
After 5850 training steps, loss on training batch is 0.246845
2019-07-04 19:44:26.591545: precision = 0.650
After 6240 training steps, loss on training batch is 0.247780
2019-07-04 19:44:46.199235: precision = 0.646
After 6630 training steps, loss on training batch is 0.094110
2019-07-04 19:45:05.781312: precision = 0.657
After 7020 training steps, loss on training batch is 0.198711
2019-07-04 19:45:25.442736: precision = 0.639
After 7410 training steps, loss on training batch is 0.131553
2019-07-04 19:45:45.006915: precision = 0.649
After 7800 training steps, loss on training batch is 0.113680
2019-07-04 19:46:04.616542: precision = 0.650
After 8190 training steps, loss on training batch is 0.102252
2019-07-04 19:46:24.261291: precision = 0.655
After 8580 training steps, loss on training batch is 0.281215
2019-07-04 19:46:43.874629: precision = 0.658
After 8970 training steps, loss on training batch is 0.105617
2019-07-04 19:47:03.528449: precision = 0.647
After 9360 training steps, loss on training batch is 0.087625
2019-07-04 19:47:23.176928: precision = 0.658
After 9750 training steps, loss on training batch is 0.195050
2019-07-04 19:47:42.859988: precision = 0.651
After 10140 training steps, loss on training batch is 0.161300
2019-07-04 19:48:02.575579: precision = 0.652
After 10530 training steps, loss on training batch is 0.067466
2019-07-04 19:48:22.252317: precision = 0.655
After 10920 training steps, loss on training batch is 0.133583
2019-07-04 19:48:41.915507: precision = 0.658
After 11310 training steps, loss on training batch is 0.161416
2019-07-04 19:49:01.497725: precision = 0.660
After 11700 training steps, loss on training batch is 0.174402
2019-07-04 19:49:21.119413: precision = 0.654
After 12090 training steps, loss on training batch is 0.119696
2019-07-04 19:49:40.658354: precision = 0.658
After 12480 training steps, loss on training batch is 0.145597
2019-07-04 19:50:00.273999: precision = 0.655
After 12870 training steps, loss on training batch is 0.036560
2019-07-04 19:50:19.951352: precision = 0.651
After 13260 training steps, loss on training batch is 0.018596
2019-07-04 19:50:39.553672: precision = 0.658
After 13650 training steps, loss on training batch is 0.209167
2019-07-04 19:50:59.235977: precision = 0.658
After 14040 training steps, loss on training batch is 0.049092
2019-07-04 19:51:18.905556: precision = 0.655
After 14430 training steps, loss on training batch is 0.006433
2019-07-04 19:51:38.521121: precision = 0.655
After 14820 training steps, loss on training batch is 0.045419
2019-07-04 19:51:58.203697: precision = 0.659
After 15210 training steps, loss on training batch is 0.040004
2019-07-04 19:52:17.881484: precision = 0.661
After 15600 training steps, loss on training batch is 0.070527
2019-07-04 19:52:37.524463: precision = 0.655
After 15990 training steps, loss on training batch is 0.035928
2019-07-04 19:52:57.196412: precision = 0.654
After 16380 training steps, loss on training batch is 0.015691
2019-07-04 19:53:16.881658: precision = 0.648
After 16770 training steps, loss on training batch is 0.164561
2019-07-04 19:53:36.481017: precision = 0.653
After 17160 training steps, loss on training batch is 0.031372
2019-07-04 19:53:56.157365: precision = 0.655
After 17550 training steps, loss on training batch is 0.069725
2019-07-04 19:54:15.818658: precision = 0.655
After 17940 training steps, loss on training batch is 0.022294
2019-07-04 19:54:35.473758: precision = 0.652
After 18330 training steps, loss on training batch is 0.038572
2019-07-04 19:54:55.162426: precision = 0.664
After 18720 training steps, loss on training batch is 0.011294
2019-07-04 19:55:14.772281: precision = 0.658
After 19110 training steps, loss on training batch is 0.046611
2019-07-04 19:55:34.461607: precision = 0.661
After 390 training steps, loss on training batch is 2.768099
2019-07-04 19:56:17.423990: precision = 0.529
After 780 training steps, loss on training batch is 2.879617
2019-07-04 19:56:37.113206: precision = 0.573
After 1170 training steps, loss on training batch is 2.118274
2019-07-04 19:56:56.827954: precision = 0.591
After 1560 training steps, loss on training batch is 1.257522
2019-07-04 19:57:16.578656: precision = 0.607
After 1950 training steps, loss on training batch is 1.290423
2019-07-04 19:57:36.176495: precision = 0.606
After 2340 training steps, loss on training batch is 1.224291
2019-07-04 19:57:55.953381: precision = 0.628
After 2730 training steps, loss on training batch is 0.951611
2019-07-04 19:58:15.683984: precision = 0.611
After 3120 training steps, loss on training batch is 0.868822
2019-07-04 19:58:35.428805: precision = 0.628
After 3510 training steps, loss on training batch is 0.750690
2019-07-04 19:58:55.152298: precision = 0.627
After 3900 training steps, loss on training batch is 0.396734
2019-07-04 19:59:14.910980: precision = 0.638
After 4290 training steps, loss on training batch is 0.234448
2019-07-04 19:59:34.610618: precision = 0.625
After 4680 training steps, loss on training batch is 0.559048
2019-07-04 19:59:54.351587: precision = 0.633
After 5070 training steps, loss on training batch is 0.398048
2019-07-04 20:00:14.123301: precision = 0.637
After 5460 training steps, loss on training batch is 0.310711
2019-07-04 20:00:33.873105: precision = 0.638
After 5850 training steps, loss on training batch is 0.373827
2019-07-04 20:00:53.611299: precision = 0.646
After 6240 training steps, loss on training batch is 0.202244
2019-07-04 20:01:13.356002: precision = 0.640
After 6630 training steps, loss on training batch is 0.157494
2019-07-04 20:01:33.014283: precision = 0.649
After 7020 training steps, loss on training batch is 0.147459
2019-07-04 20:01:52.783919: precision = 0.647
After 7410 training steps, loss on training batch is 0.200695
2019-07-04 20:02:12.497180: precision = 0.649
After 7800 training steps, loss on training batch is 0.255081
2019-07-04 20:02:32.247100: precision = 0.651
After 8190 training steps, loss on training batch is 0.220425
2019-07-04 20:02:51.990762: precision = 0.646
After 8580 training steps, loss on training batch is 0.171345
2019-07-04 20:03:11.719694: precision = 0.656
After 8970 training steps, loss on training batch is 0.241149
2019-07-04 20:03:31.437413: precision = 0.652
After 9360 training steps, loss on training batch is 0.182202
2019-07-04 20:03:51.225141: precision = 0.661
After 9750 training steps, loss on training batch is 0.145160
2019-07-04 20:04:10.939870: precision = 0.658
After 10140 training steps, loss on training batch is 0.182756
2019-07-04 20:04:30.678366: precision = 0.651
After 10530 training steps, loss on training batch is 0.212291
2019-07-04 20:04:50.367904: precision = 0.662
After 10920 training steps, loss on training batch is 0.221257
2019-07-04 20:05:10.086943: precision = 0.658
After 11310 training steps, loss on training batch is 0.101146
2019-07-04 20:05:29.713495: precision = 0.653
After 11700 training steps, loss on training batch is 0.026458
2019-07-04 20:05:49.469593: precision = 0.666
After 12090 training steps, loss on training batch is 0.056459
2019-07-04 20:06:09.155916: precision = 0.661
After 12480 training steps, loss on training batch is 0.066834
2019-07-04 20:06:28.892942: precision = 0.666
After 12870 training steps, loss on training batch is 0.198482
2019-07-04 20:06:48.661271: precision = 0.663
After 13260 training steps, loss on training batch is 0.060535
2019-07-04 20:07:08.462379: precision = 0.660
After 13650 training steps, loss on training batch is 0.023952
2019-07-04 20:07:28.211644: precision = 0.662
After 14040 training steps, loss on training batch is 0.077352
2019-07-04 20:07:47.945795: precision = 0.657
After 14430 training steps, loss on training batch is 0.020648
2019-07-04 20:08:07.727559: precision = 0.665
After 14820 training steps, loss on training batch is 0.106848
2019-07-04 20:08:27.519168: precision = 0.666
After 15210 training steps, loss on training batch is 0.054503
2019-07-04 20:08:47.281293: precision = 0.657
After 15600 training steps, loss on training batch is 0.056162
2019-07-04 20:09:07.019185: precision = 0.669
After 15990 training steps, loss on training batch is 0.171724
2019-07-04 20:09:26.740380: precision = 0.663
After 16380 training steps, loss on training batch is 0.219948
2019-07-04 20:09:46.519638: precision = 0.667
After 16770 training steps, loss on training batch is 0.034085
2019-07-04 20:10:06.237002: precision = 0.667
After 17160 training steps, loss on training batch is 0.010583
2019-07-04 20:10:25.995159: precision = 0.671
After 17550 training steps, loss on training batch is 0.098817
2019-07-04 20:10:45.754228: precision = 0.663
After 17940 training steps, loss on training batch is 0.028587
2019-07-04 20:11:05.515827: precision = 0.676
After 18330 training steps, loss on training batch is 0.215665
2019-07-04 20:11:25.250074: precision = 0.666
After 18720 training steps, loss on training batch is 0.018609
2019-07-04 20:11:45.039167: precision = 0.658
After 19110 training steps, loss on training batch is 0.063140
2019-07-04 20:12:04.848006: precision = 0.676
After 390 training steps, loss on training batch is 2.918242
2019-07-04 20:12:47.672875: precision = 0.545
After 780 training steps, loss on training batch is 2.764505
2019-07-04 20:13:07.191002: precision = 0.588
After 1170 training steps, loss on training batch is 1.672098
2019-07-04 20:13:26.864156: precision = 0.606
After 1560 training steps, loss on training batch is 1.073199
2019-07-04 20:13:46.555869: precision = 0.611
After 1950 training steps, loss on training batch is 1.147671
2019-07-04 20:14:06.254924: precision = 0.626
After 2340 training steps, loss on training batch is 1.143163
2019-07-04 20:14:25.895631: precision = 0.623
After 2730 training steps, loss on training batch is 0.846505
2019-07-04 20:14:45.500534: precision = 0.635
After 3120 training steps, loss on training batch is 0.649976
2019-07-04 20:15:05.182492: precision = 0.627
After 3510 training steps, loss on training batch is 0.456545
2019-07-04 20:15:24.867489: precision = 0.630
After 3900 training steps, loss on training batch is 0.457026
2019-07-04 20:15:44.518535: precision = 0.636
After 4290 training steps, loss on training batch is 0.343474
2019-07-04 20:16:04.125093: precision = 0.645
After 4680 training steps, loss on training batch is 0.248874
2019-07-04 20:16:23.778644: precision = 0.649
After 5070 training steps, loss on training batch is 0.306144
2019-07-04 20:16:43.404176: precision = 0.646
After 5460 training steps, loss on training batch is 0.228768
2019-07-04 20:17:03.030982: precision = 0.653
After 5850 training steps, loss on training batch is 0.215500
2019-07-04 20:17:22.723388: precision = 0.653
After 6240 training steps, loss on training batch is 0.302709
2019-07-04 20:17:42.363695: precision = 0.655
After 6630 training steps, loss on training batch is 0.061521
2019-07-04 20:18:01.983577: precision = 0.653
After 7020 training steps, loss on training batch is 0.099977
2019-07-04 20:18:21.615308: precision = 0.652
After 7410 training steps, loss on training batch is 0.408358
2019-07-04 20:18:41.248955: precision = 0.656
After 7800 training steps, loss on training batch is 0.097349
2019-07-04 20:19:00.896480: precision = 0.653
After 8190 training steps, loss on training batch is 0.180627
2019-07-04 20:19:20.570010: precision = 0.656
After 8580 training steps, loss on training batch is 0.091575
2019-07-04 20:19:40.167612: precision = 0.660
After 8970 training steps, loss on training batch is 0.122681
2019-07-04 20:19:59.826564: precision = 0.658
After 9360 training steps, loss on training batch is 0.245824
2019-07-04 20:20:19.506296: precision = 0.661
After 9750 training steps, loss on training batch is 0.081126
2019-07-04 20:20:39.094988: precision = 0.658
After 10140 training steps, loss on training batch is 0.033811
2019-07-04 20:20:58.649033: precision = 0.667
After 10530 training steps, loss on training batch is 0.155906
2019-07-04 20:21:18.316936: precision = 0.664
After 10920 training steps, loss on training batch is 0.144239
2019-07-04 20:21:37.962933: precision = 0.655
After 11310 training steps, loss on training batch is 0.148621
2019-07-04 20:21:57.568454: precision = 0.666
After 11700 training steps, loss on training batch is 0.091410
2019-07-04 20:22:17.142168: precision = 0.660
After 12090 training steps, loss on training batch is 0.051842
2019-07-04 20:22:36.791540: precision = 0.663
After 12480 training steps, loss on training batch is 0.146102
2019-07-04 20:22:56.450600: precision = 0.655
After 12870 training steps, loss on training batch is 0.134199
2019-07-04 20:23:16.103433: precision = 0.666
After 13260 training steps, loss on training batch is 0.117654
2019-07-04 20:23:35.768618: precision = 0.662
After 13650 training steps, loss on training batch is 0.048376
2019-07-04 20:23:55.438119: precision = 0.673
After 14040 training steps, loss on training batch is 0.180933
2019-07-04 20:24:15.100478: precision = 0.659
After 14430 training steps, loss on training batch is 0.045433
2019-07-04 20:24:34.727063: precision = 0.664
After 14820 training steps, loss on training batch is 0.061995
2019-07-04 20:24:54.229426: precision = 0.667
After 15210 training steps, loss on training batch is 0.032253
2019-07-04 20:25:13.807168: precision = 0.663
After 15600 training steps, loss on training batch is 0.016731
2019-07-04 20:25:33.427182: precision = 0.668
After 15990 training steps, loss on training batch is 0.111816
2019-07-04 20:25:53.087365: precision = 0.666
After 16380 training steps, loss on training batch is 0.097389
2019-07-04 20:26:12.741840: precision = 0.672
After 16770 training steps, loss on training batch is 0.129275
2019-07-04 20:26:32.404665: precision = 0.670
After 17160 training steps, loss on training batch is 0.020555
2019-07-04 20:26:52.057354: precision = 0.662
After 17550 training steps, loss on training batch is 0.129781
2019-07-04 20:27:11.727601: precision = 0.661
After 17940 training steps, loss on training batch is 0.103331
2019-07-04 20:27:31.354095: precision = 0.675
After 18330 training steps, loss on training batch is 0.067034
2019-07-04 20:27:50.969233: precision = 0.674
After 18720 training steps, loss on training batch is 0.192712
2019-07-04 20:28:10.599296: precision = 0.673
After 19110 training steps, loss on training batch is 0.043867
2019-07-04 20:28:30.237246: precision = 0.668
After 390 training steps, loss on training batch is 2.896129
2019-07-04 20:29:12.930832: precision = 0.539
After 780 training steps, loss on training batch is 2.620975
2019-07-04 20:29:32.651581: precision = 0.585
After 1170 training steps, loss on training batch is 2.175265
2019-07-04 20:29:52.358996: precision = 0.601
After 1560 training steps, loss on training batch is 1.212119
2019-07-04 20:30:12.042206: precision = 0.616
After 1950 training steps, loss on training batch is 1.130887
2019-07-04 20:30:31.718525: precision = 0.609
After 2340 training steps, loss on training batch is 1.441158
2019-07-04 20:30:51.388452: precision = 0.618
After 2730 training steps, loss on training batch is 0.749742
2019-07-04 20:31:11.028171: precision = 0.610
After 3120 training steps, loss on training batch is 0.733200
2019-07-04 20:31:30.776021: precision = 0.615
After 3510 training steps, loss on training batch is 0.719544
2019-07-04 20:31:50.471970: precision = 0.629
After 3900 training steps, loss on training batch is 0.577630
2019-07-04 20:32:10.219989: precision = 0.632
After 4290 training steps, loss on training batch is 0.190119
2019-07-04 20:32:29.845076: precision = 0.635
After 4680 training steps, loss on training batch is 0.113433
2019-07-04 20:32:49.492520: precision = 0.641
After 5070 training steps, loss on training batch is 0.372181
2019-07-04 20:33:09.241484: precision = 0.645
After 5460 training steps, loss on training batch is 0.383643
2019-07-04 20:33:29.026063: precision = 0.641
After 5850 training steps, loss on training batch is 0.221099
2019-07-04 20:33:48.782607: precision = 0.641
After 6240 training steps, loss on training batch is 0.156085
2019-07-04 20:34:08.582542: precision = 0.641
After 6630 training steps, loss on training batch is 0.218626
2019-07-04 20:34:28.318211: precision = 0.645
After 7020 training steps, loss on training batch is 0.272811
2019-07-04 20:34:48.101163: precision = 0.651
After 7410 training steps, loss on training batch is 0.101757
2019-07-04 20:35:07.896302: precision = 0.662
After 7800 training steps, loss on training batch is 0.157350
2019-07-04 20:35:27.669692: precision = 0.655
After 8190 training steps, loss on training batch is 0.149426
2019-07-04 20:35:47.418210: precision = 0.654
After 8580 training steps, loss on training batch is 0.239322
2019-07-04 20:36:07.198204: precision = 0.660
After 8970 training steps, loss on training batch is 0.093555
2019-07-04 20:36:26.925194: precision = 0.658
After 9360 training steps, loss on training batch is 0.087803
2019-07-04 20:36:46.724484: precision = 0.647
After 9750 training steps, loss on training batch is 0.195975
2019-07-04 20:37:06.515116: precision = 0.655
After 10140 training steps, loss on training batch is 0.225032
2019-07-04 20:37:26.371857: precision = 0.649
After 10530 training steps, loss on training batch is 0.126827
2019-07-04 20:37:46.206362: precision = 0.660
After 10920 training steps, loss on training batch is 0.164650
2019-07-04 20:38:06.026893: precision = 0.665
After 11310 training steps, loss on training batch is 0.082961
2019-07-04 20:38:25.775221: precision = 0.659
After 11700 training steps, loss on training batch is 0.056988
2019-07-04 20:38:45.566494: precision = 0.662
After 12090 training steps, loss on training batch is 0.093040
2019-07-04 20:39:05.437128: precision = 0.661
After 12480 training steps, loss on training batch is 0.193964
2019-07-04 20:39:25.214314: precision = 0.656
After 12870 training steps, loss on training batch is 0.205090
2019-07-04 20:39:45.021578: precision = 0.660
After 13260 training steps, loss on training batch is 0.042186
2019-07-04 20:40:04.785855: precision = 0.653
After 13650 training steps, loss on training batch is 0.100956
2019-07-04 20:40:24.599104: precision = 0.666
After 14040 training steps, loss on training batch is 0.042748
2019-07-04 20:40:44.384428: precision = 0.668
After 14430 training steps, loss on training batch is 0.214798
2019-07-04 20:41:04.174343: precision = 0.667
After 14820 training steps, loss on training batch is 0.032027
2019-07-04 20:41:23.944837: precision = 0.664
After 15210 training steps, loss on training batch is 0.042668
2019-07-04 20:41:43.765016: precision = 0.674
After 15600 training steps, loss on training batch is 0.124650
2019-07-04 20:42:03.559171: precision = 0.670
After 15990 training steps, loss on training batch is 0.104238
2019-07-04 20:42:23.355791: precision = 0.670
After 16380 training steps, loss on training batch is 0.110023
2019-07-04 20:42:43.201628: precision = 0.666
After 16770 training steps, loss on training batch is 0.055063
2019-07-04 20:43:03.012597: precision = 0.658
After 17160 training steps, loss on training batch is 0.134773
2019-07-04 20:43:22.710306: precision = 0.666
After 17550 training steps, loss on training batch is 0.087092
2019-07-04 20:43:42.493087: precision = 0.667
After 17940 training steps, loss on training batch is 0.135279
2019-07-04 20:44:02.221349: precision = 0.669
After 18330 training steps, loss on training batch is 0.032006
2019-07-04 20:44:22.012450: precision = 0.666
After 18720 training steps, loss on training batch is 0.048518
2019-07-04 20:44:41.802097: precision = 0.670
After 19110 training steps, loss on training batch is 0.012639
2019-07-04 20:45:01.613626: precision = 0.671
